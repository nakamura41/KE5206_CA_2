{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\ggplot\\utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\ggplot\\stats\\smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import SMOTE #for SMOTE -> install package using: conda install -c conda-forge imbalanced-learn\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import TomekLinks \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import ggplot\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC\n",
    "#from sklearn.svm import SVR #just Testing for regression on other continous data of dataset\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "\n",
    "import pylab as pl\n",
    "from itertools import cycle\n",
    "from sklearn import cross_validation\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#features_list = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','class']\n",
    "#dataset1=pd.read_csv(\"data/Heart_Disease_Data.csv\")\n",
    "dataset1=pd.read_csv(\"data/1and5yearbankrupt.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# SVM requires that each data instance is represented as a vector of real numbers\n",
    "# If you already have numeric dtypes (int8|16|32|64,float64,boolean) you can convert it to another \"numeric\" dtype using Pandas .astype() method. Demo: In [90]: df = pd.DataFrame(np.random.randint(10**5,10**7,(5,3)),columns=list('abc'), dtype=np.int64) In [91]: df Out[91]: a b c 0 9059440 9590567 2076918 1 5861102 4566089 1947323 2 6636568 162770 2487991 3 6794572 5236903 5628779 4 470121 4044395 4546794 In [92]: df.dtypes Out[92]: a int64 b int64 c int64 dtype: object In [93]: df['a'] = df['a'].astype(float) In [94]: df.dtypes Out[94]: a float64 b int64 c int64 dtype: object It won't work for object (string) dtypes, that can't be converted to numbers: In [95]: df.loc[1, 'b'] = 'XXXXXX' In [96]: df Out[96]:...\n",
    "# Just make everything numeric for ease, later we will convert to ordinal/one-hot encoding.\n",
    "dataset1 = dataset1.convert_objects(convert_numeric=True)\n",
    "dataset1 = dataset1.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count missing value in terms of colunms #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.088238</td>\n",
       "      <td>0.554720</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>1.02050</td>\n",
       "      <td>-66.5200</td>\n",
       "      <td>0.342040</td>\n",
       "      <td>0.109490</td>\n",
       "      <td>0.577520</td>\n",
       "      <td>1.08810</td>\n",
       "      <td>0.320360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080955</td>\n",
       "      <td>0.275430</td>\n",
       "      <td>0.91905</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>7.2711</td>\n",
       "      <td>4.73430</td>\n",
       "      <td>142.76000</td>\n",
       "      <td>2.55680</td>\n",
       "      <td>3.25970</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.006202</td>\n",
       "      <td>0.484650</td>\n",
       "      <td>0.232980</td>\n",
       "      <td>1.59980</td>\n",
       "      <td>6.1825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.006202</td>\n",
       "      <td>1.063400</td>\n",
       "      <td>1.27570</td>\n",
       "      <td>0.515350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028591</td>\n",
       "      <td>-0.012035</td>\n",
       "      <td>1.00470</td>\n",
       "      <td>0.152220</td>\n",
       "      <td>6.0911</td>\n",
       "      <td>3.27490</td>\n",
       "      <td>111.14000</td>\n",
       "      <td>3.28410</td>\n",
       "      <td>3.37000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.130240</td>\n",
       "      <td>0.221420</td>\n",
       "      <td>0.577510</td>\n",
       "      <td>3.60820</td>\n",
       "      <td>120.0400</td>\n",
       "      <td>0.187640</td>\n",
       "      <td>0.162120</td>\n",
       "      <td>3.059000</td>\n",
       "      <td>1.14150</td>\n",
       "      <td>0.677310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123960</td>\n",
       "      <td>0.192290</td>\n",
       "      <td>0.87604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.7934</td>\n",
       "      <td>2.98700</td>\n",
       "      <td>71.53100</td>\n",
       "      <td>5.10270</td>\n",
       "      <td>5.61880</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089951</td>\n",
       "      <td>0.887000</td>\n",
       "      <td>0.269270</td>\n",
       "      <td>1.52220</td>\n",
       "      <td>-55.9920</td>\n",
       "      <td>-0.073957</td>\n",
       "      <td>-0.089951</td>\n",
       "      <td>0.127400</td>\n",
       "      <td>1.27540</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418840</td>\n",
       "      <td>-0.796020</td>\n",
       "      <td>0.59074</td>\n",
       "      <td>2.878700</td>\n",
       "      <td>7.6524</td>\n",
       "      <td>3.33020</td>\n",
       "      <td>147.56000</td>\n",
       "      <td>2.47350</td>\n",
       "      <td>5.92990</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048179</td>\n",
       "      <td>0.550410</td>\n",
       "      <td>0.107650</td>\n",
       "      <td>1.24370</td>\n",
       "      <td>-22.9590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059280</td>\n",
       "      <td>0.816820</td>\n",
       "      <td>1.51500</td>\n",
       "      <td>0.449590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.107160</td>\n",
       "      <td>0.77048</td>\n",
       "      <td>0.139380</td>\n",
       "      <td>10.1180</td>\n",
       "      <td>4.09500</td>\n",
       "      <td>106.43000</td>\n",
       "      <td>3.42940</td>\n",
       "      <td>3.36220</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.231560</td>\n",
       "      <td>0.510470</td>\n",
       "      <td>0.472910</td>\n",
       "      <td>1.93930</td>\n",
       "      <td>15.1020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287690</td>\n",
       "      <td>0.958990</td>\n",
       "      <td>1.79150</td>\n",
       "      <td>0.489530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184030</td>\n",
       "      <td>0.473030</td>\n",
       "      <td>0.83996</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>4.6071</td>\n",
       "      <td>4.92200</td>\n",
       "      <td>102.58000</td>\n",
       "      <td>3.55810</td>\n",
       "      <td>75.94100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.099486</td>\n",
       "      <td>0.599910</td>\n",
       "      <td>0.374890</td>\n",
       "      <td>1.65290</td>\n",
       "      <td>19.0360</td>\n",
       "      <td>0.210840</td>\n",
       "      <td>0.123950</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>1.09720</td>\n",
       "      <td>0.400090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088581</td>\n",
       "      <td>0.248660</td>\n",
       "      <td>0.91142</td>\n",
       "      <td>0.064344</td>\n",
       "      <td>5.4655</td>\n",
       "      <td>2.47840</td>\n",
       "      <td>130.02000</td>\n",
       "      <td>2.80720</td>\n",
       "      <td>31.64500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.078518</td>\n",
       "      <td>0.205460</td>\n",
       "      <td>0.103930</td>\n",
       "      <td>2.79390</td>\n",
       "      <td>77.7840</td>\n",
       "      <td>0.365150</td>\n",
       "      <td>0.093388</td>\n",
       "      <td>3.867200</td>\n",
       "      <td>1.23220</td>\n",
       "      <td>0.794540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188420</td>\n",
       "      <td>0.098822</td>\n",
       "      <td>0.81158</td>\n",
       "      <td>0.185660</td>\n",
       "      <td>11.3790</td>\n",
       "      <td>3.16920</td>\n",
       "      <td>53.57500</td>\n",
       "      <td>6.81290</td>\n",
       "      <td>0.47096</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.125040</td>\n",
       "      <td>0.354400</td>\n",
       "      <td>0.314190</td>\n",
       "      <td>2.71270</td>\n",
       "      <td>17.9420</td>\n",
       "      <td>0.305750</td>\n",
       "      <td>0.158430</td>\n",
       "      <td>1.821700</td>\n",
       "      <td>1.23620</td>\n",
       "      <td>0.645600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191090</td>\n",
       "      <td>0.193680</td>\n",
       "      <td>0.80891</td>\n",
       "      <td>0.264790</td>\n",
       "      <td>4.7737</td>\n",
       "      <td>5.74790</td>\n",
       "      <td>59.84100</td>\n",
       "      <td>6.09950</td>\n",
       "      <td>2.22740</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.293430</td>\n",
       "      <td>0.586490</td>\n",
       "      <td>0.083392</td>\n",
       "      <td>1.16940</td>\n",
       "      <td>-55.1520</td>\n",
       "      <td>-0.282110</td>\n",
       "      <td>0.293430</td>\n",
       "      <td>0.705070</td>\n",
       "      <td>1.63760</td>\n",
       "      <td>0.413510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348820</td>\n",
       "      <td>0.709610</td>\n",
       "      <td>0.72150</td>\n",
       "      <td>0.021024</td>\n",
       "      <td>9.8135</td>\n",
       "      <td>5.51040</td>\n",
       "      <td>109.70000</td>\n",
       "      <td>3.32730</td>\n",
       "      <td>3.85820</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.185140</td>\n",
       "      <td>0.339960</td>\n",
       "      <td>0.382720</td>\n",
       "      <td>2.12580</td>\n",
       "      <td>54.4260</td>\n",
       "      <td>0.630350</td>\n",
       "      <td>0.231410</td>\n",
       "      <td>1.842300</td>\n",
       "      <td>1.14610</td>\n",
       "      <td>0.626310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127450</td>\n",
       "      <td>0.295610</td>\n",
       "      <td>0.87255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.6010</td>\n",
       "      <td>3.40340</td>\n",
       "      <td>63.10700</td>\n",
       "      <td>5.78380</td>\n",
       "      <td>7.09020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.090516</td>\n",
       "      <td>0.314290</td>\n",
       "      <td>0.424650</td>\n",
       "      <td>3.20710</td>\n",
       "      <td>24.8740</td>\n",
       "      <td>0.055652</td>\n",
       "      <td>0.105280</td>\n",
       "      <td>2.149900</td>\n",
       "      <td>1.04880</td>\n",
       "      <td>0.675690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046572</td>\n",
       "      <td>0.133960</td>\n",
       "      <td>0.95343</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>7.7104</td>\n",
       "      <td>14.58700</td>\n",
       "      <td>32.18200</td>\n",
       "      <td>11.34200</td>\n",
       "      <td>5.69810</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.153290</td>\n",
       "      <td>0.515630</td>\n",
       "      <td>0.310280</td>\n",
       "      <td>1.60170</td>\n",
       "      <td>19.2110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193670</td>\n",
       "      <td>0.939360</td>\n",
       "      <td>4.28280</td>\n",
       "      <td>0.484370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050271</td>\n",
       "      <td>0.316470</td>\n",
       "      <td>0.95493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.3090</td>\n",
       "      <td>14.21100</td>\n",
       "      <td>43.94500</td>\n",
       "      <td>8.30580</td>\n",
       "      <td>24.60200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.011567</td>\n",
       "      <td>0.352370</td>\n",
       "      <td>0.073708</td>\n",
       "      <td>1.25830</td>\n",
       "      <td>-43.3650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017250</td>\n",
       "      <td>1.837900</td>\n",
       "      <td>0.96593</td>\n",
       "      <td>0.647630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039168</td>\n",
       "      <td>0.017860</td>\n",
       "      <td>0.98218</td>\n",
       "      <td>0.097927</td>\n",
       "      <td>5.4821</td>\n",
       "      <td>5.72260</td>\n",
       "      <td>107.84000</td>\n",
       "      <td>3.38480</td>\n",
       "      <td>1.50710</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.373360</td>\n",
       "      <td>0.207180</td>\n",
       "      <td>0.633830</td>\n",
       "      <td>4.05930</td>\n",
       "      <td>92.2840</td>\n",
       "      <td>0.334810</td>\n",
       "      <td>0.373360</td>\n",
       "      <td>3.826600</td>\n",
       "      <td>2.47250</td>\n",
       "      <td>0.792820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149640</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.85002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.8820</td>\n",
       "      <td>3.51580</td>\n",
       "      <td>30.58500</td>\n",
       "      <td>11.93400</td>\n",
       "      <td>15.55200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.083120</td>\n",
       "      <td>0.293720</td>\n",
       "      <td>0.696570</td>\n",
       "      <td>3.53180</td>\n",
       "      <td>33.2490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104690</td>\n",
       "      <td>2.404700</td>\n",
       "      <td>2.13300</td>\n",
       "      <td>0.706280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053772</td>\n",
       "      <td>0.117690</td>\n",
       "      <td>0.95156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.2886</td>\n",
       "      <td>6.04630</td>\n",
       "      <td>47.08000</td>\n",
       "      <td>7.75280</td>\n",
       "      <td>75.37100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.095972</td>\n",
       "      <td>0.936940</td>\n",
       "      <td>-0.053287</td>\n",
       "      <td>0.94312</td>\n",
       "      <td>-60.8890</td>\n",
       "      <td>-0.207520</td>\n",
       "      <td>-0.095972</td>\n",
       "      <td>0.067299</td>\n",
       "      <td>1.79050</td>\n",
       "      <td>0.063055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006285</td>\n",
       "      <td>-1.522000</td>\n",
       "      <td>1.05320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.2933</td>\n",
       "      <td>2.90580</td>\n",
       "      <td>190.98000</td>\n",
       "      <td>1.91120</td>\n",
       "      <td>15.37400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.130330</td>\n",
       "      <td>0.774240</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>1.27210</td>\n",
       "      <td>7.3584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164870</td>\n",
       "      <td>0.291590</td>\n",
       "      <td>2.76570</td>\n",
       "      <td>0.225760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079606</td>\n",
       "      <td>0.577280</td>\n",
       "      <td>0.94055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.6060</td>\n",
       "      <td>3.59320</td>\n",
       "      <td>102.18000</td>\n",
       "      <td>3.57210</td>\n",
       "      <td>183.64000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.081442</td>\n",
       "      <td>0.701270</td>\n",
       "      <td>0.386960</td>\n",
       "      <td>1.65550</td>\n",
       "      <td>38.3050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.425990</td>\n",
       "      <td>1.97970</td>\n",
       "      <td>0.298730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062960</td>\n",
       "      <td>0.272620</td>\n",
       "      <td>0.94841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.4640</td>\n",
       "      <td>5.92580</td>\n",
       "      <td>108.85000</td>\n",
       "      <td>3.35340</td>\n",
       "      <td>87.29500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.002132</td>\n",
       "      <td>0.250650</td>\n",
       "      <td>0.351250</td>\n",
       "      <td>2.47990</td>\n",
       "      <td>31.8620</td>\n",
       "      <td>0.124150</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>2.337000</td>\n",
       "      <td>1.06370</td>\n",
       "      <td>0.585770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059913</td>\n",
       "      <td>-0.003639</td>\n",
       "      <td>0.94009</td>\n",
       "      <td>0.022707</td>\n",
       "      <td>5.2793</td>\n",
       "      <td>4.82970</td>\n",
       "      <td>67.19000</td>\n",
       "      <td>5.43240</td>\n",
       "      <td>3.13420</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.057271</td>\n",
       "      <td>0.533640</td>\n",
       "      <td>0.399000</td>\n",
       "      <td>1.86640</td>\n",
       "      <td>-6.9193</td>\n",
       "      <td>0.060353</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.873920</td>\n",
       "      <td>1.29320</td>\n",
       "      <td>0.466360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116120</td>\n",
       "      <td>0.122810</td>\n",
       "      <td>0.92114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.3421</td>\n",
       "      <td>3.20420</td>\n",
       "      <td>129.98000</td>\n",
       "      <td>2.80820</td>\n",
       "      <td>9.20480</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.136060</td>\n",
       "      <td>0.295880</td>\n",
       "      <td>0.476740</td>\n",
       "      <td>2.61130</td>\n",
       "      <td>70.8870</td>\n",
       "      <td>0.414150</td>\n",
       "      <td>0.168590</td>\n",
       "      <td>2.340300</td>\n",
       "      <td>1.15510</td>\n",
       "      <td>0.692440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134270</td>\n",
       "      <td>0.196490</td>\n",
       "      <td>0.86573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.9278</td>\n",
       "      <td>3.47320</td>\n",
       "      <td>83.44400</td>\n",
       "      <td>4.37420</td>\n",
       "      <td>5.69200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.072197</td>\n",
       "      <td>0.792440</td>\n",
       "      <td>0.286910</td>\n",
       "      <td>1.40240</td>\n",
       "      <td>35.0650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093762</td>\n",
       "      <td>0.261930</td>\n",
       "      <td>2.20960</td>\n",
       "      <td>0.207560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072076</td>\n",
       "      <td>0.347830</td>\n",
       "      <td>0.95757</td>\n",
       "      <td>0.121840</td>\n",
       "      <td>26.7060</td>\n",
       "      <td>2.63990</td>\n",
       "      <td>117.79000</td>\n",
       "      <td>3.09860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.213680</td>\n",
       "      <td>0.795680</td>\n",
       "      <td>0.026812</td>\n",
       "      <td>1.04720</td>\n",
       "      <td>-72.4390</td>\n",
       "      <td>-0.295010</td>\n",
       "      <td>-0.213680</td>\n",
       "      <td>0.204890</td>\n",
       "      <td>0.88019</td>\n",
       "      <td>0.163020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136110</td>\n",
       "      <td>-1.310700</td>\n",
       "      <td>1.13610</td>\n",
       "      <td>1.393700</td>\n",
       "      <td>4.1127</td>\n",
       "      <td>3.43340</td>\n",
       "      <td>202.03000</td>\n",
       "      <td>1.80660</td>\n",
       "      <td>2.53760</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.903630</td>\n",
       "      <td>0.229350</td>\n",
       "      <td>1.94690</td>\n",
       "      <td>-5.2940</td>\n",
       "      <td>0.218960</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.024442</td>\n",
       "      <td>1.09290</td>\n",
       "      <td>0.022087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084988</td>\n",
       "      <td>5.546500</td>\n",
       "      <td>0.91501</td>\n",
       "      <td>29.947000</td>\n",
       "      <td>14.6790</td>\n",
       "      <td>10.62900</td>\n",
       "      <td>46.93700</td>\n",
       "      <td>7.77640</td>\n",
       "      <td>3.56400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.068391</td>\n",
       "      <td>0.314640</td>\n",
       "      <td>0.454090</td>\n",
       "      <td>2.93340</td>\n",
       "      <td>62.1340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081912</td>\n",
       "      <td>2.178200</td>\n",
       "      <td>1.21570</td>\n",
       "      <td>0.685360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059109</td>\n",
       "      <td>0.099789</td>\n",
       "      <td>0.94086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.1225</td>\n",
       "      <td>3.90330</td>\n",
       "      <td>70.51900</td>\n",
       "      <td>5.17590</td>\n",
       "      <td>3.90850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.161690</td>\n",
       "      <td>0.592430</td>\n",
       "      <td>0.402650</td>\n",
       "      <td>1.68040</td>\n",
       "      <td>-39.4520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202520</td>\n",
       "      <td>0.687960</td>\n",
       "      <td>2.34010</td>\n",
       "      <td>0.407570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097682</td>\n",
       "      <td>0.396720</td>\n",
       "      <td>0.91425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>131.2200</td>\n",
       "      <td>6.48770</td>\n",
       "      <td>92.30200</td>\n",
       "      <td>3.95440</td>\n",
       "      <td>420.19000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.126200</td>\n",
       "      <td>0.109310</td>\n",
       "      <td>0.859530</td>\n",
       "      <td>24.88400</td>\n",
       "      <td>818.9200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126200</td>\n",
       "      <td>8.148300</td>\n",
       "      <td>0.51703</td>\n",
       "      <td>0.890690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269610</td>\n",
       "      <td>0.141690</td>\n",
       "      <td>0.73223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64346</td>\n",
       "      <td>25.40600</td>\n",
       "      <td>14.36700</td>\n",
       "      <td>4.94850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.125010</td>\n",
       "      <td>0.408550</td>\n",
       "      <td>0.371830</td>\n",
       "      <td>2.01380</td>\n",
       "      <td>-7.9209</td>\n",
       "      <td>0.213890</td>\n",
       "      <td>0.156880</td>\n",
       "      <td>1.403200</td>\n",
       "      <td>1.09420</td>\n",
       "      <td>0.573270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086059</td>\n",
       "      <td>0.218060</td>\n",
       "      <td>0.91394</td>\n",
       "      <td>0.072877</td>\n",
       "      <td>5.0881</td>\n",
       "      <td>8.55690</td>\n",
       "      <td>64.65400</td>\n",
       "      <td>5.64550</td>\n",
       "      <td>7.92110</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.540430</td>\n",
       "      <td>0.163510</td>\n",
       "      <td>1.30400</td>\n",
       "      <td>-20.9770</td>\n",
       "      <td>0.378830</td>\n",
       "      <td>0.115960</td>\n",
       "      <td>0.834970</td>\n",
       "      <td>1.06470</td>\n",
       "      <td>0.451240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060753</td>\n",
       "      <td>0.207560</td>\n",
       "      <td>0.93925</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>7.9988</td>\n",
       "      <td>6.20450</td>\n",
       "      <td>85.41800</td>\n",
       "      <td>4.27310</td>\n",
       "      <td>7.69520</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12907</th>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.141440</td>\n",
       "      <td>0.449040</td>\n",
       "      <td>4.17480</td>\n",
       "      <td>60.8960</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>-0.004791</td>\n",
       "      <td>5.952200</td>\n",
       "      <td>0.99613</td>\n",
       "      <td>0.841880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>1.00390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.5574</td>\n",
       "      <td>4.51530</td>\n",
       "      <td>34.11700</td>\n",
       "      <td>10.69800</td>\n",
       "      <td>3.69500</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12908</th>\n",
       "      <td>-0.018516</td>\n",
       "      <td>0.328860</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>-29.2980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.018781</td>\n",
       "      <td>2.040800</td>\n",
       "      <td>1.11420</td>\n",
       "      <td>0.671130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031889</td>\n",
       "      <td>-0.027589</td>\n",
       "      <td>1.01620</td>\n",
       "      <td>0.039008</td>\n",
       "      <td>9.3983</td>\n",
       "      <td>6.34080</td>\n",
       "      <td>89.16300</td>\n",
       "      <td>4.09360</td>\n",
       "      <td>1.69370</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12909</th>\n",
       "      <td>0.022830</td>\n",
       "      <td>0.711320</td>\n",
       "      <td>-0.060759</td>\n",
       "      <td>0.69120</td>\n",
       "      <td>-14.0810</td>\n",
       "      <td>-0.048958</td>\n",
       "      <td>0.024774</td>\n",
       "      <td>0.405840</td>\n",
       "      <td>2.34630</td>\n",
       "      <td>0.288680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>0.079084</td>\n",
       "      <td>0.97892</td>\n",
       "      <td>0.654850</td>\n",
       "      <td>83.1690</td>\n",
       "      <td>25.43700</td>\n",
       "      <td>30.60800</td>\n",
       "      <td>11.92500</td>\n",
       "      <td>2.71560</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12910</th>\n",
       "      <td>-0.000153</td>\n",
       "      <td>0.505820</td>\n",
       "      <td>-0.421270</td>\n",
       "      <td>0.15367</td>\n",
       "      <td>-883.0300</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.004183</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.91488</td>\n",
       "      <td>0.228240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093040</td>\n",
       "      <td>-0.000669</td>\n",
       "      <td>1.09300</td>\n",
       "      <td>0.035313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.26550</td>\n",
       "      <td>1094.00000</td>\n",
       "      <td>0.33365</td>\n",
       "      <td>0.17983</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12911</th>\n",
       "      <td>-0.289710</td>\n",
       "      <td>1.187400</td>\n",
       "      <td>-0.465320</td>\n",
       "      <td>0.53814</td>\n",
       "      <td>-223.2800</td>\n",
       "      <td>-0.289710</td>\n",
       "      <td>-0.289710</td>\n",
       "      <td>-0.158310</td>\n",
       "      <td>0.81801</td>\n",
       "      <td>-0.187980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222480</td>\n",
       "      <td>1.541200</td>\n",
       "      <td>1.22250</td>\n",
       "      <td>-0.956790</td>\n",
       "      <td>3.7491</td>\n",
       "      <td>4.11090</td>\n",
       "      <td>354.26000</td>\n",
       "      <td>1.03030</td>\n",
       "      <td>2.28260</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12912</th>\n",
       "      <td>0.009313</td>\n",
       "      <td>0.617610</td>\n",
       "      <td>-0.222330</td>\n",
       "      <td>0.60119</td>\n",
       "      <td>-62.6300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014544</td>\n",
       "      <td>0.619150</td>\n",
       "      <td>2.08020</td>\n",
       "      <td>0.382390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008170</td>\n",
       "      <td>0.024354</td>\n",
       "      <td>1.00190</td>\n",
       "      <td>0.071883</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>10.08000</td>\n",
       "      <td>97.81900</td>\n",
       "      <td>3.73140</td>\n",
       "      <td>3.12880</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12913</th>\n",
       "      <td>0.167730</td>\n",
       "      <td>0.773890</td>\n",
       "      <td>0.166630</td>\n",
       "      <td>1.24140</td>\n",
       "      <td>-28.0390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217420</td>\n",
       "      <td>0.292170</td>\n",
       "      <td>4.88180</td>\n",
       "      <td>0.226110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043982</td>\n",
       "      <td>0.741830</td>\n",
       "      <td>0.95170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.3158</td>\n",
       "      <td>15.93100</td>\n",
       "      <td>51.61100</td>\n",
       "      <td>7.07220</td>\n",
       "      <td>34.13400</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12914</th>\n",
       "      <td>-0.330330</td>\n",
       "      <td>0.932660</td>\n",
       "      <td>-0.476930</td>\n",
       "      <td>0.48863</td>\n",
       "      <td>-350.8300</td>\n",
       "      <td>-0.330330</td>\n",
       "      <td>-0.330330</td>\n",
       "      <td>0.068734</td>\n",
       "      <td>0.70565</td>\n",
       "      <td>0.064105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.417130</td>\n",
       "      <td>-5.153000</td>\n",
       "      <td>1.41710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.8878</td>\n",
       "      <td>7.52440</td>\n",
       "      <td>504.65000</td>\n",
       "      <td>0.72328</td>\n",
       "      <td>1.23940</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12915</th>\n",
       "      <td>0.046080</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.996180</td>\n",
       "      <td>261.50000</td>\n",
       "      <td>174.3300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046080</td>\n",
       "      <td>260.500000</td>\n",
       "      <td>2.08780</td>\n",
       "      <td>0.996180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.046257</td>\n",
       "      <td>0.97800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.41920</td>\n",
       "      <td>0.66856</td>\n",
       "      <td>545.95000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12916</th>\n",
       "      <td>0.133310</td>\n",
       "      <td>0.164510</td>\n",
       "      <td>0.400840</td>\n",
       "      <td>3.43660</td>\n",
       "      <td>2.8373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169280</td>\n",
       "      <td>5.078600</td>\n",
       "      <td>3.65580</td>\n",
       "      <td>0.835490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066065</td>\n",
       "      <td>0.159550</td>\n",
       "      <td>0.94701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.8104</td>\n",
       "      <td>21.32800</td>\n",
       "      <td>16.42500</td>\n",
       "      <td>22.22200</td>\n",
       "      <td>8.41110</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12917</th>\n",
       "      <td>0.038665</td>\n",
       "      <td>0.071884</td>\n",
       "      <td>0.488840</td>\n",
       "      <td>7.80040</td>\n",
       "      <td>221.0100</td>\n",
       "      <td>0.038665</td>\n",
       "      <td>0.045892</td>\n",
       "      <td>11.068000</td>\n",
       "      <td>1.07650</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071070</td>\n",
       "      <td>0.048599</td>\n",
       "      <td>0.92893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.1470</td>\n",
       "      <td>1.46790</td>\n",
       "      <td>53.73500</td>\n",
       "      <td>6.79260</td>\n",
       "      <td>1.11160</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12918</th>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.851600</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>1.00860</td>\n",
       "      <td>-44.4670</td>\n",
       "      <td>0.086248</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.174290</td>\n",
       "      <td>1.02970</td>\n",
       "      <td>0.148420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198900</td>\n",
       "      <td>0.007349</td>\n",
       "      <td>0.97403</td>\n",
       "      <td>2.033100</td>\n",
       "      <td>6.8515</td>\n",
       "      <td>4.10960</td>\n",
       "      <td>142.83000</td>\n",
       "      <td>2.55560</td>\n",
       "      <td>1.73460</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12919</th>\n",
       "      <td>-0.091442</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>-0.047216</td>\n",
       "      <td>0.92568</td>\n",
       "      <td>-7.2952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.090374</td>\n",
       "      <td>0.417440</td>\n",
       "      <td>9.13450</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>-0.310490</td>\n",
       "      <td>1.00740</td>\n",
       "      <td>0.077583</td>\n",
       "      <td>72.8930</td>\n",
       "      <td>20.79000</td>\n",
       "      <td>25.38400</td>\n",
       "      <td>14.37900</td>\n",
       "      <td>22.18000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12920</th>\n",
       "      <td>0.138090</td>\n",
       "      <td>3.335700</td>\n",
       "      <td>-2.364000</td>\n",
       "      <td>0.29128</td>\n",
       "      <td>-88.3820</td>\n",
       "      <td>-3.396300</td>\n",
       "      <td>0.138090</td>\n",
       "      <td>-0.700210</td>\n",
       "      <td>9.98520</td>\n",
       "      <td>-2.335700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011347</td>\n",
       "      <td>-0.059122</td>\n",
       "      <td>0.97866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>231.9000</td>\n",
       "      <td>12.65100</td>\n",
       "      <td>121.93000</td>\n",
       "      <td>2.99350</td>\n",
       "      <td>351.85000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12921</th>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>1.00050</td>\n",
       "      <td>-43.1910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128380</td>\n",
       "      <td>0.200190</td>\n",
       "      <td>2.51440</td>\n",
       "      <td>0.166820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150320</td>\n",
       "      <td>0.589090</td>\n",
       "      <td>0.85877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.2728</td>\n",
       "      <td>4.92070</td>\n",
       "      <td>116.48000</td>\n",
       "      <td>3.13360</td>\n",
       "      <td>12.74800</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12922</th>\n",
       "      <td>0.006404</td>\n",
       "      <td>0.674770</td>\n",
       "      <td>-0.008438</td>\n",
       "      <td>0.98668</td>\n",
       "      <td>-45.5990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>0.482000</td>\n",
       "      <td>1.91090</td>\n",
       "      <td>0.325230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.019690</td>\n",
       "      <td>0.81928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.9144</td>\n",
       "      <td>4.99670</td>\n",
       "      <td>121.00000</td>\n",
       "      <td>3.01640</td>\n",
       "      <td>5.09650</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12923</th>\n",
       "      <td>0.037953</td>\n",
       "      <td>0.122390</td>\n",
       "      <td>0.230840</td>\n",
       "      <td>2.88600</td>\n",
       "      <td>19.5810</td>\n",
       "      <td>0.037953</td>\n",
       "      <td>0.040082</td>\n",
       "      <td>6.886400</td>\n",
       "      <td>0.98825</td>\n",
       "      <td>0.842840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011887</td>\n",
       "      <td>0.045030</td>\n",
       "      <td>1.01190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.8809</td>\n",
       "      <td>34.64100</td>\n",
       "      <td>60.06000</td>\n",
       "      <td>6.07730</td>\n",
       "      <td>1.15040</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12924</th>\n",
       "      <td>0.161170</td>\n",
       "      <td>0.072135</td>\n",
       "      <td>0.636490</td>\n",
       "      <td>12.02500</td>\n",
       "      <td>178.9000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199860</td>\n",
       "      <td>12.863000</td>\n",
       "      <td>1.29080</td>\n",
       "      <td>0.927860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150780</td>\n",
       "      <td>0.173710</td>\n",
       "      <td>0.84152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.2700</td>\n",
       "      <td>6.31650</td>\n",
       "      <td>16.32500</td>\n",
       "      <td>22.35800</td>\n",
       "      <td>4.22300</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12925</th>\n",
       "      <td>0.019432</td>\n",
       "      <td>0.616330</td>\n",
       "      <td>0.057718</td>\n",
       "      <td>1.10620</td>\n",
       "      <td>-162.8800</td>\n",
       "      <td>0.019432</td>\n",
       "      <td>0.008018</td>\n",
       "      <td>-0.232850</td>\n",
       "      <td>1.06560</td>\n",
       "      <td>-0.143510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061544</td>\n",
       "      <td>-0.135400</td>\n",
       "      <td>0.93846</td>\n",
       "      <td>-0.508510</td>\n",
       "      <td>2.1849</td>\n",
       "      <td>16.65800</td>\n",
       "      <td>179.33000</td>\n",
       "      <td>2.03540</td>\n",
       "      <td>2.77270</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12926</th>\n",
       "      <td>0.022707</td>\n",
       "      <td>0.598550</td>\n",
       "      <td>-0.152680</td>\n",
       "      <td>0.65004</td>\n",
       "      <td>-55.4880</td>\n",
       "      <td>0.022707</td>\n",
       "      <td>0.023087</td>\n",
       "      <td>0.666780</td>\n",
       "      <td>0.99912</td>\n",
       "      <td>0.399100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000878</td>\n",
       "      <td>0.056896</td>\n",
       "      <td>1.00090</td>\n",
       "      <td>0.406580</td>\n",
       "      <td>15.1100</td>\n",
       "      <td>13.14500</td>\n",
       "      <td>83.39800</td>\n",
       "      <td>4.37660</td>\n",
       "      <td>2.66530</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12927</th>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.730140</td>\n",
       "      <td>0.083767</td>\n",
       "      <td>1.28650</td>\n",
       "      <td>-19.8070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020462</td>\n",
       "      <td>0.369610</td>\n",
       "      <td>2.19550</td>\n",
       "      <td>0.269860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031304</td>\n",
       "      <td>0.058928</td>\n",
       "      <td>1.00160</td>\n",
       "      <td>1.470900</td>\n",
       "      <td>11.0380</td>\n",
       "      <td>12.56200</td>\n",
       "      <td>48.60000</td>\n",
       "      <td>7.51020</td>\n",
       "      <td>3.51900</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12928</th>\n",
       "      <td>-0.051896</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.054014</td>\n",
       "      <td>1.10610</td>\n",
       "      <td>-9.7422</td>\n",
       "      <td>-0.051896</td>\n",
       "      <td>-0.051896</td>\n",
       "      <td>0.686440</td>\n",
       "      <td>0.98574</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014470</td>\n",
       "      <td>-0.140970</td>\n",
       "      <td>1.01450</td>\n",
       "      <td>0.073630</td>\n",
       "      <td>21.6900</td>\n",
       "      <td>13.71200</td>\n",
       "      <td>66.22400</td>\n",
       "      <td>5.51160</td>\n",
       "      <td>6.42520</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12929</th>\n",
       "      <td>-0.031617</td>\n",
       "      <td>0.811750</td>\n",
       "      <td>-0.202300</td>\n",
       "      <td>0.61087</td>\n",
       "      <td>-44.0110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.031617</td>\n",
       "      <td>0.231900</td>\n",
       "      <td>3.03470</td>\n",
       "      <td>0.188250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018387</td>\n",
       "      <td>-0.167950</td>\n",
       "      <td>0.98125</td>\n",
       "      <td>1.101500</td>\n",
       "      <td>20.3810</td>\n",
       "      <td>20.39500</td>\n",
       "      <td>62.52900</td>\n",
       "      <td>5.83730</td>\n",
       "      <td>4.44690</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12930</th>\n",
       "      <td>0.025664</td>\n",
       "      <td>0.762040</td>\n",
       "      <td>0.091220</td>\n",
       "      <td>1.12630</td>\n",
       "      <td>-62.1550</td>\n",
       "      <td>0.025664</td>\n",
       "      <td>0.037809</td>\n",
       "      <td>0.213700</td>\n",
       "      <td>1.06870</td>\n",
       "      <td>0.162850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064291</td>\n",
       "      <td>0.157590</td>\n",
       "      <td>0.93571</td>\n",
       "      <td>0.245150</td>\n",
       "      <td>4.8872</td>\n",
       "      <td>3.66920</td>\n",
       "      <td>158.21000</td>\n",
       "      <td>2.30710</td>\n",
       "      <td>8.92490</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12931</th>\n",
       "      <td>0.069049</td>\n",
       "      <td>0.682320</td>\n",
       "      <td>0.069829</td>\n",
       "      <td>1.12710</td>\n",
       "      <td>-54.5330</td>\n",
       "      <td>0.069049</td>\n",
       "      <td>0.075024</td>\n",
       "      <td>0.424540</td>\n",
       "      <td>1.05380</td>\n",
       "      <td>0.289670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051048</td>\n",
       "      <td>0.238370</td>\n",
       "      <td>0.94895</td>\n",
       "      <td>0.458290</td>\n",
       "      <td>5.5224</td>\n",
       "      <td>3.90480</td>\n",
       "      <td>145.40000</td>\n",
       "      <td>2.51040</td>\n",
       "      <td>3.62500</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12932</th>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>-0.136190</td>\n",
       "      <td>0.60839</td>\n",
       "      <td>-18.4490</td>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.972030</td>\n",
       "      <td>1.01210</td>\n",
       "      <td>0.460840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011909</td>\n",
       "      <td>0.039866</td>\n",
       "      <td>0.98809</td>\n",
       "      <td>0.274140</td>\n",
       "      <td>73.5050</td>\n",
       "      <td>79.23700</td>\n",
       "      <td>31.26800</td>\n",
       "      <td>11.67300</td>\n",
       "      <td>5.14890</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12933</th>\n",
       "      <td>-0.013359</td>\n",
       "      <td>0.583540</td>\n",
       "      <td>-0.022650</td>\n",
       "      <td>0.92896</td>\n",
       "      <td>-42.2320</td>\n",
       "      <td>-0.013359</td>\n",
       "      <td>-0.015036</td>\n",
       "      <td>0.562890</td>\n",
       "      <td>0.98904</td>\n",
       "      <td>0.328470</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011082</td>\n",
       "      <td>-0.040671</td>\n",
       "      <td>1.01110</td>\n",
       "      <td>0.805920</td>\n",
       "      <td>10.5990</td>\n",
       "      <td>7.17400</td>\n",
       "      <td>94.09200</td>\n",
       "      <td>3.87920</td>\n",
       "      <td>1.75720</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12934</th>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.502760</td>\n",
       "      <td>0.439230</td>\n",
       "      <td>1.87360</td>\n",
       "      <td>9.7417</td>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.012022</td>\n",
       "      <td>0.983560</td>\n",
       "      <td>1.00830</td>\n",
       "      <td>0.494490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>0.99174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.4700</td>\n",
       "      <td>6.07590</td>\n",
       "      <td>51.01900</td>\n",
       "      <td>7.15420</td>\n",
       "      <td>62.00100</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12935</th>\n",
       "      <td>-0.041643</td>\n",
       "      <td>0.848100</td>\n",
       "      <td>-0.128520</td>\n",
       "      <td>0.57485</td>\n",
       "      <td>-121.9200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.036795</td>\n",
       "      <td>0.179010</td>\n",
       "      <td>0.42138</td>\n",
       "      <td>0.151820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232720</td>\n",
       "      <td>-0.274290</td>\n",
       "      <td>0.98788</td>\n",
       "      <td>3.593100</td>\n",
       "      <td>39.7030</td>\n",
       "      <td>3.14200</td>\n",
       "      <td>261.85000</td>\n",
       "      <td>1.39390</td>\n",
       "      <td>0.51005</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12936</th>\n",
       "      <td>0.014946</td>\n",
       "      <td>0.946480</td>\n",
       "      <td>0.032110</td>\n",
       "      <td>1.03630</td>\n",
       "      <td>-20.5810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015260</td>\n",
       "      <td>0.056357</td>\n",
       "      <td>2.96940</td>\n",
       "      <td>0.053341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>0.280210</td>\n",
       "      <td>0.97443</td>\n",
       "      <td>1.179200</td>\n",
       "      <td>15.0360</td>\n",
       "      <td>4.17410</td>\n",
       "      <td>108.64000</td>\n",
       "      <td>3.35990</td>\n",
       "      <td>35.11800</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12937 rows  65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Attr1     Attr2     Attr3      Attr4     Attr5     Attr6     Attr7  \\\n",
       "0      0.088238  0.554720  0.011340    1.02050  -66.5200  0.342040  0.109490   \n",
       "1     -0.006202  0.484650  0.232980    1.59980    6.1825  0.000000 -0.006202   \n",
       "2      0.130240  0.221420  0.577510    3.60820  120.0400  0.187640  0.162120   \n",
       "3     -0.089951  0.887000  0.269270    1.52220  -55.9920 -0.073957 -0.089951   \n",
       "4      0.048179  0.550410  0.107650    1.24370  -22.9590  0.000000  0.059280   \n",
       "5      0.231560  0.510470  0.472910    1.93930   15.1020  0.000000  0.287690   \n",
       "6      0.099486  0.599910  0.374890    1.65290   19.0360  0.210840  0.123950   \n",
       "7      0.078518  0.205460  0.103930    2.79390   77.7840  0.365150  0.093388   \n",
       "8      0.125040  0.354400  0.314190    2.71270   17.9420  0.305750  0.158430   \n",
       "9      0.293430  0.586490  0.083392    1.16940  -55.1520 -0.282110  0.293430   \n",
       "10     0.185140  0.339960  0.382720    2.12580   54.4260  0.630350  0.231410   \n",
       "11     0.090516  0.314290  0.424650    3.20710   24.8740  0.055652  0.105280   \n",
       "12     0.153290  0.515630  0.310280    1.60170   19.2110  0.000000  0.193670   \n",
       "13     0.011567  0.352370  0.073708    1.25830  -43.3650  0.000000  0.017250   \n",
       "14     0.373360  0.207180  0.633830    4.05930   92.2840  0.334810  0.373360   \n",
       "15     0.083120  0.293720  0.696570    3.53180   33.2490  0.000000  0.104690   \n",
       "16    -0.095972  0.936940 -0.053287    0.94312  -60.8890 -0.207520 -0.095972   \n",
       "17     0.130330  0.774240  0.210700    1.27210    7.3584  0.000000  0.164870   \n",
       "18     0.081442  0.701270  0.386960    1.65550   38.3050  0.000000  0.102900   \n",
       "19    -0.002132  0.250650  0.351250    2.47990   31.8620  0.124150  0.005896   \n",
       "20     0.057271  0.533640  0.399000    1.86640   -6.9193  0.060353  0.103500   \n",
       "21     0.136060  0.295880  0.476740    2.61130   70.8870  0.414150  0.168590   \n",
       "22     0.072197  0.792440  0.286910    1.40240   35.0650  0.000000  0.093762   \n",
       "23    -0.213680  0.795680  0.026812    1.04720  -72.4390 -0.295010 -0.213680   \n",
       "24     0.122500  0.903630  0.229350    1.94690   -5.2940  0.218960  0.122500   \n",
       "25     0.068391  0.314640  0.454090    2.93340   62.1340  0.000000  0.081912   \n",
       "26     0.161690  0.592430  0.402650    1.68040  -39.4520  0.000000  0.202520   \n",
       "27     0.126200  0.109310  0.859530   24.88400  818.9200  0.000000  0.126200   \n",
       "28     0.125010  0.408550  0.371830    2.01380   -7.9209  0.213890  0.156880   \n",
       "29     0.093660  0.540430  0.163510    1.30400  -20.9770  0.378830  0.115960   \n",
       "...         ...       ...       ...        ...       ...       ...       ...   \n",
       "12907  0.003802  0.141440  0.449040    4.17480   60.8960  0.003802 -0.004791   \n",
       "12908 -0.018516  0.328860  0.069952    1.25700  -29.2980  0.000000 -0.018781   \n",
       "12909  0.022830  0.711320 -0.060759    0.69120  -14.0810 -0.048958  0.024774   \n",
       "12910 -0.000153  0.505820 -0.421270    0.15367 -883.0300 -0.000153 -0.004183   \n",
       "12911 -0.289710  1.187400 -0.465320    0.53814 -223.2800 -0.289710 -0.289710   \n",
       "12912  0.009313  0.617610 -0.222330    0.60119  -62.6300  0.000000  0.014544   \n",
       "12913  0.167730  0.773890  0.166630    1.24140  -28.0390  0.000000  0.217420   \n",
       "12914 -0.330330  0.932660 -0.476930    0.48863 -350.8300 -0.330330 -0.330330   \n",
       "12915  0.046080  0.003824  0.996180  261.50000  174.3300  0.000000  0.046080   \n",
       "12916  0.133310  0.164510  0.400840    3.43660    2.8373  0.000000  0.169280   \n",
       "12917  0.038665  0.071884  0.488840    7.80040  221.0100  0.038665  0.045892   \n",
       "12918  0.001091  0.851600  0.003463    1.00860  -44.4670  0.086248  0.001091   \n",
       "12919 -0.091442  0.705500 -0.047216    0.92568   -7.2952  0.000000 -0.090374   \n",
       "12920  0.138090  3.335700 -2.364000    0.29128  -88.3820 -3.396300  0.138090   \n",
       "12921  0.098271  0.833300  0.000426    1.00050  -43.1910  0.000000  0.128380   \n",
       "12922  0.006404  0.674770 -0.008438    0.98668  -45.5990  0.000000  0.009958   \n",
       "12923  0.037953  0.122390  0.230840    2.88600   19.5810  0.037953  0.040082   \n",
       "12924  0.161170  0.072135  0.636490   12.02500  178.9000  0.000000  0.199860   \n",
       "12925  0.019432  0.616330  0.057718    1.10620 -162.8800  0.019432  0.008018   \n",
       "12926  0.022707  0.598550 -0.152680    0.65004  -55.4880  0.022707  0.023087   \n",
       "12927  0.015903  0.730140  0.083767    1.28650  -19.8070  0.000000  0.020462   \n",
       "12928 -0.051896  0.536300  0.054014    1.10610   -9.7422 -0.051896 -0.051896   \n",
       "12929 -0.031617  0.811750 -0.202300    0.61087  -44.0110  0.000000 -0.031617   \n",
       "12930  0.025664  0.762040  0.091220    1.12630  -62.1550  0.025664  0.037809   \n",
       "12931  0.069049  0.682320  0.069829    1.12710  -54.5330  0.069049  0.075024   \n",
       "12932  0.018371  0.474100 -0.136190    0.60839  -18.4490  0.018371  0.018371   \n",
       "12933 -0.013359  0.583540 -0.022650    0.92896  -42.2320 -0.013359 -0.015036   \n",
       "12934  0.006338  0.502760  0.439230    1.87360    9.7417  0.006338  0.012022   \n",
       "12935 -0.041643  0.848100 -0.128520    0.57485 -121.9200  0.000000 -0.036795   \n",
       "12936  0.014946  0.946480  0.032110    1.03630  -20.5810  0.000000  0.015260   \n",
       "\n",
       "            Attr8    Attr9    Attr10  ...      Attr56    Attr57   Attr58  \\\n",
       "0        0.577520  1.08810  0.320360  ...    0.080955  0.275430  0.91905   \n",
       "1        1.063400  1.27570  0.515350  ...   -0.028591 -0.012035  1.00470   \n",
       "2        3.059000  1.14150  0.677310  ...    0.123960  0.192290  0.87604   \n",
       "3        0.127400  1.27540  0.113000  ...    0.418840 -0.796020  0.59074   \n",
       "4        0.816820  1.51500  0.449590  ...    0.240400  0.107160  0.77048   \n",
       "5        0.958990  1.79150  0.489530  ...    0.184030  0.473030  0.83996   \n",
       "6        0.666900  1.09720  0.400090  ...    0.088581  0.248660  0.91142   \n",
       "7        3.867200  1.23220  0.794540  ...    0.188420  0.098822  0.81158   \n",
       "8        1.821700  1.23620  0.645600  ...    0.191090  0.193680  0.80891   \n",
       "9        0.705070  1.63760  0.413510  ...    0.348820  0.709610  0.72150   \n",
       "10       1.842300  1.14610  0.626310  ...    0.127450  0.295610  0.87255   \n",
       "11       2.149900  1.04880  0.675690  ...    0.046572  0.133960  0.95343   \n",
       "12       0.939360  4.28280  0.484370  ...    0.050271  0.316470  0.95493   \n",
       "13       1.837900  0.96593  0.647630  ...    0.039168  0.017860  0.98218   \n",
       "14       3.826600  2.47250  0.792820  ...    0.149640  0.470930  0.85002   \n",
       "15       2.404700  2.13300  0.706280  ...    0.053772  0.117690  0.95156   \n",
       "16       0.067299  1.79050  0.063055  ...   -0.006285 -1.522000  1.05320   \n",
       "17       0.291590  2.76570  0.225760  ...    0.079606  0.577280  0.94055   \n",
       "18       0.425990  1.97970  0.298730  ...    0.062960  0.272620  0.94841   \n",
       "19       2.337000  1.06370  0.585770  ...    0.059913 -0.003639  0.94009   \n",
       "20       0.873920  1.29320  0.466360  ...    0.116120  0.122810  0.92114   \n",
       "21       2.340300  1.15510  0.692440  ...    0.134270  0.196490  0.86573   \n",
       "22       0.261930  2.20960  0.207560  ...    0.072076  0.347830  0.95757   \n",
       "23       0.204890  0.88019  0.163020  ...   -0.136110 -1.310700  1.13610   \n",
       "24       0.024442  1.09290  0.022087  ...    0.084988  5.546500  0.91501   \n",
       "25       2.178200  1.21570  0.685360  ...   -0.059109  0.099789  0.94086   \n",
       "26       0.687960  2.34010  0.407570  ...    0.097682  0.396720  0.91425   \n",
       "27       8.148300  0.51703  0.890690  ...    0.269610  0.141690  0.73223   \n",
       "28       1.403200  1.09420  0.573270  ...    0.086059  0.218060  0.91394   \n",
       "29       0.834970  1.06470  0.451240  ...    0.060753  0.207560  0.93925   \n",
       "...           ...      ...       ...  ...         ...       ...      ...   \n",
       "12907    5.952200  0.99613  0.841880  ...   -0.003885  0.004516  1.00390   \n",
       "12908    2.040800  1.11420  0.671130  ...   -0.031889 -0.027589  1.01620   \n",
       "12909    0.405840  2.34630  0.288680  ...    0.004260  0.079084  0.97892   \n",
       "12910    0.451220  0.91488  0.228240  ...   -0.093040 -0.000669  1.09300   \n",
       "12911   -0.158310  0.81801 -0.187980  ...   -0.222480  1.541200  1.22250   \n",
       "12912    0.619150  2.08020  0.382390  ...    0.008170  0.024354  1.00190   \n",
       "12913    0.292170  4.88180  0.226110  ...    0.043982  0.741830  0.95170   \n",
       "12914    0.068734  0.70565  0.064105  ...   -0.417130 -5.153000  1.41710   \n",
       "12915  260.500000  2.08780  0.996180  ...    0.019324  0.046257  0.97800   \n",
       "12916    5.078600  3.65580  0.835490  ...    0.066065  0.159550  0.94701   \n",
       "12917   11.068000  1.07650  0.795600  ...    0.071070  0.048599  0.92893   \n",
       "12918    0.174290  1.02970  0.148420  ...   -0.198900  0.007349  0.97403   \n",
       "12919    0.417440  9.13450  0.294500  ...    0.000966 -0.310490  1.00740   \n",
       "12920   -0.700210  9.98520 -2.335700  ...    0.011347 -0.059122  0.97866   \n",
       "12921    0.200190  2.51440  0.166820  ...    0.150320  0.589090  0.85877   \n",
       "12922    0.482000  1.91090  0.325230  ...    0.180200  0.019690  0.81928   \n",
       "12923    6.886400  0.98825  0.842840  ...   -0.011887  0.045030  1.01190   \n",
       "12924   12.863000  1.29080  0.927860  ...    0.150780  0.173710  0.84152   \n",
       "12925   -0.232850  1.06560 -0.143510  ...    0.061544 -0.135400  0.93846   \n",
       "12926    0.666780  0.99912  0.399100  ...   -0.000878  0.056896  1.00090   \n",
       "12927    0.369610  2.19550  0.269860  ...    0.031304  0.058928  1.00160   \n",
       "12928    0.686440  0.98574  0.368140  ...   -0.014470 -0.140970  1.01450   \n",
       "12929    0.231900  3.03470  0.188250  ...    0.018387 -0.167950  0.98125   \n",
       "12930    0.213700  1.06870  0.162850  ...    0.064291  0.157590  0.93571   \n",
       "12931    0.424540  1.05380  0.289670  ...    0.051048  0.238370  0.94895   \n",
       "12932    0.972030  1.01210  0.460840  ...    0.011909  0.039866  0.98809   \n",
       "12933    0.562890  0.98904  0.328470  ...   -0.011082 -0.040671  1.01110   \n",
       "12934    0.983560  1.00830  0.494490  ...    0.008258  0.012817  0.99174   \n",
       "12935    0.179010  0.42138  0.151820  ...   -0.232720 -0.274290  0.98788   \n",
       "12936    0.056357  2.96940  0.053341  ...    0.015705  0.280210  0.97443   \n",
       "\n",
       "          Attr59    Attr60    Attr61      Attr62     Attr63     Attr64  class  \n",
       "0       0.002024    7.2711   4.73430   142.76000    2.55680    3.25970    0.0  \n",
       "1       0.152220    6.0911   3.27490   111.14000    3.28410    3.37000    0.0  \n",
       "2       0.000000    8.7934   2.98700    71.53100    5.10270    5.61880    0.0  \n",
       "3       2.878700    7.6524   3.33020   147.56000    2.47350    5.92990    0.0  \n",
       "4       0.139380   10.1180   4.09500   106.43000    3.42940    3.36220    0.0  \n",
       "5       0.014242    4.6071   4.92200   102.58000    3.55810   75.94100    0.0  \n",
       "6       0.064344    5.4655   2.47840   130.02000    2.80720   31.64500    0.0  \n",
       "7       0.185660   11.3790   3.16920    53.57500    6.81290    0.47096    0.0  \n",
       "8       0.264790    4.7737   5.74790    59.84100    6.09950    2.22740    0.0  \n",
       "9       0.021024    9.8135   5.51040   109.70000    3.32730    3.85820    0.0  \n",
       "10      0.000000   15.6010   3.40340    63.10700    5.78380    7.09020    0.0  \n",
       "11      0.180400    7.7104  14.58700    32.18200   11.34200    5.69810    0.0  \n",
       "12      0.000000   70.3090  14.21100    43.94500    8.30580   24.60200    0.0  \n",
       "13      0.097927    5.4821   5.72260   107.84000    3.38480    1.50710    0.0  \n",
       "14      0.000000   26.8820   3.51580    30.58500   11.93400   15.55200    0.0  \n",
       "15      0.000000    4.2886   6.04630    47.08000    7.75280   75.37100    0.0  \n",
       "16      0.000000    7.2933   2.90580   190.98000    1.91120   15.37400    0.0  \n",
       "17      0.000000   17.6060   3.59320   102.18000    3.57210  183.64000    0.0  \n",
       "18      0.000000   10.4640   5.92580   108.85000    3.35340   87.29500    0.0  \n",
       "19      0.022707    5.2793   4.82970    67.19000    5.43240    3.13420    0.0  \n",
       "20      0.000000    3.3421   3.20420   129.98000    2.80820    9.20480    0.0  \n",
       "21      0.000000    4.9278   3.47320    83.44400    4.37420    5.69200    0.0  \n",
       "22      0.121840   26.7060   2.63990   117.79000    3.09860        NaN    0.0  \n",
       "23      1.393700    4.1127   3.43340   202.03000    1.80660    2.53760    0.0  \n",
       "24     29.947000   14.6790  10.62900    46.93700    7.77640    3.56400    0.0  \n",
       "25      0.000000    5.1225   3.90330    70.51900    5.17590    3.90850    0.0  \n",
       "26      0.000000  131.2200   6.48770    92.30200    3.95440  420.19000    0.0  \n",
       "27      0.000000       NaN   0.64346    25.40600   14.36700    4.94850    0.0  \n",
       "28      0.072877    5.0881   8.55690    64.65400    5.64550    7.92110    0.0  \n",
       "29      0.005745    7.9988   6.20450    85.41800    4.27310    7.69520    0.0  \n",
       "...          ...       ...       ...         ...        ...        ...    ...  \n",
       "12907   0.000000    7.5574   4.51530    34.11700   10.69800    3.69500    2.0  \n",
       "12908   0.039008    9.3983   6.34080    89.16300    4.09360    1.69370    2.0  \n",
       "12909   0.654850   83.1690  25.43700    30.60800   11.92500    2.71560    2.0  \n",
       "12910   0.035313       NaN   2.26550  1094.00000    0.33365    0.17983    2.0  \n",
       "12911  -0.956790    3.7491   4.11090   354.26000    1.03030    2.28260    2.0  \n",
       "12912   0.071883   20.0000  10.08000    97.81900    3.73140    3.12880    2.0  \n",
       "12913   0.000000    9.3158  15.93100    51.61100    7.07220   34.13400    2.0  \n",
       "12914   0.000000    1.8878   7.52440   504.65000    0.72328    1.23940    2.0  \n",
       "12915   0.000000       NaN   6.41920     0.66856  545.95000        NaN    2.0  \n",
       "12916   0.000000    9.8104  21.32800    16.42500   22.22200    8.41110    2.0  \n",
       "12917   0.000000    2.1470   1.46790    53.73500    6.79260    1.11160    2.0  \n",
       "12918   2.033100    6.8515   4.10960   142.83000    2.55560    1.73460    2.0  \n",
       "12919   0.077583   72.8930  20.79000    25.38400   14.37900   22.18000    2.0  \n",
       "12920   0.000000  231.9000  12.65100   121.93000    2.99350  351.85000    2.0  \n",
       "12921   0.000000    9.2728   4.92070   116.48000    3.13360   12.74800    2.0  \n",
       "12922   0.000000    8.9144   4.99670   121.00000    3.01640    5.09650    2.0  \n",
       "12923   0.000000    3.8809  34.64100    60.06000    6.07730    1.15040    2.0  \n",
       "12924   0.000000   12.2700   6.31650    16.32500   22.35800    4.22300    2.0  \n",
       "12925  -0.508510    2.1849  16.65800   179.33000    2.03540    2.77270    2.0  \n",
       "12926   0.406580   15.1100  13.14500    83.39800    4.37660    2.66530    2.0  \n",
       "12927   1.470900   11.0380  12.56200    48.60000    7.51020    3.51900    2.0  \n",
       "12928   0.073630   21.6900  13.71200    66.22400    5.51160    6.42520    2.0  \n",
       "12929   1.101500   20.3810  20.39500    62.52900    5.83730    4.44690    2.0  \n",
       "12930   0.245150    4.8872   3.66920   158.21000    2.30710    8.92490    2.0  \n",
       "12931   0.458290    5.5224   3.90480   145.40000    2.51040    3.62500    2.0  \n",
       "12932   0.274140   73.5050  79.23700    31.26800   11.67300    5.14890    2.0  \n",
       "12933   0.805920   10.5990   7.17400    94.09200    3.87920    1.75720    2.0  \n",
       "12934   0.000000   10.4700   6.07590    51.01900    7.15420   62.00100    2.0  \n",
       "12935   3.593100   39.7030   3.14200   261.85000    1.39390    0.51005    2.0  \n",
       "12936   1.179200   15.0360   4.17410   108.64000    3.35990   35.11800    2.0  \n",
       "\n",
       "[12937 rows x 65 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attr1      True\n",
      "Attr2      True\n",
      "Attr3      True\n",
      "Attr4      True\n",
      "Attr5      True\n",
      "Attr6      True\n",
      "Attr7      True\n",
      "Attr8      True\n",
      "Attr9      True\n",
      "Attr10     True\n",
      "Attr11     True\n",
      "Attr12     True\n",
      "Attr13    False\n",
      "Attr14     True\n",
      "Attr15     True\n",
      "Attr16     True\n",
      "Attr17     True\n",
      "Attr18     True\n",
      "Attr19    False\n",
      "Attr20    False\n",
      "Attr21     True\n",
      "Attr22     True\n",
      "Attr23    False\n",
      "Attr24     True\n",
      "Attr25     True\n",
      "Attr26     True\n",
      "Attr27     True\n",
      "Attr28     True\n",
      "Attr29     True\n",
      "Attr30    False\n",
      "          ...  \n",
      "Attr36     True\n",
      "Attr37     True\n",
      "Attr38     True\n",
      "Attr39    False\n",
      "Attr40     True\n",
      "Attr41     True\n",
      "Attr42    False\n",
      "Attr43    False\n",
      "Attr44    False\n",
      "Attr45     True\n",
      "Attr46     True\n",
      "Attr47     True\n",
      "Attr48     True\n",
      "Attr49    False\n",
      "Attr50     True\n",
      "Attr51     True\n",
      "Attr52     True\n",
      "Attr53     True\n",
      "Attr54     True\n",
      "Attr55    False\n",
      "Attr56    False\n",
      "Attr57     True\n",
      "Attr58    False\n",
      "Attr59     True\n",
      "Attr60     True\n",
      "Attr61     True\n",
      "Attr62    False\n",
      "Attr63     True\n",
      "Attr64     True\n",
      "class     False\n",
      "Length: 65, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "#dataset.shape[0] - dataset.count()\n",
    "print(dataset1.isnull().any())\n",
    "dataset1 = dataset1.replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check for duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.088238</td>\n",
       "      <td>0.554720</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>1.02050</td>\n",
       "      <td>-66.5200</td>\n",
       "      <td>0.342040</td>\n",
       "      <td>0.109490</td>\n",
       "      <td>0.577520</td>\n",
       "      <td>1.08810</td>\n",
       "      <td>0.320360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080955</td>\n",
       "      <td>0.275430</td>\n",
       "      <td>0.91905</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>7.2711</td>\n",
       "      <td>4.73430</td>\n",
       "      <td>142.76000</td>\n",
       "      <td>2.55680</td>\n",
       "      <td>3.25970</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.006202</td>\n",
       "      <td>0.484650</td>\n",
       "      <td>0.232980</td>\n",
       "      <td>1.59980</td>\n",
       "      <td>6.1825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.006202</td>\n",
       "      <td>1.063400</td>\n",
       "      <td>1.27570</td>\n",
       "      <td>0.515350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028591</td>\n",
       "      <td>-0.012035</td>\n",
       "      <td>1.00470</td>\n",
       "      <td>0.152220</td>\n",
       "      <td>6.0911</td>\n",
       "      <td>3.27490</td>\n",
       "      <td>111.14000</td>\n",
       "      <td>3.28410</td>\n",
       "      <td>3.37000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.130240</td>\n",
       "      <td>0.221420</td>\n",
       "      <td>0.577510</td>\n",
       "      <td>3.60820</td>\n",
       "      <td>120.0400</td>\n",
       "      <td>0.187640</td>\n",
       "      <td>0.162120</td>\n",
       "      <td>3.059000</td>\n",
       "      <td>1.14150</td>\n",
       "      <td>0.677310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123960</td>\n",
       "      <td>0.192290</td>\n",
       "      <td>0.87604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.7934</td>\n",
       "      <td>2.98700</td>\n",
       "      <td>71.53100</td>\n",
       "      <td>5.10270</td>\n",
       "      <td>5.61880</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089951</td>\n",
       "      <td>0.887000</td>\n",
       "      <td>0.269270</td>\n",
       "      <td>1.52220</td>\n",
       "      <td>-55.9920</td>\n",
       "      <td>-0.073957</td>\n",
       "      <td>-0.089951</td>\n",
       "      <td>0.127400</td>\n",
       "      <td>1.27540</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418840</td>\n",
       "      <td>-0.796020</td>\n",
       "      <td>0.59074</td>\n",
       "      <td>2.878700</td>\n",
       "      <td>7.6524</td>\n",
       "      <td>3.33020</td>\n",
       "      <td>147.56000</td>\n",
       "      <td>2.47350</td>\n",
       "      <td>5.92990</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048179</td>\n",
       "      <td>0.550410</td>\n",
       "      <td>0.107650</td>\n",
       "      <td>1.24370</td>\n",
       "      <td>-22.9590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059280</td>\n",
       "      <td>0.816820</td>\n",
       "      <td>1.51500</td>\n",
       "      <td>0.449590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.107160</td>\n",
       "      <td>0.77048</td>\n",
       "      <td>0.139380</td>\n",
       "      <td>10.1180</td>\n",
       "      <td>4.09500</td>\n",
       "      <td>106.43000</td>\n",
       "      <td>3.42940</td>\n",
       "      <td>3.36220</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.231560</td>\n",
       "      <td>0.510470</td>\n",
       "      <td>0.472910</td>\n",
       "      <td>1.93930</td>\n",
       "      <td>15.1020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287690</td>\n",
       "      <td>0.958990</td>\n",
       "      <td>1.79150</td>\n",
       "      <td>0.489530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184030</td>\n",
       "      <td>0.473030</td>\n",
       "      <td>0.83996</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>4.6071</td>\n",
       "      <td>4.92200</td>\n",
       "      <td>102.58000</td>\n",
       "      <td>3.55810</td>\n",
       "      <td>75.94100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.099486</td>\n",
       "      <td>0.599910</td>\n",
       "      <td>0.374890</td>\n",
       "      <td>1.65290</td>\n",
       "      <td>19.0360</td>\n",
       "      <td>0.210840</td>\n",
       "      <td>0.123950</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>1.09720</td>\n",
       "      <td>0.400090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088581</td>\n",
       "      <td>0.248660</td>\n",
       "      <td>0.91142</td>\n",
       "      <td>0.064344</td>\n",
       "      <td>5.4655</td>\n",
       "      <td>2.47840</td>\n",
       "      <td>130.02000</td>\n",
       "      <td>2.80720</td>\n",
       "      <td>31.64500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.078518</td>\n",
       "      <td>0.205460</td>\n",
       "      <td>0.103930</td>\n",
       "      <td>2.79390</td>\n",
       "      <td>77.7840</td>\n",
       "      <td>0.365150</td>\n",
       "      <td>0.093388</td>\n",
       "      <td>3.867200</td>\n",
       "      <td>1.23220</td>\n",
       "      <td>0.794540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188420</td>\n",
       "      <td>0.098822</td>\n",
       "      <td>0.81158</td>\n",
       "      <td>0.185660</td>\n",
       "      <td>11.3790</td>\n",
       "      <td>3.16920</td>\n",
       "      <td>53.57500</td>\n",
       "      <td>6.81290</td>\n",
       "      <td>0.47096</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.125040</td>\n",
       "      <td>0.354400</td>\n",
       "      <td>0.314190</td>\n",
       "      <td>2.71270</td>\n",
       "      <td>17.9420</td>\n",
       "      <td>0.305750</td>\n",
       "      <td>0.158430</td>\n",
       "      <td>1.821700</td>\n",
       "      <td>1.23620</td>\n",
       "      <td>0.645600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191090</td>\n",
       "      <td>0.193680</td>\n",
       "      <td>0.80891</td>\n",
       "      <td>0.264790</td>\n",
       "      <td>4.7737</td>\n",
       "      <td>5.74790</td>\n",
       "      <td>59.84100</td>\n",
       "      <td>6.09950</td>\n",
       "      <td>2.22740</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.293430</td>\n",
       "      <td>0.586490</td>\n",
       "      <td>0.083392</td>\n",
       "      <td>1.16940</td>\n",
       "      <td>-55.1520</td>\n",
       "      <td>-0.282110</td>\n",
       "      <td>0.293430</td>\n",
       "      <td>0.705070</td>\n",
       "      <td>1.63760</td>\n",
       "      <td>0.413510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348820</td>\n",
       "      <td>0.709610</td>\n",
       "      <td>0.72150</td>\n",
       "      <td>0.021024</td>\n",
       "      <td>9.8135</td>\n",
       "      <td>5.51040</td>\n",
       "      <td>109.70000</td>\n",
       "      <td>3.32730</td>\n",
       "      <td>3.85820</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.185140</td>\n",
       "      <td>0.339960</td>\n",
       "      <td>0.382720</td>\n",
       "      <td>2.12580</td>\n",
       "      <td>54.4260</td>\n",
       "      <td>0.630350</td>\n",
       "      <td>0.231410</td>\n",
       "      <td>1.842300</td>\n",
       "      <td>1.14610</td>\n",
       "      <td>0.626310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127450</td>\n",
       "      <td>0.295610</td>\n",
       "      <td>0.87255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.6010</td>\n",
       "      <td>3.40340</td>\n",
       "      <td>63.10700</td>\n",
       "      <td>5.78380</td>\n",
       "      <td>7.09020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.090516</td>\n",
       "      <td>0.314290</td>\n",
       "      <td>0.424650</td>\n",
       "      <td>3.20710</td>\n",
       "      <td>24.8740</td>\n",
       "      <td>0.055652</td>\n",
       "      <td>0.105280</td>\n",
       "      <td>2.149900</td>\n",
       "      <td>1.04880</td>\n",
       "      <td>0.675690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046572</td>\n",
       "      <td>0.133960</td>\n",
       "      <td>0.95343</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>7.7104</td>\n",
       "      <td>14.58700</td>\n",
       "      <td>32.18200</td>\n",
       "      <td>11.34200</td>\n",
       "      <td>5.69810</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.153290</td>\n",
       "      <td>0.515630</td>\n",
       "      <td>0.310280</td>\n",
       "      <td>1.60170</td>\n",
       "      <td>19.2110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193670</td>\n",
       "      <td>0.939360</td>\n",
       "      <td>4.28280</td>\n",
       "      <td>0.484370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050271</td>\n",
       "      <td>0.316470</td>\n",
       "      <td>0.95493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.3090</td>\n",
       "      <td>14.21100</td>\n",
       "      <td>43.94500</td>\n",
       "      <td>8.30580</td>\n",
       "      <td>24.60200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.011567</td>\n",
       "      <td>0.352370</td>\n",
       "      <td>0.073708</td>\n",
       "      <td>1.25830</td>\n",
       "      <td>-43.3650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017250</td>\n",
       "      <td>1.837900</td>\n",
       "      <td>0.96593</td>\n",
       "      <td>0.647630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039168</td>\n",
       "      <td>0.017860</td>\n",
       "      <td>0.98218</td>\n",
       "      <td>0.097927</td>\n",
       "      <td>5.4821</td>\n",
       "      <td>5.72260</td>\n",
       "      <td>107.84000</td>\n",
       "      <td>3.38480</td>\n",
       "      <td>1.50710</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.373360</td>\n",
       "      <td>0.207180</td>\n",
       "      <td>0.633830</td>\n",
       "      <td>4.05930</td>\n",
       "      <td>92.2840</td>\n",
       "      <td>0.334810</td>\n",
       "      <td>0.373360</td>\n",
       "      <td>3.826600</td>\n",
       "      <td>2.47250</td>\n",
       "      <td>0.792820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149640</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.85002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.8820</td>\n",
       "      <td>3.51580</td>\n",
       "      <td>30.58500</td>\n",
       "      <td>11.93400</td>\n",
       "      <td>15.55200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.083120</td>\n",
       "      <td>0.293720</td>\n",
       "      <td>0.696570</td>\n",
       "      <td>3.53180</td>\n",
       "      <td>33.2490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104690</td>\n",
       "      <td>2.404700</td>\n",
       "      <td>2.13300</td>\n",
       "      <td>0.706280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053772</td>\n",
       "      <td>0.117690</td>\n",
       "      <td>0.95156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.2886</td>\n",
       "      <td>6.04630</td>\n",
       "      <td>47.08000</td>\n",
       "      <td>7.75280</td>\n",
       "      <td>75.37100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.095972</td>\n",
       "      <td>0.936940</td>\n",
       "      <td>-0.053287</td>\n",
       "      <td>0.94312</td>\n",
       "      <td>-60.8890</td>\n",
       "      <td>-0.207520</td>\n",
       "      <td>-0.095972</td>\n",
       "      <td>0.067299</td>\n",
       "      <td>1.79050</td>\n",
       "      <td>0.063055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006285</td>\n",
       "      <td>-1.522000</td>\n",
       "      <td>1.05320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.2933</td>\n",
       "      <td>2.90580</td>\n",
       "      <td>190.98000</td>\n",
       "      <td>1.91120</td>\n",
       "      <td>15.37400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.130330</td>\n",
       "      <td>0.774240</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>1.27210</td>\n",
       "      <td>7.3584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164870</td>\n",
       "      <td>0.291590</td>\n",
       "      <td>2.76570</td>\n",
       "      <td>0.225760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079606</td>\n",
       "      <td>0.577280</td>\n",
       "      <td>0.94055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.6060</td>\n",
       "      <td>3.59320</td>\n",
       "      <td>102.18000</td>\n",
       "      <td>3.57210</td>\n",
       "      <td>183.64000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.081442</td>\n",
       "      <td>0.701270</td>\n",
       "      <td>0.386960</td>\n",
       "      <td>1.65550</td>\n",
       "      <td>38.3050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.425990</td>\n",
       "      <td>1.97970</td>\n",
       "      <td>0.298730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062960</td>\n",
       "      <td>0.272620</td>\n",
       "      <td>0.94841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.4640</td>\n",
       "      <td>5.92580</td>\n",
       "      <td>108.85000</td>\n",
       "      <td>3.35340</td>\n",
       "      <td>87.29500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.002132</td>\n",
       "      <td>0.250650</td>\n",
       "      <td>0.351250</td>\n",
       "      <td>2.47990</td>\n",
       "      <td>31.8620</td>\n",
       "      <td>0.124150</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>2.337000</td>\n",
       "      <td>1.06370</td>\n",
       "      <td>0.585770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059913</td>\n",
       "      <td>-0.003639</td>\n",
       "      <td>0.94009</td>\n",
       "      <td>0.022707</td>\n",
       "      <td>5.2793</td>\n",
       "      <td>4.82970</td>\n",
       "      <td>67.19000</td>\n",
       "      <td>5.43240</td>\n",
       "      <td>3.13420</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.057271</td>\n",
       "      <td>0.533640</td>\n",
       "      <td>0.399000</td>\n",
       "      <td>1.86640</td>\n",
       "      <td>-6.9193</td>\n",
       "      <td>0.060353</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.873920</td>\n",
       "      <td>1.29320</td>\n",
       "      <td>0.466360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116120</td>\n",
       "      <td>0.122810</td>\n",
       "      <td>0.92114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.3421</td>\n",
       "      <td>3.20420</td>\n",
       "      <td>129.98000</td>\n",
       "      <td>2.80820</td>\n",
       "      <td>9.20480</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.136060</td>\n",
       "      <td>0.295880</td>\n",
       "      <td>0.476740</td>\n",
       "      <td>2.61130</td>\n",
       "      <td>70.8870</td>\n",
       "      <td>0.414150</td>\n",
       "      <td>0.168590</td>\n",
       "      <td>2.340300</td>\n",
       "      <td>1.15510</td>\n",
       "      <td>0.692440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134270</td>\n",
       "      <td>0.196490</td>\n",
       "      <td>0.86573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.9278</td>\n",
       "      <td>3.47320</td>\n",
       "      <td>83.44400</td>\n",
       "      <td>4.37420</td>\n",
       "      <td>5.69200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.072197</td>\n",
       "      <td>0.792440</td>\n",
       "      <td>0.286910</td>\n",
       "      <td>1.40240</td>\n",
       "      <td>35.0650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093762</td>\n",
       "      <td>0.261930</td>\n",
       "      <td>2.20960</td>\n",
       "      <td>0.207560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072076</td>\n",
       "      <td>0.347830</td>\n",
       "      <td>0.95757</td>\n",
       "      <td>0.121840</td>\n",
       "      <td>26.7060</td>\n",
       "      <td>2.63990</td>\n",
       "      <td>117.79000</td>\n",
       "      <td>3.09860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.213680</td>\n",
       "      <td>0.795680</td>\n",
       "      <td>0.026812</td>\n",
       "      <td>1.04720</td>\n",
       "      <td>-72.4390</td>\n",
       "      <td>-0.295010</td>\n",
       "      <td>-0.213680</td>\n",
       "      <td>0.204890</td>\n",
       "      <td>0.88019</td>\n",
       "      <td>0.163020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136110</td>\n",
       "      <td>-1.310700</td>\n",
       "      <td>1.13610</td>\n",
       "      <td>1.393700</td>\n",
       "      <td>4.1127</td>\n",
       "      <td>3.43340</td>\n",
       "      <td>202.03000</td>\n",
       "      <td>1.80660</td>\n",
       "      <td>2.53760</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.903630</td>\n",
       "      <td>0.229350</td>\n",
       "      <td>1.94690</td>\n",
       "      <td>-5.2940</td>\n",
       "      <td>0.218960</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.024442</td>\n",
       "      <td>1.09290</td>\n",
       "      <td>0.022087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084988</td>\n",
       "      <td>5.546500</td>\n",
       "      <td>0.91501</td>\n",
       "      <td>29.947000</td>\n",
       "      <td>14.6790</td>\n",
       "      <td>10.62900</td>\n",
       "      <td>46.93700</td>\n",
       "      <td>7.77640</td>\n",
       "      <td>3.56400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.068391</td>\n",
       "      <td>0.314640</td>\n",
       "      <td>0.454090</td>\n",
       "      <td>2.93340</td>\n",
       "      <td>62.1340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081912</td>\n",
       "      <td>2.178200</td>\n",
       "      <td>1.21570</td>\n",
       "      <td>0.685360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059109</td>\n",
       "      <td>0.099789</td>\n",
       "      <td>0.94086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.1225</td>\n",
       "      <td>3.90330</td>\n",
       "      <td>70.51900</td>\n",
       "      <td>5.17590</td>\n",
       "      <td>3.90850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.161690</td>\n",
       "      <td>0.592430</td>\n",
       "      <td>0.402650</td>\n",
       "      <td>1.68040</td>\n",
       "      <td>-39.4520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202520</td>\n",
       "      <td>0.687960</td>\n",
       "      <td>2.34010</td>\n",
       "      <td>0.407570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097682</td>\n",
       "      <td>0.396720</td>\n",
       "      <td>0.91425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>131.2200</td>\n",
       "      <td>6.48770</td>\n",
       "      <td>92.30200</td>\n",
       "      <td>3.95440</td>\n",
       "      <td>420.19000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.126200</td>\n",
       "      <td>0.109310</td>\n",
       "      <td>0.859530</td>\n",
       "      <td>24.88400</td>\n",
       "      <td>818.9200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126200</td>\n",
       "      <td>8.148300</td>\n",
       "      <td>0.51703</td>\n",
       "      <td>0.890690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269610</td>\n",
       "      <td>0.141690</td>\n",
       "      <td>0.73223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64346</td>\n",
       "      <td>25.40600</td>\n",
       "      <td>14.36700</td>\n",
       "      <td>4.94850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.125010</td>\n",
       "      <td>0.408550</td>\n",
       "      <td>0.371830</td>\n",
       "      <td>2.01380</td>\n",
       "      <td>-7.9209</td>\n",
       "      <td>0.213890</td>\n",
       "      <td>0.156880</td>\n",
       "      <td>1.403200</td>\n",
       "      <td>1.09420</td>\n",
       "      <td>0.573270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086059</td>\n",
       "      <td>0.218060</td>\n",
       "      <td>0.91394</td>\n",
       "      <td>0.072877</td>\n",
       "      <td>5.0881</td>\n",
       "      <td>8.55690</td>\n",
       "      <td>64.65400</td>\n",
       "      <td>5.64550</td>\n",
       "      <td>7.92110</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.540430</td>\n",
       "      <td>0.163510</td>\n",
       "      <td>1.30400</td>\n",
       "      <td>-20.9770</td>\n",
       "      <td>0.378830</td>\n",
       "      <td>0.115960</td>\n",
       "      <td>0.834970</td>\n",
       "      <td>1.06470</td>\n",
       "      <td>0.451240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060753</td>\n",
       "      <td>0.207560</td>\n",
       "      <td>0.93925</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>7.9988</td>\n",
       "      <td>6.20450</td>\n",
       "      <td>85.41800</td>\n",
       "      <td>4.27310</td>\n",
       "      <td>7.69520</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12907</th>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.141440</td>\n",
       "      <td>0.449040</td>\n",
       "      <td>4.17480</td>\n",
       "      <td>60.8960</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>-0.004791</td>\n",
       "      <td>5.952200</td>\n",
       "      <td>0.99613</td>\n",
       "      <td>0.841880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>1.00390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.5574</td>\n",
       "      <td>4.51530</td>\n",
       "      <td>34.11700</td>\n",
       "      <td>10.69800</td>\n",
       "      <td>3.69500</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12908</th>\n",
       "      <td>-0.018516</td>\n",
       "      <td>0.328860</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>-29.2980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.018781</td>\n",
       "      <td>2.040800</td>\n",
       "      <td>1.11420</td>\n",
       "      <td>0.671130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031889</td>\n",
       "      <td>-0.027589</td>\n",
       "      <td>1.01620</td>\n",
       "      <td>0.039008</td>\n",
       "      <td>9.3983</td>\n",
       "      <td>6.34080</td>\n",
       "      <td>89.16300</td>\n",
       "      <td>4.09360</td>\n",
       "      <td>1.69370</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12909</th>\n",
       "      <td>0.022830</td>\n",
       "      <td>0.711320</td>\n",
       "      <td>-0.060759</td>\n",
       "      <td>0.69120</td>\n",
       "      <td>-14.0810</td>\n",
       "      <td>-0.048958</td>\n",
       "      <td>0.024774</td>\n",
       "      <td>0.405840</td>\n",
       "      <td>2.34630</td>\n",
       "      <td>0.288680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>0.079084</td>\n",
       "      <td>0.97892</td>\n",
       "      <td>0.654850</td>\n",
       "      <td>83.1690</td>\n",
       "      <td>25.43700</td>\n",
       "      <td>30.60800</td>\n",
       "      <td>11.92500</td>\n",
       "      <td>2.71560</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12910</th>\n",
       "      <td>-0.000153</td>\n",
       "      <td>0.505820</td>\n",
       "      <td>-0.421270</td>\n",
       "      <td>0.15367</td>\n",
       "      <td>-883.0300</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.004183</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.91488</td>\n",
       "      <td>0.228240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093040</td>\n",
       "      <td>-0.000669</td>\n",
       "      <td>1.09300</td>\n",
       "      <td>0.035313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.26550</td>\n",
       "      <td>1094.00000</td>\n",
       "      <td>0.33365</td>\n",
       "      <td>0.17983</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12911</th>\n",
       "      <td>-0.289710</td>\n",
       "      <td>1.187400</td>\n",
       "      <td>-0.465320</td>\n",
       "      <td>0.53814</td>\n",
       "      <td>-223.2800</td>\n",
       "      <td>-0.289710</td>\n",
       "      <td>-0.289710</td>\n",
       "      <td>-0.158310</td>\n",
       "      <td>0.81801</td>\n",
       "      <td>-0.187980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222480</td>\n",
       "      <td>1.541200</td>\n",
       "      <td>1.22250</td>\n",
       "      <td>-0.956790</td>\n",
       "      <td>3.7491</td>\n",
       "      <td>4.11090</td>\n",
       "      <td>354.26000</td>\n",
       "      <td>1.03030</td>\n",
       "      <td>2.28260</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12912</th>\n",
       "      <td>0.009313</td>\n",
       "      <td>0.617610</td>\n",
       "      <td>-0.222330</td>\n",
       "      <td>0.60119</td>\n",
       "      <td>-62.6300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014544</td>\n",
       "      <td>0.619150</td>\n",
       "      <td>2.08020</td>\n",
       "      <td>0.382390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008170</td>\n",
       "      <td>0.024354</td>\n",
       "      <td>1.00190</td>\n",
       "      <td>0.071883</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>10.08000</td>\n",
       "      <td>97.81900</td>\n",
       "      <td>3.73140</td>\n",
       "      <td>3.12880</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12913</th>\n",
       "      <td>0.167730</td>\n",
       "      <td>0.773890</td>\n",
       "      <td>0.166630</td>\n",
       "      <td>1.24140</td>\n",
       "      <td>-28.0390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217420</td>\n",
       "      <td>0.292170</td>\n",
       "      <td>4.88180</td>\n",
       "      <td>0.226110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043982</td>\n",
       "      <td>0.741830</td>\n",
       "      <td>0.95170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.3158</td>\n",
       "      <td>15.93100</td>\n",
       "      <td>51.61100</td>\n",
       "      <td>7.07220</td>\n",
       "      <td>34.13400</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12914</th>\n",
       "      <td>-0.330330</td>\n",
       "      <td>0.932660</td>\n",
       "      <td>-0.476930</td>\n",
       "      <td>0.48863</td>\n",
       "      <td>-350.8300</td>\n",
       "      <td>-0.330330</td>\n",
       "      <td>-0.330330</td>\n",
       "      <td>0.068734</td>\n",
       "      <td>0.70565</td>\n",
       "      <td>0.064105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.417130</td>\n",
       "      <td>-5.153000</td>\n",
       "      <td>1.41710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.8878</td>\n",
       "      <td>7.52440</td>\n",
       "      <td>504.65000</td>\n",
       "      <td>0.72328</td>\n",
       "      <td>1.23940</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12915</th>\n",
       "      <td>0.046080</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.996180</td>\n",
       "      <td>261.50000</td>\n",
       "      <td>174.3300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046080</td>\n",
       "      <td>260.500000</td>\n",
       "      <td>2.08780</td>\n",
       "      <td>0.996180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.046257</td>\n",
       "      <td>0.97800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.41920</td>\n",
       "      <td>0.66856</td>\n",
       "      <td>545.95000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12916</th>\n",
       "      <td>0.133310</td>\n",
       "      <td>0.164510</td>\n",
       "      <td>0.400840</td>\n",
       "      <td>3.43660</td>\n",
       "      <td>2.8373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169280</td>\n",
       "      <td>5.078600</td>\n",
       "      <td>3.65580</td>\n",
       "      <td>0.835490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066065</td>\n",
       "      <td>0.159550</td>\n",
       "      <td>0.94701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.8104</td>\n",
       "      <td>21.32800</td>\n",
       "      <td>16.42500</td>\n",
       "      <td>22.22200</td>\n",
       "      <td>8.41110</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12917</th>\n",
       "      <td>0.038665</td>\n",
       "      <td>0.071884</td>\n",
       "      <td>0.488840</td>\n",
       "      <td>7.80040</td>\n",
       "      <td>221.0100</td>\n",
       "      <td>0.038665</td>\n",
       "      <td>0.045892</td>\n",
       "      <td>11.068000</td>\n",
       "      <td>1.07650</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071070</td>\n",
       "      <td>0.048599</td>\n",
       "      <td>0.92893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.1470</td>\n",
       "      <td>1.46790</td>\n",
       "      <td>53.73500</td>\n",
       "      <td>6.79260</td>\n",
       "      <td>1.11160</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12918</th>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.851600</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>1.00860</td>\n",
       "      <td>-44.4670</td>\n",
       "      <td>0.086248</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.174290</td>\n",
       "      <td>1.02970</td>\n",
       "      <td>0.148420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198900</td>\n",
       "      <td>0.007349</td>\n",
       "      <td>0.97403</td>\n",
       "      <td>2.033100</td>\n",
       "      <td>6.8515</td>\n",
       "      <td>4.10960</td>\n",
       "      <td>142.83000</td>\n",
       "      <td>2.55560</td>\n",
       "      <td>1.73460</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12919</th>\n",
       "      <td>-0.091442</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>-0.047216</td>\n",
       "      <td>0.92568</td>\n",
       "      <td>-7.2952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.090374</td>\n",
       "      <td>0.417440</td>\n",
       "      <td>9.13450</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>-0.310490</td>\n",
       "      <td>1.00740</td>\n",
       "      <td>0.077583</td>\n",
       "      <td>72.8930</td>\n",
       "      <td>20.79000</td>\n",
       "      <td>25.38400</td>\n",
       "      <td>14.37900</td>\n",
       "      <td>22.18000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12920</th>\n",
       "      <td>0.138090</td>\n",
       "      <td>3.335700</td>\n",
       "      <td>-2.364000</td>\n",
       "      <td>0.29128</td>\n",
       "      <td>-88.3820</td>\n",
       "      <td>-3.396300</td>\n",
       "      <td>0.138090</td>\n",
       "      <td>-0.700210</td>\n",
       "      <td>9.98520</td>\n",
       "      <td>-2.335700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011347</td>\n",
       "      <td>-0.059122</td>\n",
       "      <td>0.97866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>231.9000</td>\n",
       "      <td>12.65100</td>\n",
       "      <td>121.93000</td>\n",
       "      <td>2.99350</td>\n",
       "      <td>351.85000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12921</th>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>1.00050</td>\n",
       "      <td>-43.1910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128380</td>\n",
       "      <td>0.200190</td>\n",
       "      <td>2.51440</td>\n",
       "      <td>0.166820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150320</td>\n",
       "      <td>0.589090</td>\n",
       "      <td>0.85877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.2728</td>\n",
       "      <td>4.92070</td>\n",
       "      <td>116.48000</td>\n",
       "      <td>3.13360</td>\n",
       "      <td>12.74800</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12922</th>\n",
       "      <td>0.006404</td>\n",
       "      <td>0.674770</td>\n",
       "      <td>-0.008438</td>\n",
       "      <td>0.98668</td>\n",
       "      <td>-45.5990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>0.482000</td>\n",
       "      <td>1.91090</td>\n",
       "      <td>0.325230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.019690</td>\n",
       "      <td>0.81928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.9144</td>\n",
       "      <td>4.99670</td>\n",
       "      <td>121.00000</td>\n",
       "      <td>3.01640</td>\n",
       "      <td>5.09650</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12923</th>\n",
       "      <td>0.037953</td>\n",
       "      <td>0.122390</td>\n",
       "      <td>0.230840</td>\n",
       "      <td>2.88600</td>\n",
       "      <td>19.5810</td>\n",
       "      <td>0.037953</td>\n",
       "      <td>0.040082</td>\n",
       "      <td>6.886400</td>\n",
       "      <td>0.98825</td>\n",
       "      <td>0.842840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011887</td>\n",
       "      <td>0.045030</td>\n",
       "      <td>1.01190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.8809</td>\n",
       "      <td>34.64100</td>\n",
       "      <td>60.06000</td>\n",
       "      <td>6.07730</td>\n",
       "      <td>1.15040</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12924</th>\n",
       "      <td>0.161170</td>\n",
       "      <td>0.072135</td>\n",
       "      <td>0.636490</td>\n",
       "      <td>12.02500</td>\n",
       "      <td>178.9000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199860</td>\n",
       "      <td>12.863000</td>\n",
       "      <td>1.29080</td>\n",
       "      <td>0.927860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150780</td>\n",
       "      <td>0.173710</td>\n",
       "      <td>0.84152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.2700</td>\n",
       "      <td>6.31650</td>\n",
       "      <td>16.32500</td>\n",
       "      <td>22.35800</td>\n",
       "      <td>4.22300</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12925</th>\n",
       "      <td>0.019432</td>\n",
       "      <td>0.616330</td>\n",
       "      <td>0.057718</td>\n",
       "      <td>1.10620</td>\n",
       "      <td>-162.8800</td>\n",
       "      <td>0.019432</td>\n",
       "      <td>0.008018</td>\n",
       "      <td>-0.232850</td>\n",
       "      <td>1.06560</td>\n",
       "      <td>-0.143510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061544</td>\n",
       "      <td>-0.135400</td>\n",
       "      <td>0.93846</td>\n",
       "      <td>-0.508510</td>\n",
       "      <td>2.1849</td>\n",
       "      <td>16.65800</td>\n",
       "      <td>179.33000</td>\n",
       "      <td>2.03540</td>\n",
       "      <td>2.77270</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12926</th>\n",
       "      <td>0.022707</td>\n",
       "      <td>0.598550</td>\n",
       "      <td>-0.152680</td>\n",
       "      <td>0.65004</td>\n",
       "      <td>-55.4880</td>\n",
       "      <td>0.022707</td>\n",
       "      <td>0.023087</td>\n",
       "      <td>0.666780</td>\n",
       "      <td>0.99912</td>\n",
       "      <td>0.399100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000878</td>\n",
       "      <td>0.056896</td>\n",
       "      <td>1.00090</td>\n",
       "      <td>0.406580</td>\n",
       "      <td>15.1100</td>\n",
       "      <td>13.14500</td>\n",
       "      <td>83.39800</td>\n",
       "      <td>4.37660</td>\n",
       "      <td>2.66530</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12927</th>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.730140</td>\n",
       "      <td>0.083767</td>\n",
       "      <td>1.28650</td>\n",
       "      <td>-19.8070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020462</td>\n",
       "      <td>0.369610</td>\n",
       "      <td>2.19550</td>\n",
       "      <td>0.269860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031304</td>\n",
       "      <td>0.058928</td>\n",
       "      <td>1.00160</td>\n",
       "      <td>1.470900</td>\n",
       "      <td>11.0380</td>\n",
       "      <td>12.56200</td>\n",
       "      <td>48.60000</td>\n",
       "      <td>7.51020</td>\n",
       "      <td>3.51900</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12928</th>\n",
       "      <td>-0.051896</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.054014</td>\n",
       "      <td>1.10610</td>\n",
       "      <td>-9.7422</td>\n",
       "      <td>-0.051896</td>\n",
       "      <td>-0.051896</td>\n",
       "      <td>0.686440</td>\n",
       "      <td>0.98574</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014470</td>\n",
       "      <td>-0.140970</td>\n",
       "      <td>1.01450</td>\n",
       "      <td>0.073630</td>\n",
       "      <td>21.6900</td>\n",
       "      <td>13.71200</td>\n",
       "      <td>66.22400</td>\n",
       "      <td>5.51160</td>\n",
       "      <td>6.42520</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12929</th>\n",
       "      <td>-0.031617</td>\n",
       "      <td>0.811750</td>\n",
       "      <td>-0.202300</td>\n",
       "      <td>0.61087</td>\n",
       "      <td>-44.0110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.031617</td>\n",
       "      <td>0.231900</td>\n",
       "      <td>3.03470</td>\n",
       "      <td>0.188250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018387</td>\n",
       "      <td>-0.167950</td>\n",
       "      <td>0.98125</td>\n",
       "      <td>1.101500</td>\n",
       "      <td>20.3810</td>\n",
       "      <td>20.39500</td>\n",
       "      <td>62.52900</td>\n",
       "      <td>5.83730</td>\n",
       "      <td>4.44690</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12930</th>\n",
       "      <td>0.025664</td>\n",
       "      <td>0.762040</td>\n",
       "      <td>0.091220</td>\n",
       "      <td>1.12630</td>\n",
       "      <td>-62.1550</td>\n",
       "      <td>0.025664</td>\n",
       "      <td>0.037809</td>\n",
       "      <td>0.213700</td>\n",
       "      <td>1.06870</td>\n",
       "      <td>0.162850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064291</td>\n",
       "      <td>0.157590</td>\n",
       "      <td>0.93571</td>\n",
       "      <td>0.245150</td>\n",
       "      <td>4.8872</td>\n",
       "      <td>3.66920</td>\n",
       "      <td>158.21000</td>\n",
       "      <td>2.30710</td>\n",
       "      <td>8.92490</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12931</th>\n",
       "      <td>0.069049</td>\n",
       "      <td>0.682320</td>\n",
       "      <td>0.069829</td>\n",
       "      <td>1.12710</td>\n",
       "      <td>-54.5330</td>\n",
       "      <td>0.069049</td>\n",
       "      <td>0.075024</td>\n",
       "      <td>0.424540</td>\n",
       "      <td>1.05380</td>\n",
       "      <td>0.289670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051048</td>\n",
       "      <td>0.238370</td>\n",
       "      <td>0.94895</td>\n",
       "      <td>0.458290</td>\n",
       "      <td>5.5224</td>\n",
       "      <td>3.90480</td>\n",
       "      <td>145.40000</td>\n",
       "      <td>2.51040</td>\n",
       "      <td>3.62500</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12932</th>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>-0.136190</td>\n",
       "      <td>0.60839</td>\n",
       "      <td>-18.4490</td>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.972030</td>\n",
       "      <td>1.01210</td>\n",
       "      <td>0.460840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011909</td>\n",
       "      <td>0.039866</td>\n",
       "      <td>0.98809</td>\n",
       "      <td>0.274140</td>\n",
       "      <td>73.5050</td>\n",
       "      <td>79.23700</td>\n",
       "      <td>31.26800</td>\n",
       "      <td>11.67300</td>\n",
       "      <td>5.14890</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12933</th>\n",
       "      <td>-0.013359</td>\n",
       "      <td>0.583540</td>\n",
       "      <td>-0.022650</td>\n",
       "      <td>0.92896</td>\n",
       "      <td>-42.2320</td>\n",
       "      <td>-0.013359</td>\n",
       "      <td>-0.015036</td>\n",
       "      <td>0.562890</td>\n",
       "      <td>0.98904</td>\n",
       "      <td>0.328470</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011082</td>\n",
       "      <td>-0.040671</td>\n",
       "      <td>1.01110</td>\n",
       "      <td>0.805920</td>\n",
       "      <td>10.5990</td>\n",
       "      <td>7.17400</td>\n",
       "      <td>94.09200</td>\n",
       "      <td>3.87920</td>\n",
       "      <td>1.75720</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12934</th>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.502760</td>\n",
       "      <td>0.439230</td>\n",
       "      <td>1.87360</td>\n",
       "      <td>9.7417</td>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.012022</td>\n",
       "      <td>0.983560</td>\n",
       "      <td>1.00830</td>\n",
       "      <td>0.494490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>0.99174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.4700</td>\n",
       "      <td>6.07590</td>\n",
       "      <td>51.01900</td>\n",
       "      <td>7.15420</td>\n",
       "      <td>62.00100</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12935</th>\n",
       "      <td>-0.041643</td>\n",
       "      <td>0.848100</td>\n",
       "      <td>-0.128520</td>\n",
       "      <td>0.57485</td>\n",
       "      <td>-121.9200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.036795</td>\n",
       "      <td>0.179010</td>\n",
       "      <td>0.42138</td>\n",
       "      <td>0.151820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232720</td>\n",
       "      <td>-0.274290</td>\n",
       "      <td>0.98788</td>\n",
       "      <td>3.593100</td>\n",
       "      <td>39.7030</td>\n",
       "      <td>3.14200</td>\n",
       "      <td>261.85000</td>\n",
       "      <td>1.39390</td>\n",
       "      <td>0.51005</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12936</th>\n",
       "      <td>0.014946</td>\n",
       "      <td>0.946480</td>\n",
       "      <td>0.032110</td>\n",
       "      <td>1.03630</td>\n",
       "      <td>-20.5810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015260</td>\n",
       "      <td>0.056357</td>\n",
       "      <td>2.96940</td>\n",
       "      <td>0.053341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>0.280210</td>\n",
       "      <td>0.97443</td>\n",
       "      <td>1.179200</td>\n",
       "      <td>15.0360</td>\n",
       "      <td>4.17410</td>\n",
       "      <td>108.64000</td>\n",
       "      <td>3.35990</td>\n",
       "      <td>35.11800</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12653 rows  65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Attr1     Attr2     Attr3      Attr4     Attr5     Attr6     Attr7  \\\n",
       "0      0.088238  0.554720  0.011340    1.02050  -66.5200  0.342040  0.109490   \n",
       "1     -0.006202  0.484650  0.232980    1.59980    6.1825  0.000000 -0.006202   \n",
       "2      0.130240  0.221420  0.577510    3.60820  120.0400  0.187640  0.162120   \n",
       "3     -0.089951  0.887000  0.269270    1.52220  -55.9920 -0.073957 -0.089951   \n",
       "4      0.048179  0.550410  0.107650    1.24370  -22.9590  0.000000  0.059280   \n",
       "5      0.231560  0.510470  0.472910    1.93930   15.1020  0.000000  0.287690   \n",
       "6      0.099486  0.599910  0.374890    1.65290   19.0360  0.210840  0.123950   \n",
       "7      0.078518  0.205460  0.103930    2.79390   77.7840  0.365150  0.093388   \n",
       "8      0.125040  0.354400  0.314190    2.71270   17.9420  0.305750  0.158430   \n",
       "9      0.293430  0.586490  0.083392    1.16940  -55.1520 -0.282110  0.293430   \n",
       "10     0.185140  0.339960  0.382720    2.12580   54.4260  0.630350  0.231410   \n",
       "11     0.090516  0.314290  0.424650    3.20710   24.8740  0.055652  0.105280   \n",
       "12     0.153290  0.515630  0.310280    1.60170   19.2110  0.000000  0.193670   \n",
       "13     0.011567  0.352370  0.073708    1.25830  -43.3650  0.000000  0.017250   \n",
       "14     0.373360  0.207180  0.633830    4.05930   92.2840  0.334810  0.373360   \n",
       "15     0.083120  0.293720  0.696570    3.53180   33.2490  0.000000  0.104690   \n",
       "16    -0.095972  0.936940 -0.053287    0.94312  -60.8890 -0.207520 -0.095972   \n",
       "17     0.130330  0.774240  0.210700    1.27210    7.3584  0.000000  0.164870   \n",
       "18     0.081442  0.701270  0.386960    1.65550   38.3050  0.000000  0.102900   \n",
       "19    -0.002132  0.250650  0.351250    2.47990   31.8620  0.124150  0.005896   \n",
       "20     0.057271  0.533640  0.399000    1.86640   -6.9193  0.060353  0.103500   \n",
       "21     0.136060  0.295880  0.476740    2.61130   70.8870  0.414150  0.168590   \n",
       "22     0.072197  0.792440  0.286910    1.40240   35.0650  0.000000  0.093762   \n",
       "23    -0.213680  0.795680  0.026812    1.04720  -72.4390 -0.295010 -0.213680   \n",
       "24     0.122500  0.903630  0.229350    1.94690   -5.2940  0.218960  0.122500   \n",
       "25     0.068391  0.314640  0.454090    2.93340   62.1340  0.000000  0.081912   \n",
       "26     0.161690  0.592430  0.402650    1.68040  -39.4520  0.000000  0.202520   \n",
       "27     0.126200  0.109310  0.859530   24.88400  818.9200  0.000000  0.126200   \n",
       "28     0.125010  0.408550  0.371830    2.01380   -7.9209  0.213890  0.156880   \n",
       "29     0.093660  0.540430  0.163510    1.30400  -20.9770  0.378830  0.115960   \n",
       "...         ...       ...       ...        ...       ...       ...       ...   \n",
       "12907  0.003802  0.141440  0.449040    4.17480   60.8960  0.003802 -0.004791   \n",
       "12908 -0.018516  0.328860  0.069952    1.25700  -29.2980  0.000000 -0.018781   \n",
       "12909  0.022830  0.711320 -0.060759    0.69120  -14.0810 -0.048958  0.024774   \n",
       "12910 -0.000153  0.505820 -0.421270    0.15367 -883.0300 -0.000153 -0.004183   \n",
       "12911 -0.289710  1.187400 -0.465320    0.53814 -223.2800 -0.289710 -0.289710   \n",
       "12912  0.009313  0.617610 -0.222330    0.60119  -62.6300  0.000000  0.014544   \n",
       "12913  0.167730  0.773890  0.166630    1.24140  -28.0390  0.000000  0.217420   \n",
       "12914 -0.330330  0.932660 -0.476930    0.48863 -350.8300 -0.330330 -0.330330   \n",
       "12915  0.046080  0.003824  0.996180  261.50000  174.3300  0.000000  0.046080   \n",
       "12916  0.133310  0.164510  0.400840    3.43660    2.8373  0.000000  0.169280   \n",
       "12917  0.038665  0.071884  0.488840    7.80040  221.0100  0.038665  0.045892   \n",
       "12918  0.001091  0.851600  0.003463    1.00860  -44.4670  0.086248  0.001091   \n",
       "12919 -0.091442  0.705500 -0.047216    0.92568   -7.2952  0.000000 -0.090374   \n",
       "12920  0.138090  3.335700 -2.364000    0.29128  -88.3820 -3.396300  0.138090   \n",
       "12921  0.098271  0.833300  0.000426    1.00050  -43.1910  0.000000  0.128380   \n",
       "12922  0.006404  0.674770 -0.008438    0.98668  -45.5990  0.000000  0.009958   \n",
       "12923  0.037953  0.122390  0.230840    2.88600   19.5810  0.037953  0.040082   \n",
       "12924  0.161170  0.072135  0.636490   12.02500  178.9000  0.000000  0.199860   \n",
       "12925  0.019432  0.616330  0.057718    1.10620 -162.8800  0.019432  0.008018   \n",
       "12926  0.022707  0.598550 -0.152680    0.65004  -55.4880  0.022707  0.023087   \n",
       "12927  0.015903  0.730140  0.083767    1.28650  -19.8070  0.000000  0.020462   \n",
       "12928 -0.051896  0.536300  0.054014    1.10610   -9.7422 -0.051896 -0.051896   \n",
       "12929 -0.031617  0.811750 -0.202300    0.61087  -44.0110  0.000000 -0.031617   \n",
       "12930  0.025664  0.762040  0.091220    1.12630  -62.1550  0.025664  0.037809   \n",
       "12931  0.069049  0.682320  0.069829    1.12710  -54.5330  0.069049  0.075024   \n",
       "12932  0.018371  0.474100 -0.136190    0.60839  -18.4490  0.018371  0.018371   \n",
       "12933 -0.013359  0.583540 -0.022650    0.92896  -42.2320 -0.013359 -0.015036   \n",
       "12934  0.006338  0.502760  0.439230    1.87360    9.7417  0.006338  0.012022   \n",
       "12935 -0.041643  0.848100 -0.128520    0.57485 -121.9200  0.000000 -0.036795   \n",
       "12936  0.014946  0.946480  0.032110    1.03630  -20.5810  0.000000  0.015260   \n",
       "\n",
       "            Attr8    Attr9    Attr10  ...      Attr56    Attr57   Attr58  \\\n",
       "0        0.577520  1.08810  0.320360  ...    0.080955  0.275430  0.91905   \n",
       "1        1.063400  1.27570  0.515350  ...   -0.028591 -0.012035  1.00470   \n",
       "2        3.059000  1.14150  0.677310  ...    0.123960  0.192290  0.87604   \n",
       "3        0.127400  1.27540  0.113000  ...    0.418840 -0.796020  0.59074   \n",
       "4        0.816820  1.51500  0.449590  ...    0.240400  0.107160  0.77048   \n",
       "5        0.958990  1.79150  0.489530  ...    0.184030  0.473030  0.83996   \n",
       "6        0.666900  1.09720  0.400090  ...    0.088581  0.248660  0.91142   \n",
       "7        3.867200  1.23220  0.794540  ...    0.188420  0.098822  0.81158   \n",
       "8        1.821700  1.23620  0.645600  ...    0.191090  0.193680  0.80891   \n",
       "9        0.705070  1.63760  0.413510  ...    0.348820  0.709610  0.72150   \n",
       "10       1.842300  1.14610  0.626310  ...    0.127450  0.295610  0.87255   \n",
       "11       2.149900  1.04880  0.675690  ...    0.046572  0.133960  0.95343   \n",
       "12       0.939360  4.28280  0.484370  ...    0.050271  0.316470  0.95493   \n",
       "13       1.837900  0.96593  0.647630  ...    0.039168  0.017860  0.98218   \n",
       "14       3.826600  2.47250  0.792820  ...    0.149640  0.470930  0.85002   \n",
       "15       2.404700  2.13300  0.706280  ...    0.053772  0.117690  0.95156   \n",
       "16       0.067299  1.79050  0.063055  ...   -0.006285 -1.522000  1.05320   \n",
       "17       0.291590  2.76570  0.225760  ...    0.079606  0.577280  0.94055   \n",
       "18       0.425990  1.97970  0.298730  ...    0.062960  0.272620  0.94841   \n",
       "19       2.337000  1.06370  0.585770  ...    0.059913 -0.003639  0.94009   \n",
       "20       0.873920  1.29320  0.466360  ...    0.116120  0.122810  0.92114   \n",
       "21       2.340300  1.15510  0.692440  ...    0.134270  0.196490  0.86573   \n",
       "22       0.261930  2.20960  0.207560  ...    0.072076  0.347830  0.95757   \n",
       "23       0.204890  0.88019  0.163020  ...   -0.136110 -1.310700  1.13610   \n",
       "24       0.024442  1.09290  0.022087  ...    0.084988  5.546500  0.91501   \n",
       "25       2.178200  1.21570  0.685360  ...   -0.059109  0.099789  0.94086   \n",
       "26       0.687960  2.34010  0.407570  ...    0.097682  0.396720  0.91425   \n",
       "27       8.148300  0.51703  0.890690  ...    0.269610  0.141690  0.73223   \n",
       "28       1.403200  1.09420  0.573270  ...    0.086059  0.218060  0.91394   \n",
       "29       0.834970  1.06470  0.451240  ...    0.060753  0.207560  0.93925   \n",
       "...           ...      ...       ...  ...         ...       ...      ...   \n",
       "12907    5.952200  0.99613  0.841880  ...   -0.003885  0.004516  1.00390   \n",
       "12908    2.040800  1.11420  0.671130  ...   -0.031889 -0.027589  1.01620   \n",
       "12909    0.405840  2.34630  0.288680  ...    0.004260  0.079084  0.97892   \n",
       "12910    0.451220  0.91488  0.228240  ...   -0.093040 -0.000669  1.09300   \n",
       "12911   -0.158310  0.81801 -0.187980  ...   -0.222480  1.541200  1.22250   \n",
       "12912    0.619150  2.08020  0.382390  ...    0.008170  0.024354  1.00190   \n",
       "12913    0.292170  4.88180  0.226110  ...    0.043982  0.741830  0.95170   \n",
       "12914    0.068734  0.70565  0.064105  ...   -0.417130 -5.153000  1.41710   \n",
       "12915  260.500000  2.08780  0.996180  ...    0.019324  0.046257  0.97800   \n",
       "12916    5.078600  3.65580  0.835490  ...    0.066065  0.159550  0.94701   \n",
       "12917   11.068000  1.07650  0.795600  ...    0.071070  0.048599  0.92893   \n",
       "12918    0.174290  1.02970  0.148420  ...   -0.198900  0.007349  0.97403   \n",
       "12919    0.417440  9.13450  0.294500  ...    0.000966 -0.310490  1.00740   \n",
       "12920   -0.700210  9.98520 -2.335700  ...    0.011347 -0.059122  0.97866   \n",
       "12921    0.200190  2.51440  0.166820  ...    0.150320  0.589090  0.85877   \n",
       "12922    0.482000  1.91090  0.325230  ...    0.180200  0.019690  0.81928   \n",
       "12923    6.886400  0.98825  0.842840  ...   -0.011887  0.045030  1.01190   \n",
       "12924   12.863000  1.29080  0.927860  ...    0.150780  0.173710  0.84152   \n",
       "12925   -0.232850  1.06560 -0.143510  ...    0.061544 -0.135400  0.93846   \n",
       "12926    0.666780  0.99912  0.399100  ...   -0.000878  0.056896  1.00090   \n",
       "12927    0.369610  2.19550  0.269860  ...    0.031304  0.058928  1.00160   \n",
       "12928    0.686440  0.98574  0.368140  ...   -0.014470 -0.140970  1.01450   \n",
       "12929    0.231900  3.03470  0.188250  ...    0.018387 -0.167950  0.98125   \n",
       "12930    0.213700  1.06870  0.162850  ...    0.064291  0.157590  0.93571   \n",
       "12931    0.424540  1.05380  0.289670  ...    0.051048  0.238370  0.94895   \n",
       "12932    0.972030  1.01210  0.460840  ...    0.011909  0.039866  0.98809   \n",
       "12933    0.562890  0.98904  0.328470  ...   -0.011082 -0.040671  1.01110   \n",
       "12934    0.983560  1.00830  0.494490  ...    0.008258  0.012817  0.99174   \n",
       "12935    0.179010  0.42138  0.151820  ...   -0.232720 -0.274290  0.98788   \n",
       "12936    0.056357  2.96940  0.053341  ...    0.015705  0.280210  0.97443   \n",
       "\n",
       "          Attr59    Attr60    Attr61      Attr62     Attr63     Attr64  class  \n",
       "0       0.002024    7.2711   4.73430   142.76000    2.55680    3.25970    0.0  \n",
       "1       0.152220    6.0911   3.27490   111.14000    3.28410    3.37000    0.0  \n",
       "2       0.000000    8.7934   2.98700    71.53100    5.10270    5.61880    0.0  \n",
       "3       2.878700    7.6524   3.33020   147.56000    2.47350    5.92990    0.0  \n",
       "4       0.139380   10.1180   4.09500   106.43000    3.42940    3.36220    0.0  \n",
       "5       0.014242    4.6071   4.92200   102.58000    3.55810   75.94100    0.0  \n",
       "6       0.064344    5.4655   2.47840   130.02000    2.80720   31.64500    0.0  \n",
       "7       0.185660   11.3790   3.16920    53.57500    6.81290    0.47096    0.0  \n",
       "8       0.264790    4.7737   5.74790    59.84100    6.09950    2.22740    0.0  \n",
       "9       0.021024    9.8135   5.51040   109.70000    3.32730    3.85820    0.0  \n",
       "10      0.000000   15.6010   3.40340    63.10700    5.78380    7.09020    0.0  \n",
       "11      0.180400    7.7104  14.58700    32.18200   11.34200    5.69810    0.0  \n",
       "12      0.000000   70.3090  14.21100    43.94500    8.30580   24.60200    0.0  \n",
       "13      0.097927    5.4821   5.72260   107.84000    3.38480    1.50710    0.0  \n",
       "14      0.000000   26.8820   3.51580    30.58500   11.93400   15.55200    0.0  \n",
       "15      0.000000    4.2886   6.04630    47.08000    7.75280   75.37100    0.0  \n",
       "16      0.000000    7.2933   2.90580   190.98000    1.91120   15.37400    0.0  \n",
       "17      0.000000   17.6060   3.59320   102.18000    3.57210  183.64000    0.0  \n",
       "18      0.000000   10.4640   5.92580   108.85000    3.35340   87.29500    0.0  \n",
       "19      0.022707    5.2793   4.82970    67.19000    5.43240    3.13420    0.0  \n",
       "20      0.000000    3.3421   3.20420   129.98000    2.80820    9.20480    0.0  \n",
       "21      0.000000    4.9278   3.47320    83.44400    4.37420    5.69200    0.0  \n",
       "22      0.121840   26.7060   2.63990   117.79000    3.09860        NaN    0.0  \n",
       "23      1.393700    4.1127   3.43340   202.03000    1.80660    2.53760    0.0  \n",
       "24     29.947000   14.6790  10.62900    46.93700    7.77640    3.56400    0.0  \n",
       "25      0.000000    5.1225   3.90330    70.51900    5.17590    3.90850    0.0  \n",
       "26      0.000000  131.2200   6.48770    92.30200    3.95440  420.19000    0.0  \n",
       "27      0.000000       NaN   0.64346    25.40600   14.36700    4.94850    0.0  \n",
       "28      0.072877    5.0881   8.55690    64.65400    5.64550    7.92110    0.0  \n",
       "29      0.005745    7.9988   6.20450    85.41800    4.27310    7.69520    0.0  \n",
       "...          ...       ...       ...         ...        ...        ...    ...  \n",
       "12907   0.000000    7.5574   4.51530    34.11700   10.69800    3.69500    2.0  \n",
       "12908   0.039008    9.3983   6.34080    89.16300    4.09360    1.69370    2.0  \n",
       "12909   0.654850   83.1690  25.43700    30.60800   11.92500    2.71560    2.0  \n",
       "12910   0.035313       NaN   2.26550  1094.00000    0.33365    0.17983    2.0  \n",
       "12911  -0.956790    3.7491   4.11090   354.26000    1.03030    2.28260    2.0  \n",
       "12912   0.071883   20.0000  10.08000    97.81900    3.73140    3.12880    2.0  \n",
       "12913   0.000000    9.3158  15.93100    51.61100    7.07220   34.13400    2.0  \n",
       "12914   0.000000    1.8878   7.52440   504.65000    0.72328    1.23940    2.0  \n",
       "12915   0.000000       NaN   6.41920     0.66856  545.95000        NaN    2.0  \n",
       "12916   0.000000    9.8104  21.32800    16.42500   22.22200    8.41110    2.0  \n",
       "12917   0.000000    2.1470   1.46790    53.73500    6.79260    1.11160    2.0  \n",
       "12918   2.033100    6.8515   4.10960   142.83000    2.55560    1.73460    2.0  \n",
       "12919   0.077583   72.8930  20.79000    25.38400   14.37900   22.18000    2.0  \n",
       "12920   0.000000  231.9000  12.65100   121.93000    2.99350  351.85000    2.0  \n",
       "12921   0.000000    9.2728   4.92070   116.48000    3.13360   12.74800    2.0  \n",
       "12922   0.000000    8.9144   4.99670   121.00000    3.01640    5.09650    2.0  \n",
       "12923   0.000000    3.8809  34.64100    60.06000    6.07730    1.15040    2.0  \n",
       "12924   0.000000   12.2700   6.31650    16.32500   22.35800    4.22300    2.0  \n",
       "12925  -0.508510    2.1849  16.65800   179.33000    2.03540    2.77270    2.0  \n",
       "12926   0.406580   15.1100  13.14500    83.39800    4.37660    2.66530    2.0  \n",
       "12927   1.470900   11.0380  12.56200    48.60000    7.51020    3.51900    2.0  \n",
       "12928   0.073630   21.6900  13.71200    66.22400    5.51160    6.42520    2.0  \n",
       "12929   1.101500   20.3810  20.39500    62.52900    5.83730    4.44690    2.0  \n",
       "12930   0.245150    4.8872   3.66920   158.21000    2.30710    8.92490    2.0  \n",
       "12931   0.458290    5.5224   3.90480   145.40000    2.51040    3.62500    2.0  \n",
       "12932   0.274140   73.5050  79.23700    31.26800   11.67300    5.14890    2.0  \n",
       "12933   0.805920   10.5990   7.17400    94.09200    3.87920    1.75720    2.0  \n",
       "12934   0.000000   10.4700   6.07590    51.01900    7.15420   62.00100    2.0  \n",
       "12935   3.593100   39.7030   3.14200   261.85000    1.39390    0.51005    2.0  \n",
       "12936   1.179200   15.0360   4.17410   108.64000    3.35990   35.11800    2.0  \n",
       "\n",
       "[12653 rows x 65 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.duplicated().any()\n",
    "dataset1.drop_duplicates(inplace=True, keep=False)\n",
    "dataset1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # based on https://pdfs.semanticscholar.org/daa0/f01f96a89fcfc5f41a2da67fb2a8966900ab.pdf \n",
    "# # we should pick these features:\n",
    "# Genetic_Based_Decision = dataset1[['cp','trestbps', 'restecg', 'thalach', 'ca', 'thal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous_vars = dataset1[['age', 'restecg', 'chol', 'thalach', 'oldpeak']] \n",
    "\n",
    "# def checkforoutlier(df):\n",
    "#     outliersnumbers = 0\n",
    "#     for column in df:\n",
    "#         for number in df[column]:\n",
    "#             if number < np.percentile(\n",
    "#                 df[column], 25)-(np.percentile(\n",
    "#                 df[column], 75)-np.percentile(\n",
    "#                 df[column], 25)) or number > np.percentile(\n",
    "#                 df[column], 75)+(np.percentile(\n",
    "#                 df[column], 75)-np.percentile(\n",
    "#                 df[column], 25)):\n",
    "#                     print(\"outlier: \", number, column)\n",
    "#                     outliersnumbers += 1\n",
    "#     return outliersnumbers, 'outliers. That is', round(float(outliersnumbers)/float(len(df[column]))*100, 0), 'percent of the total list'\n",
    "\n",
    "# print(checkforoutlier(continuous_vars))\n",
    "\n",
    "# # Thalach seems very high, but after research a heartbeat of 202 is possible: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Two variables are discrete/ordinal: ca (number of major vessels colored by fluoroscopy) and num (diagnosis of heart disease)\n",
    "# # Three can be directly viewed as 1 hot (because binary): 'sex':'male', 'fbs':'fasting blood sugar', 'exang':'exercise induced angina'\n",
    "\n",
    "# which leaves for one-hot encoding. problem is that the values aren't unique, so have to manually\n",
    "# # make extra columns:\n",
    "\n",
    "# dataset1[\"cp\"] = dataset1[\"cp\"].replace([1,2,3,4], [\"typical angina\", \"atypical angina\", \"non-angina\", \"asymptomatic angina\"])\n",
    "# dataset1[\"restecg\"] = dataset1[\"restecg\"].replace([0,1,2], [\"normalresecg\", \"ST-T wave abnormality\", \"left ventricular hypertrophy\"])\n",
    "# dataset1[\"slop\"] = dataset1[\"slop\"].replace([1,2,3], [\"upsloping\", \"flat\", \"downsloping\"])\n",
    "# dataset1[\"thal\"] = dataset1[\"thal\"].replace([3,6,7], [\"normalthal\", \"fixed defect\", \"reversible defect\"])\n",
    "\n",
    "# x = dataset1[['cp', 'restecg', 'slop', 'thal']]\n",
    "# for column in ['cp', 'restecg', 'slop', 'thal']:\n",
    "#     one_hot = pd.get_dummies(dataset1[column])\n",
    "#     dataset1 = dataset1.drop(column, axis=1)\n",
    "#     dataset1 = dataset1.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b689622a-a475-40a8-bd33-e2b5e018d528",
    "_uuid": "2e13a97aaee5c269ff8f21fd7b66135927b66156"
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing:\n",
    "dataset1.dropna(inplace=True, axis=0, how=\"any\")\n",
    "Y=dataset1[\"class\"]\n",
    "dataset1 = dataset1.drop(\"class\", axis=1)\n",
    "X=dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "2dacb7c2-6d4e-4a17-b5cb-d6ecd821c923",
    "_uuid": "c3355ae680b60b0daa713153f05b3709193af7f6"
   },
   "outputs": [],
   "source": [
    "# evaluate the model by splitting into train and test sets  #Edit by ryan, we aim to do 3 traditional sets in the end, this first split is 80/20\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "5065d9ba-be53-4431-bb86-152ee1673550",
    "_uuid": "00229a787842594dee105c79de5871548613a045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 4151, 1.0: 72, 2.0: 23})\n",
      "Counter({0.0: 1786, 1.0: 28, 2.0: 7})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "list1 = []\n",
    "for i in labels_train:\n",
    "    list1.append(i)\n",
    "counter=collections.Counter(list1)\n",
    "print(counter)\n",
    "\n",
    "list2 = []\n",
    "for i in labels_test:\n",
    "    list2.append(i)\n",
    "counter=collections.Counter(list2)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69985165650239\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print(len(features_train)/(len(features_train)+ len(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_df = pd.DataFrame(features_train)\n",
    "features_train_df.to_csv('features_train.csv', index=False)\n",
    "\n",
    "features_test_df = pd.DataFrame(features_test)\n",
    "features_test_df.to_csv('features_test.csv', index=False)\n",
    "\n",
    "labels_train_df = pd.DataFrame(labels_train)\n",
    "labels_train_df.to_csv('labels_train.csv', index=False)\n",
    "\n",
    "labels_test_df = pd.DataFrame(labels_test)\n",
    "labels_test_df.to_csv('labels_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Scores based on XGBoost (by David)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.52221\ttest-mlogloss:1.52244\n",
      "[1]\ttrain-mlogloss:1.44281\ttest-mlogloss:1.44323\n",
      "[2]\ttrain-mlogloss:1.37032\ttest-mlogloss:1.371\n",
      "[3]\ttrain-mlogloss:1.30356\ttest-mlogloss:1.3044\n",
      "[4]\ttrain-mlogloss:1.24178\ttest-mlogloss:1.24284\n",
      "[5]\ttrain-mlogloss:1.1845\ttest-mlogloss:1.18559\n",
      "[6]\ttrain-mlogloss:1.13109\ttest-mlogloss:1.13226\n",
      "[7]\ttrain-mlogloss:1.08104\ttest-mlogloss:1.0827\n",
      "[8]\ttrain-mlogloss:1.03432\ttest-mlogloss:1.03618\n",
      "[9]\ttrain-mlogloss:0.990253\ttest-mlogloss:0.992276\n",
      "[10]\ttrain-mlogloss:0.948767\ttest-mlogloss:0.951013\n",
      "[11]\ttrain-mlogloss:0.909649\ttest-mlogloss:0.912095\n",
      "[12]\ttrain-mlogloss:0.872715\ttest-mlogloss:0.875334\n",
      "[13]\ttrain-mlogloss:0.837832\ttest-mlogloss:0.840575\n",
      "[14]\ttrain-mlogloss:0.804626\ttest-mlogloss:0.807755\n",
      "[15]\ttrain-mlogloss:0.773068\ttest-mlogloss:0.776475\n",
      "[16]\ttrain-mlogloss:0.743256\ttest-mlogloss:0.746826\n",
      "[17]\ttrain-mlogloss:0.71481\ttest-mlogloss:0.718545\n",
      "[18]\ttrain-mlogloss:0.687644\ttest-mlogloss:0.691688\n",
      "[19]\ttrain-mlogloss:0.661842\ttest-mlogloss:0.666165\n",
      "[20]\ttrain-mlogloss:0.637107\ttest-mlogloss:0.641811\n",
      "[21]\ttrain-mlogloss:0.613753\ttest-mlogloss:0.618592\n",
      "[22]\ttrain-mlogloss:0.5913\ttest-mlogloss:0.596541\n",
      "[23]\ttrain-mlogloss:0.570006\ttest-mlogloss:0.575455\n",
      "[24]\ttrain-mlogloss:0.549556\ttest-mlogloss:0.555249\n",
      "[25]\ttrain-mlogloss:0.529981\ttest-mlogloss:0.535945\n",
      "[26]\ttrain-mlogloss:0.511325\ttest-mlogloss:0.517614\n",
      "[27]\ttrain-mlogloss:0.493335\ttest-mlogloss:0.49986\n",
      "[28]\ttrain-mlogloss:0.476113\ttest-mlogloss:0.482897\n",
      "[29]\ttrain-mlogloss:0.45954\ttest-mlogloss:0.466594\n",
      "[30]\ttrain-mlogloss:0.4439\ttest-mlogloss:0.451073\n",
      "[31]\ttrain-mlogloss:0.428767\ttest-mlogloss:0.436197\n",
      "[32]\ttrain-mlogloss:0.414326\ttest-mlogloss:0.422104\n",
      "[33]\ttrain-mlogloss:0.400506\ttest-mlogloss:0.40873\n",
      "[34]\ttrain-mlogloss:0.387196\ttest-mlogloss:0.395605\n",
      "[35]\ttrain-mlogloss:0.374467\ttest-mlogloss:0.383131\n",
      "[36]\ttrain-mlogloss:0.36215\ttest-mlogloss:0.371029\n",
      "[37]\ttrain-mlogloss:0.350384\ttest-mlogloss:0.359426\n",
      "[38]\ttrain-mlogloss:0.338959\ttest-mlogloss:0.348242\n",
      "[39]\ttrain-mlogloss:0.328058\ttest-mlogloss:0.337453\n",
      "[40]\ttrain-mlogloss:0.317511\ttest-mlogloss:0.32733\n",
      "[41]\ttrain-mlogloss:0.307486\ttest-mlogloss:0.317529\n",
      "[42]\ttrain-mlogloss:0.297851\ttest-mlogloss:0.308107\n",
      "[43]\ttrain-mlogloss:0.288576\ttest-mlogloss:0.299083\n",
      "[44]\ttrain-mlogloss:0.279654\ttest-mlogloss:0.290344\n",
      "[45]\ttrain-mlogloss:0.271043\ttest-mlogloss:0.282051\n",
      "[46]\ttrain-mlogloss:0.262686\ttest-mlogloss:0.27402\n",
      "[47]\ttrain-mlogloss:0.254801\ttest-mlogloss:0.266392\n",
      "[48]\ttrain-mlogloss:0.24712\ttest-mlogloss:0.258924\n",
      "[49]\ttrain-mlogloss:0.239774\ttest-mlogloss:0.251888\n",
      "[50]\ttrain-mlogloss:0.232675\ttest-mlogloss:0.245024\n",
      "[51]\ttrain-mlogloss:0.225828\ttest-mlogloss:0.238433\n",
      "[52]\ttrain-mlogloss:0.219249\ttest-mlogloss:0.232113\n",
      "[53]\ttrain-mlogloss:0.212962\ttest-mlogloss:0.226082\n",
      "[54]\ttrain-mlogloss:0.20685\ttest-mlogloss:0.220248\n",
      "[55]\ttrain-mlogloss:0.200878\ttest-mlogloss:0.214549\n",
      "[56]\ttrain-mlogloss:0.195214\ttest-mlogloss:0.209242\n",
      "[57]\ttrain-mlogloss:0.189741\ttest-mlogloss:0.204086\n",
      "[58]\ttrain-mlogloss:0.184456\ttest-mlogloss:0.199155\n",
      "[59]\ttrain-mlogloss:0.179383\ttest-mlogloss:0.194346\n",
      "[60]\ttrain-mlogloss:0.174441\ttest-mlogloss:0.189655\n",
      "[61]\ttrain-mlogloss:0.169577\ttest-mlogloss:0.185109\n",
      "[62]\ttrain-mlogloss:0.165011\ttest-mlogloss:0.180818\n",
      "[63]\ttrain-mlogloss:0.160556\ttest-mlogloss:0.176746\n",
      "[64]\ttrain-mlogloss:0.15618\ttest-mlogloss:0.172768\n",
      "[65]\ttrain-mlogloss:0.151904\ttest-mlogloss:0.169164\n",
      "[66]\ttrain-mlogloss:0.147943\ttest-mlogloss:0.165571\n",
      "[67]\ttrain-mlogloss:0.144159\ttest-mlogloss:0.162102\n",
      "[68]\ttrain-mlogloss:0.140402\ttest-mlogloss:0.158718\n",
      "[69]\ttrain-mlogloss:0.136755\ttest-mlogloss:0.155514\n",
      "[70]\ttrain-mlogloss:0.133279\ttest-mlogloss:0.152298\n",
      "[71]\ttrain-mlogloss:0.129729\ttest-mlogloss:0.149304\n",
      "[72]\ttrain-mlogloss:0.126406\ttest-mlogloss:0.146281\n",
      "[73]\ttrain-mlogloss:0.123225\ttest-mlogloss:0.143472\n",
      "[74]\ttrain-mlogloss:0.12014\ttest-mlogloss:0.140666\n",
      "[75]\ttrain-mlogloss:0.117127\ttest-mlogloss:0.138136\n",
      "[76]\ttrain-mlogloss:0.114302\ttest-mlogloss:0.135598\n",
      "[77]\ttrain-mlogloss:0.111557\ttest-mlogloss:0.133154\n",
      "[78]\ttrain-mlogloss:0.108881\ttest-mlogloss:0.130873\n",
      "[79]\ttrain-mlogloss:0.106365\ttest-mlogloss:0.12863\n",
      "[80]\ttrain-mlogloss:0.103816\ttest-mlogloss:0.126528\n",
      "[81]\ttrain-mlogloss:0.101349\ttest-mlogloss:0.124371\n",
      "[82]\ttrain-mlogloss:0.098873\ttest-mlogloss:0.122376\n",
      "[83]\ttrain-mlogloss:0.096464\ttest-mlogloss:0.120447\n",
      "[84]\ttrain-mlogloss:0.094215\ttest-mlogloss:0.118558\n",
      "[85]\ttrain-mlogloss:0.091931\ttest-mlogloss:0.116711\n",
      "[86]\ttrain-mlogloss:0.089776\ttest-mlogloss:0.114934\n",
      "[87]\ttrain-mlogloss:0.087765\ttest-mlogloss:0.113324\n",
      "[88]\ttrain-mlogloss:0.085787\ttest-mlogloss:0.111757\n",
      "[89]\ttrain-mlogloss:0.083756\ttest-mlogloss:0.110115\n",
      "[90]\ttrain-mlogloss:0.081889\ttest-mlogloss:0.108618\n",
      "[91]\ttrain-mlogloss:0.08004\ttest-mlogloss:0.10714\n",
      "[92]\ttrain-mlogloss:0.078108\ttest-mlogloss:0.105659\n",
      "[93]\ttrain-mlogloss:0.076395\ttest-mlogloss:0.104343\n",
      "[94]\ttrain-mlogloss:0.074678\ttest-mlogloss:0.10305\n",
      "[95]\ttrain-mlogloss:0.073149\ttest-mlogloss:0.101813\n",
      "[96]\ttrain-mlogloss:0.071593\ttest-mlogloss:0.100648\n",
      "[97]\ttrain-mlogloss:0.070078\ttest-mlogloss:0.09948\n",
      "[98]\ttrain-mlogloss:0.068571\ttest-mlogloss:0.098342\n",
      "[99]\ttrain-mlogloss:0.067119\ttest-mlogloss:0.097285\n",
      "[100]\ttrain-mlogloss:0.065715\ttest-mlogloss:0.09626\n",
      "[101]\ttrain-mlogloss:0.064363\ttest-mlogloss:0.09529\n",
      "[102]\ttrain-mlogloss:0.06296\ttest-mlogloss:0.094255\n",
      "[103]\ttrain-mlogloss:0.061633\ttest-mlogloss:0.093388\n",
      "[104]\ttrain-mlogloss:0.060356\ttest-mlogloss:0.092563\n",
      "[105]\ttrain-mlogloss:0.059141\ttest-mlogloss:0.091734\n",
      "[106]\ttrain-mlogloss:0.057931\ttest-mlogloss:0.090923\n",
      "[107]\ttrain-mlogloss:0.056829\ttest-mlogloss:0.090162\n",
      "[108]\ttrain-mlogloss:0.055714\ttest-mlogloss:0.089441\n",
      "[109]\ttrain-mlogloss:0.054669\ttest-mlogloss:0.088668\n",
      "[110]\ttrain-mlogloss:0.05358\ttest-mlogloss:0.087857\n",
      "[111]\ttrain-mlogloss:0.052543\ttest-mlogloss:0.087137\n",
      "[112]\ttrain-mlogloss:0.051537\ttest-mlogloss:0.086524\n",
      "[113]\ttrain-mlogloss:0.050592\ttest-mlogloss:0.085922\n",
      "[114]\ttrain-mlogloss:0.049684\ttest-mlogloss:0.085381\n",
      "[115]\ttrain-mlogloss:0.048612\ttest-mlogloss:0.084682\n",
      "[116]\ttrain-mlogloss:0.047605\ttest-mlogloss:0.083989\n",
      "[117]\ttrain-mlogloss:0.046796\ttest-mlogloss:0.083491\n",
      "[118]\ttrain-mlogloss:0.045983\ttest-mlogloss:0.082981\n",
      "[119]\ttrain-mlogloss:0.045157\ttest-mlogloss:0.082487\n",
      "[120]\ttrain-mlogloss:0.04436\ttest-mlogloss:0.081964\n",
      "[121]\ttrain-mlogloss:0.04357\ttest-mlogloss:0.081495\n",
      "[122]\ttrain-mlogloss:0.042868\ttest-mlogloss:0.081083\n",
      "[123]\ttrain-mlogloss:0.04205\ttest-mlogloss:0.080544\n",
      "[124]\ttrain-mlogloss:0.041334\ttest-mlogloss:0.080125\n",
      "[125]\ttrain-mlogloss:0.040708\ttest-mlogloss:0.079695\n",
      "[126]\ttrain-mlogloss:0.040072\ttest-mlogloss:0.079268\n",
      "[127]\ttrain-mlogloss:0.039338\ttest-mlogloss:0.078834\n",
      "[128]\ttrain-mlogloss:0.038709\ttest-mlogloss:0.078473\n",
      "[129]\ttrain-mlogloss:0.038087\ttest-mlogloss:0.078108\n",
      "[130]\ttrain-mlogloss:0.037432\ttest-mlogloss:0.077786\n",
      "[131]\ttrain-mlogloss:0.036832\ttest-mlogloss:0.077523\n",
      "[132]\ttrain-mlogloss:0.036259\ttest-mlogloss:0.07721\n",
      "[133]\ttrain-mlogloss:0.03564\ttest-mlogloss:0.076867\n",
      "[134]\ttrain-mlogloss:0.035106\ttest-mlogloss:0.076523\n",
      "[135]\ttrain-mlogloss:0.034544\ttest-mlogloss:0.076231\n",
      "[136]\ttrain-mlogloss:0.034017\ttest-mlogloss:0.076028\n",
      "[137]\ttrain-mlogloss:0.033523\ttest-mlogloss:0.075782\n",
      "[138]\ttrain-mlogloss:0.033033\ttest-mlogloss:0.075581\n",
      "[139]\ttrain-mlogloss:0.032596\ttest-mlogloss:0.075436\n",
      "[140]\ttrain-mlogloss:0.032172\ttest-mlogloss:0.075253\n",
      "[141]\ttrain-mlogloss:0.031676\ttest-mlogloss:0.075087\n",
      "[142]\ttrain-mlogloss:0.031193\ttest-mlogloss:0.07487\n",
      "[143]\ttrain-mlogloss:0.030733\ttest-mlogloss:0.074671\n",
      "[144]\ttrain-mlogloss:0.030349\ttest-mlogloss:0.074571\n",
      "[145]\ttrain-mlogloss:0.029943\ttest-mlogloss:0.074387\n",
      "[146]\ttrain-mlogloss:0.029509\ttest-mlogloss:0.074214\n",
      "[147]\ttrain-mlogloss:0.02914\ttest-mlogloss:0.074105\n",
      "[148]\ttrain-mlogloss:0.028746\ttest-mlogloss:0.073782\n",
      "[149]\ttrain-mlogloss:0.028316\ttest-mlogloss:0.073608\n",
      "[150]\ttrain-mlogloss:0.027878\ttest-mlogloss:0.073395\n",
      "[151]\ttrain-mlogloss:0.027503\ttest-mlogloss:0.073282\n",
      "[152]\ttrain-mlogloss:0.027159\ttest-mlogloss:0.073082\n",
      "[153]\ttrain-mlogloss:0.026781\ttest-mlogloss:0.072906\n",
      "[154]\ttrain-mlogloss:0.02645\ttest-mlogloss:0.07273\n",
      "[155]\ttrain-mlogloss:0.02612\ttest-mlogloss:0.072644\n",
      "[156]\ttrain-mlogloss:0.025744\ttest-mlogloss:0.072447\n",
      "[157]\ttrain-mlogloss:0.025437\ttest-mlogloss:0.072402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158]\ttrain-mlogloss:0.02508\ttest-mlogloss:0.072312\n",
      "[159]\ttrain-mlogloss:0.024756\ttest-mlogloss:0.072195\n",
      "[160]\ttrain-mlogloss:0.024424\ttest-mlogloss:0.072145\n",
      "[161]\ttrain-mlogloss:0.024129\ttest-mlogloss:0.072112\n",
      "[162]\ttrain-mlogloss:0.023768\ttest-mlogloss:0.071986\n",
      "[163]\ttrain-mlogloss:0.023466\ttest-mlogloss:0.071893\n",
      "[164]\ttrain-mlogloss:0.023192\ttest-mlogloss:0.071752\n",
      "[165]\ttrain-mlogloss:0.022911\ttest-mlogloss:0.071627\n",
      "[166]\ttrain-mlogloss:0.022616\ttest-mlogloss:0.071623\n",
      "[167]\ttrain-mlogloss:0.022364\ttest-mlogloss:0.071493\n",
      "[168]\ttrain-mlogloss:0.022108\ttest-mlogloss:0.071453\n",
      "[169]\ttrain-mlogloss:0.021841\ttest-mlogloss:0.071334\n",
      "[170]\ttrain-mlogloss:0.021596\ttest-mlogloss:0.071208\n",
      "[171]\ttrain-mlogloss:0.02133\ttest-mlogloss:0.071202\n",
      "[172]\ttrain-mlogloss:0.021092\ttest-mlogloss:0.071119\n",
      "[173]\ttrain-mlogloss:0.020835\ttest-mlogloss:0.071021\n",
      "[174]\ttrain-mlogloss:0.020612\ttest-mlogloss:0.070899\n",
      "[175]\ttrain-mlogloss:0.020378\ttest-mlogloss:0.07079\n",
      "[176]\ttrain-mlogloss:0.020109\ttest-mlogloss:0.070674\n",
      "[177]\ttrain-mlogloss:0.019907\ttest-mlogloss:0.070615\n",
      "[178]\ttrain-mlogloss:0.01971\ttest-mlogloss:0.070526\n",
      "[179]\ttrain-mlogloss:0.019492\ttest-mlogloss:0.070449\n",
      "[180]\ttrain-mlogloss:0.019269\ttest-mlogloss:0.070494\n",
      "[181]\ttrain-mlogloss:0.019039\ttest-mlogloss:0.070534\n",
      "[182]\ttrain-mlogloss:0.018846\ttest-mlogloss:0.070493\n",
      "[183]\ttrain-mlogloss:0.018651\ttest-mlogloss:0.070447\n",
      "[184]\ttrain-mlogloss:0.018437\ttest-mlogloss:0.070371\n",
      "[185]\ttrain-mlogloss:0.01824\ttest-mlogloss:0.070264\n",
      "[186]\ttrain-mlogloss:0.01807\ttest-mlogloss:0.070244\n",
      "[187]\ttrain-mlogloss:0.017869\ttest-mlogloss:0.070196\n",
      "[188]\ttrain-mlogloss:0.017644\ttest-mlogloss:0.070152\n",
      "[189]\ttrain-mlogloss:0.017457\ttest-mlogloss:0.070142\n",
      "[190]\ttrain-mlogloss:0.017288\ttest-mlogloss:0.070138\n",
      "[191]\ttrain-mlogloss:0.017118\ttest-mlogloss:0.070124\n",
      "[192]\ttrain-mlogloss:0.016954\ttest-mlogloss:0.070087\n",
      "[193]\ttrain-mlogloss:0.016785\ttest-mlogloss:0.070074\n",
      "[194]\ttrain-mlogloss:0.016584\ttest-mlogloss:0.070003\n",
      "[195]\ttrain-mlogloss:0.016409\ttest-mlogloss:0.070026\n",
      "[196]\ttrain-mlogloss:0.016242\ttest-mlogloss:0.069984\n",
      "[197]\ttrain-mlogloss:0.016096\ttest-mlogloss:0.069949\n",
      "[198]\ttrain-mlogloss:0.015916\ttest-mlogloss:0.069862\n",
      "[199]\ttrain-mlogloss:0.015733\ttest-mlogloss:0.069907\n",
      "[200]\ttrain-mlogloss:0.015545\ttest-mlogloss:0.069882\n",
      "[201]\ttrain-mlogloss:0.015377\ttest-mlogloss:0.069877\n",
      "[202]\ttrain-mlogloss:0.015241\ttest-mlogloss:0.069836\n",
      "[203]\ttrain-mlogloss:0.015113\ttest-mlogloss:0.069851\n",
      "[204]\ttrain-mlogloss:0.014998\ttest-mlogloss:0.069817\n",
      "[205]\ttrain-mlogloss:0.014853\ttest-mlogloss:0.069794\n",
      "[206]\ttrain-mlogloss:0.014707\ttest-mlogloss:0.06978\n",
      "[207]\ttrain-mlogloss:0.014588\ttest-mlogloss:0.069742\n",
      "[208]\ttrain-mlogloss:0.014423\ttest-mlogloss:0.069789\n",
      "[209]\ttrain-mlogloss:0.014295\ttest-mlogloss:0.069721\n",
      "[210]\ttrain-mlogloss:0.014183\ttest-mlogloss:0.069691\n",
      "[211]\ttrain-mlogloss:0.014028\ttest-mlogloss:0.069771\n",
      "[212]\ttrain-mlogloss:0.013871\ttest-mlogloss:0.069778\n",
      "[213]\ttrain-mlogloss:0.013752\ttest-mlogloss:0.069792\n",
      "[214]\ttrain-mlogloss:0.013617\ttest-mlogloss:0.069861\n",
      "[215]\ttrain-mlogloss:0.013508\ttest-mlogloss:0.069897\n",
      "[216]\ttrain-mlogloss:0.013403\ttest-mlogloss:0.069895\n",
      "[217]\ttrain-mlogloss:0.013306\ttest-mlogloss:0.069929\n",
      "[218]\ttrain-mlogloss:0.0132\ttest-mlogloss:0.069983\n",
      "[219]\ttrain-mlogloss:0.013104\ttest-mlogloss:0.069924\n",
      "[220]\ttrain-mlogloss:0.012961\ttest-mlogloss:0.06996\n",
      "[221]\ttrain-mlogloss:0.01285\ttest-mlogloss:0.069925\n",
      "[222]\ttrain-mlogloss:0.012724\ttest-mlogloss:0.06987\n",
      "[223]\ttrain-mlogloss:0.012623\ttest-mlogloss:0.069873\n",
      "[224]\ttrain-mlogloss:0.012507\ttest-mlogloss:0.069839\n",
      "[225]\ttrain-mlogloss:0.01238\ttest-mlogloss:0.069897\n",
      "[226]\ttrain-mlogloss:0.01225\ttest-mlogloss:0.069855\n",
      "[227]\ttrain-mlogloss:0.012137\ttest-mlogloss:0.06984\n",
      "[228]\ttrain-mlogloss:0.012052\ttest-mlogloss:0.069828\n",
      "[229]\ttrain-mlogloss:0.011947\ttest-mlogloss:0.069892\n",
      "[230]\ttrain-mlogloss:0.011846\ttest-mlogloss:0.069835\n",
      "[231]\ttrain-mlogloss:0.011763\ttest-mlogloss:0.069858\n",
      "[232]\ttrain-mlogloss:0.011674\ttest-mlogloss:0.069855\n",
      "[233]\ttrain-mlogloss:0.01158\ttest-mlogloss:0.069801\n",
      "[234]\ttrain-mlogloss:0.011477\ttest-mlogloss:0.069871\n",
      "[235]\ttrain-mlogloss:0.011366\ttest-mlogloss:0.069882\n",
      "[236]\ttrain-mlogloss:0.011276\ttest-mlogloss:0.069832\n",
      "[237]\ttrain-mlogloss:0.011166\ttest-mlogloss:0.069807\n",
      "[238]\ttrain-mlogloss:0.011088\ttest-mlogloss:0.069755\n",
      "[239]\ttrain-mlogloss:0.01099\ttest-mlogloss:0.069797\n",
      "[240]\ttrain-mlogloss:0.010899\ttest-mlogloss:0.06983\n",
      "[241]\ttrain-mlogloss:0.010818\ttest-mlogloss:0.069867\n",
      "[242]\ttrain-mlogloss:0.010744\ttest-mlogloss:0.069901\n",
      "[243]\ttrain-mlogloss:0.01066\ttest-mlogloss:0.069938\n",
      "[244]\ttrain-mlogloss:0.010582\ttest-mlogloss:0.069949\n",
      "[245]\ttrain-mlogloss:0.010494\ttest-mlogloss:0.070021\n",
      "[246]\ttrain-mlogloss:0.010414\ttest-mlogloss:0.070032\n",
      "[247]\ttrain-mlogloss:0.010342\ttest-mlogloss:0.070008\n",
      "[248]\ttrain-mlogloss:0.010265\ttest-mlogloss:0.069966\n",
      "[249]\ttrain-mlogloss:0.010181\ttest-mlogloss:0.069972\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import operator\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4,\n",
    "    'silent': 1,\n",
    "    'num_class': 5\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(features_train, labels_train)\n",
    "dtest = xgb.DMatrix(features_test, labels_test)\n",
    "\n",
    "num_boost_rounds = 250\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'test')]\n",
    "\n",
    "# train model\n",
    "xgb_model = xgb.train(xgb_params, dtrain, num_boost_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1dde8040240>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dde33c6cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dde88ad128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importance = xgb_model.get_fscore()\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "importance_df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "\n",
    "# Plot Feature Importance\n",
    "plt.figure()\n",
    "importance_df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(dataframe, importance_scores_df, threshold=0.4):\n",
    "    normalized_df = importance_scores_df.copy()\n",
    "    normalized_df['fscore'] = (importance_scores_df['fscore'] - importance_scores_df['fscore'].min())/(importance_scores_df['fscore'].max()-importance_scores_df['fscore'].min())\n",
    "    normalized_df = normalized_df[normalized_df['fscore'] >= threshold]\n",
    "    new_dataframe = dataframe.filter(items=normalized_df['feature'].tolist())\n",
    "    return new_dataframe\n",
    "\n",
    "features_train_009_df = feature_selection(features_train_df, importance_df, threshold=0.09)\n",
    "features_train_009 = features_train_009_df.as_matrix()\n",
    "features_train_04_df = feature_selection(features_train_df, importance_df, threshold=0.4)\n",
    "features_train_04 = features_train_04_df.as_matrix()\n",
    "\n",
    "features_test_009_df = feature_selection(features_test_df, importance_df, threshold=0.09)\n",
    "features_test_009 = features_test_009_df.as_matrix()\n",
    "features_test_04_df = feature_selection(features_test_df, importance_df, threshold=0.4)\n",
    "features_test_04 = features_test_04_df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components for Dataset with threshold=0.4 is 4\n",
      "Number of components for Dataset with threshold=0.09 is 15\n"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "def eigen_scores(dataframe):\n",
    "    dataframe-=np.mean(dataframe, axis=0)\n",
    "    dataframe/=np.std(dataframe, axis=0)\n",
    "    cov_mat=np.cov(dataframe, rowvar=False)\n",
    "    evals, evecs = np.linalg.eigh(cov_mat)\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evecs = evecs[:,idx]\n",
    "    evals = evals[idx]\n",
    "    return evals, evecs\n",
    "\n",
    "def my_pca(dataframe, n_components):\n",
    "    pca=PCA(n_components=n_components)\n",
    "    return pca.fit_transform(dataframe)\n",
    "\n",
    "\n",
    "\n",
    "eigenvalues, eigenvectors = eigen_scores(features_test_04.copy())\n",
    "n_components_04 = len(np.where(eigenvalues >= 1)[0])\n",
    "print(\"Number of components for Dataset with threshold=0.4 is {}\".format(n_components_04))\n",
    "\n",
    "features_train_04_pca = my_pca(features_train_04, n_components_04)\n",
    "features_test_04_pca = my_pca(features_test_04, n_components_04)\n",
    "\n",
    "eigenvalues, eigenvectors = eigen_scores(features_test_009.copy())\n",
    "n_components_009 = len(np.where(eigenvalues >= 1)[0])\n",
    "print(\"Number of components for Dataset with threshold=0.09 is {}\".format(n_components_009))\n",
    "\n",
    "features_train_009_pca = my_pca(features_train_009, n_components_009)\n",
    "features_test_009_pca = my_pca(features_test_009, n_components_009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling And Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an relatively small dataset. Therefore, we should do our feature selection based on a cross-\n",
    "validated set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE for SVM - Balancing only on the training set, not the validation set  [This is for the traditional training -not the cross validated one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #further divide the 'traditional' non-cross set into training 80/20  for pure training and cross validation  \n",
    "# features_train_notoversampled, features_validate, labels_train_notoversampled, labels_validate = train_test_split(features_train, labels_train, test_size = .2, random_state=0)\n",
    "\n",
    "# sm = SMOTE(random_state=0, ratio = 1.0, kind= 'svm' )\n",
    "# #x_train_res, y_train_res = sm.fit_sample(x_train, y_train)\n",
    "# features_train_oversampled, labels_train_oversampled = sm.fit_sample(features_train_notoversampled, labels_train_notoversampled)\n",
    "\n",
    "# #re-enter into original variables\n",
    "# ##features_train = features_train_oversampled\n",
    "# ##labels_train = labels_train_oversampled\n",
    "\n",
    "# #Below 2 lines if we want to want to force the array back into dataframe    \n",
    "# ##features_train = pd.DataFrame(features_train_oversampled,columns=[\"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\"thalach\",\"exang\",\"oldpeak\",\"slop\",\"ca\",\"thal\"])\n",
    "# ##labels_train = pd.DataFrame(labels_train_oversampled,columns=[\"pred_attribute\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler (by David)\n",
    "SVC Models are only any good when the data is scaled. Lets scale the data and build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing.data import QuantileTransformer\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "Standard_scaler = StandardScaler()\n",
    "Robust_scaler = preprocessing.RobustScaler(quantile_range=(25, 75))\n",
    "Quantile_scalar = preprocessing.QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "features_train = Standard_scaler.fit_transform(features_train)\n",
    "features_test = Standard_scaler.transform(features_test)\n",
    "\n",
    "features_train_009 = Standard_scaler.fit_transform(features_train_009)\n",
    "features_test_009 = Standard_scaler.transform(features_test_009)\n",
    "\n",
    "features_train_04 = Standard_scaler.fit_transform(features_train_04)\n",
    "features_test_04 = Standard_scaler.transform(features_test_04)\n",
    "\n",
    "features_train_009_pca = Standard_scaler.fit_transform(features_train_009_pca)\n",
    "features_test_009_pca = Standard_scaler.transform(features_test_009_pca)\n",
    "\n",
    "features_train_04_pca = Standard_scaler.fit_transform(features_train_04_pca)\n",
    "features_test_04_pca = Standard_scaler.transform(features_test_04_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing only on the training set, not the validation set\n",
    "Unfortunately SMOTE categorial implementation is not really implemented\n",
    "We will do undersampling of majority AND oversampling of minority -> Done using external program SPSS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export data to files for external program to balance the data\n",
    "\n",
    "# # without features selection\n",
    "# train = np.concatenate((features_train, labels_train.values.reshape((-1, 1))), axis=1)\n",
    "# train_df = pd.DataFrame(train) \n",
    "# train_df.to_csv(\"Undata/train_NoEng_NB.csv\", index=False)\n",
    "\n",
    "# # train_009\n",
    "# train_009 = np.concatenate((features_train_009, labels_train.values.reshape((-1, 1))), axis=1)\n",
    "# train_009_df = pd.DataFrame(train_009) \n",
    "# train_009_df.to_csv(\"Undata/train_009_NB.csv\", index=False) \n",
    "    \n",
    "# # train_04 \n",
    "# train_04 = np.concatenate((features_train_04, labels_train.values.reshape((-1, 1))), axis=1)\n",
    "# train_04_df = pd.DataFrame(train_04) \n",
    "# train_04_df.to_csv(\"Undata/train_04_NB.csv\", index=False)\n",
    "\n",
    "# # train_009_pca \n",
    "# train_009_pca = np.concatenate((features_train_009_pca, labels_train.values.reshape((-1, 1))), axis=1)\n",
    "# train_009_pca_df = pd.DataFrame(train_009_pca) \n",
    "# train_009_pca_df.to_csv(\"Undata/train_009_pca_NB.csv\", index=False)\n",
    "\n",
    "# # train_04_pca\n",
    "# train_04_pca = np.concatenate((features_train_04_pca, labels_train.values.reshape((-1, 1))), axis=1)\n",
    "# train_04_pca_df = pd.DataFrame(train_04_pca) \n",
    "# train_04_pca_df.to_csv(\"Undata/train_04_pca_NB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temp code!\n",
    "# labels train009\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = labels_train.count()\n",
    "print(total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #further divide the 'traditional' non-cross set into training 80/20  for pure training and cross validation  \n",
    "# #features_train_notoversampled, features_validate, labels_train_notoversampled, labels_validate = train_test_split(features_train, labels_train, test_size = .2, random_state=0)\n",
    "\n",
    "# # sm = SMOTE(random_state=0, ratio = 1.0, kind= 'svm' )\n",
    "# # #x_train_res, y_train_res = sm.fit_sample(x_train, y_train)\n",
    "# # features_train_oversampled, labels_train_oversampled = sm.fit_sample(features_train, labels_train)\n",
    "\n",
    "# rus = RandomUnderSampler(ratio=0.3,random_state=42)\n",
    "# features_train, labels_train = rus.fit_sample(features_train, labels_train)\n",
    "# # total_rows = labels_train.count()\n",
    "# # print(total_rows)\n",
    "# len(labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = SMOTEENN(random_state=0)\n",
    "# #x_train_res, y_train_res = sm.fit_sample(x_train, y_train)\n",
    "# features_train_oversampled, labels_train_oversampled = sm.fit_sample(features_train, labels_train)\n",
    "\n",
    "\n",
    "# #re-enter into original variables\n",
    "# features_train = features_train_oversampled\n",
    "# labels_train = labels_train_oversampled\n",
    "\n",
    "# len(labels_train)\n",
    "# #Below 2 lines if we want to want to force the array back into dataframe    \n",
    "# ##features_train = pd.DataFrame(features_train_oversampled,columns=[\"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\"thalach\",\"exang\",\"oldpeak\",\"slop\",\"ca\",\"thal\"])\n",
    "# ##labels_train = pd.DataFrame(labels_train_oversampled,columns=[\"pred_attribute\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # without features selection\n",
    "# train_noEng_df = pd.read_csv(\"Bdata/train_noEng_Balanced.csv\")\n",
    "# labels_train_noEng = train_noEng_df[\"22\"]\n",
    "# train_noEng_df = train_noEng_df.drop(\"22\", axis=1)\n",
    "# features_train_noEng = train_noEng_df.as_matrix()\n",
    "# print(features_train_noEng)\n",
    "\n",
    "# # train_009\n",
    "# train_009_df = pd.read_csv(\"Bdata/train_009_B.csv\")\n",
    "# labels_train_009 = train_009_df[\"14\"]\n",
    "# train_009_df = train_009_df.drop(\"14\", axis=1)\n",
    "# features_train_009 = train_009_df.as_matrix()\n",
    "\n",
    "# # train_04 \n",
    "# train_04_df = pd.read_csv(\"Bdata/train_04_B.csv\")\n",
    "# labels_train_04 = train_04_df[\"5\"]\n",
    "# train_04_df = train_04_df.drop(\"5\", axis=1)\n",
    "# features_train_04 = train_04_df.as_matrix()\n",
    "\n",
    "# # train_009_pca \n",
    "# train_009_pca_df = pd.read_csv(\"Bdata/train_009_PCA_B.csv\")\n",
    "# labels_train_009_pca = train_009_pca_df[\"5\"]\n",
    "# train_009_pca_df = train_009_pca_df.drop(\"5\", axis=1)\n",
    "# features_train_009_pca = train_009_pca_df.as_matrix()\n",
    "\n",
    "# # train_04_pca\n",
    "# train_04_pca_df = pd.read_csv(\"Bdata/train_04_PCA_B.csv\")\n",
    "# labels_train_04_pca = train_04_pca_df[\"2\"]\n",
    "# train_04_pca_df = train_04_pca_df.drop(\"2\", axis=1)\n",
    "# features_train_04_pca = train_04_pca_df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52483633,  0.99307616,  0.01881451, ..., -0.02420943,\n",
       "        -0.0481059 , -0.51593223],\n",
       "       [ 0.13404317, -0.17595256,  0.00570822, ..., -0.0246157 ,\n",
       "        -0.16369792,  0.58085292],\n",
       "       [-0.1844455 , -0.334862  , -0.01189615, ..., -0.0236486 ,\n",
       "        -0.2131816 , -0.66129966],\n",
       "       ..., \n",
       "       [-0.23947626, -0.69516728, -0.03823553, ..., -0.02268032,\n",
       "        -0.05085357, -0.67199474],\n",
       "       [-0.34478555, -0.11569117,  0.03342268, ..., -0.02765812,\n",
       "        -0.13750265, -0.70381406],\n",
       "       [ 0.39089888, -0.26507451, -0.02682582, ..., -0.02073824,\n",
       "        -0.05971286, -0.37754938]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train_009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn import grid_search\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "def checkmetrics(pred, labels_test, name):\n",
    "    sns.set()\n",
    "    print('The accuracy of ', name, 'is: ', accuracy_score(pred, labels_test))\n",
    "    matrix = confusion_matrix(labels_test, pred)\n",
    "    ax = sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    print(ax)\n",
    "    print(classification_report(pred, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature selection using RFECV to pick best features,\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.linear_model import RandomizedLasso\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# rlasso = RandomizedLasso(alpha=0.025)\n",
    "# names = features_list\n",
    "# rlasso.fit(features_train, labels_train)\n",
    " \n",
    "# print(\"Features sorted by their score using lasso:\")\n",
    "# print(sorted(zip(map(lambda x: round(x, 4), rlasso.scores_), \n",
    "#                  names), reverse=True))\n",
    "\n",
    "# #use linear regression as the model\n",
    "# lr = LinearRegression()\n",
    "# #rank all features, i.e continue the elimination until the last one\n",
    "# rfe = RFE(lr, n_features_to_select=1)\n",
    "# rfe.fit(X,Y)\n",
    " \n",
    "# print(\"Features sorted by their score using Linear Regression:\")\n",
    "# print(sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with HyperParameters -Tuning - Default Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    3.1s remaining:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score: 0.703956665097 with parameters: {'class_weight': 'balanced'}\n",
      "The accuracy of  C-Support Vector Classification is:  0.708401976936\n",
      "Axes(0.125,0.125;0.62x0.755)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      1.00      0.83      1272\n",
      "        1.0       0.61      0.11      0.19       148\n",
      "        2.0       0.71      0.01      0.02       401\n",
      "\n",
      "avg / total       0.70      0.71      0.60      1821\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dde7c026a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#svm.SVC  C-Support Vector Classification.\n",
    "parameters ={\n",
    "#          'C': [0.5,1.0,2.0,5.0,10.0],\n",
    "#          'kernel': ['linear','poly','rbf'],\n",
    "#          'degree': [2,3,4,5,6],\n",
    "#         'gamma': [1e-3,1e-2,1,2,'auto'],\n",
    "#         'tol' : [1e-4],\n",
    "#         'decision_function_shape' : ['ovo','ovr'],\n",
    "        'class_weight' : ['balanced'],\n",
    "        }\n",
    "SVM = svm.SVC()\n",
    "\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train, labels_train)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train, labels_train)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'C-Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.5s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score: 0.977861516722 with parameters: {}\n",
      "The accuracy of  C-Support Vector Classification is:  0.980779791323\n",
      "Axes(0.125,0.125;0.62x0.755)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.98      0.99      1821\n",
      "        1.0       0.00      0.00      0.00         0\n",
      "        2.0       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.98      0.99      1821\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dde7afd940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#svm.SVC  C-Support Vector Classification.\n",
    "parameters ={\n",
    "#          'C': [0.005,0.01,0.05,0.1,0.25,0.5,0.75,1.0,2.0],\n",
    "#          'kernel': ['linear','poly','rbf'],\n",
    "#          'degree': [2,3,4,5,6],\n",
    "#         'gamma': [1e-3,1e-2,1,2,'auto'],\n",
    "#         'tol' : [1e-4],\n",
    "#         'decision_function_shape' : ['ovo','ovr'],\n",
    "        }\n",
    "SVM = svm.SVC()\n",
    "\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train, labels_train)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train, labels_train)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'C-Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearSVC Linear Support Vector Classification\n",
    "\n",
    "parameters ={\n",
    "#       'penalty' : ['l2'], #Specifies the norm used in the penalization. The l2 penalty is the standard used in SVC. The l1 leads to coef_ vectors that are sparse.\n",
    "#       'loss' : ['squared_hinge'], #Specifies the loss function. hinge is the standard SVM loss (used e.g. by the SVC class) while squared_hinge is the square of the hinge loss.\n",
    "#        dual' : ['False'], #Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features.\n",
    "        'tol' : [1e-5,1e-4,1e-3],\n",
    "        'C': [0.05,0.1,0.5,0.8,1.0,2.0,5.0,10.0,15.0],\n",
    "        'multi_class'  : ['ovr','crammer_singer'],\n",
    "        'random_state' : [42],\n",
    "        'tol' : [1e-4],\n",
    "        'max_iter' : [1000],\n",
    "        }\n",
    "SVM = LinearSVC()\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train, labels_train)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train, labels_train)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'Linear Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.svm.NuSVC Nu-Support Vector Classification\n",
    "# defaults: nu=0.5, kernel=rbf, degree=3, gamma=auto, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=ovr, random_state=None)\n",
    "parameters = {\n",
    "    'nu': [0.001,0.0025,0.005,0.0075,0.1], #other larger nu values is \"not fleasible\"\n",
    "    'kernel': ['linear','poly','rbf'],\n",
    "    'degree': [2,3,4,5,6],\n",
    "    'gamma': [1e-5,1e-4,1e-3,1e-2,'auto'],\n",
    "    'tol' : [1e-4],\n",
    "    'decision_function_shape' : ['ovo','ovr'],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "SVM = NuSVC()\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train, labels_train)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train, labels_train)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'Nu-Support Vector Classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Important Features with threshold >= 0.4 (XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.9s remaining:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score: 0.655440414508 with parameters: {'class_weight': 'balanced'}\n",
      "The accuracy of  C-Support Vector Classification is:  0.686985172982\n",
      "Axes(0.125,0.125;0.62x0.755)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      1.00      0.81      1231\n",
      "        1.0       0.61      0.10      0.17       169\n",
      "        2.0       0.86      0.01      0.03       421\n",
      "\n",
      "avg / total       0.72      0.69      0.57      1821\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dde6282048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#svm.SVC  C-Support Vector Classification.\n",
    "parameters ={\n",
    "#          'C': [0.005,0.01,0.05,0.1,0.25,0.5,0.75,1.0,2.0],\n",
    "#          'kernel': ['linear','poly','rbf'],\n",
    "#          'degree': [2,3,4,5,6],\n",
    "#         'gamma': [1e-3,1e-2,1,2,'auto'],\n",
    "#         'tol' : [1e-4],\n",
    "#         'decision_function_shape' : ['ovo','ovr'],\n",
    "            'class_weight' : ['balanced'],\n",
    "\n",
    "        }\n",
    "SVM = svm.SVC()\n",
    "\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train_04, labels_train)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train_04, labels_train)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test_04)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'C-Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearSVC Linear Support Vector Classification\n",
    "\n",
    "parameters ={\n",
    "#       'penalty' : ['l2'], #Specifies the norm used in the penalization. The l2 penalty is the standard used in SVC. The l1 leads to coef_ vectors that are sparse.\n",
    "#       'loss' : ['squared_hinge'], #Specifies the loss function. hinge is the standard SVM loss (used e.g. by the SVC class) while squared_hinge is the square of the hinge loss.\n",
    "#        dual' : ['False'], #Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features.\n",
    "        'tol' : [1e-5,1e-4,1e-3],\n",
    "        'C': [0.05,0.1,0.5,0.8,1.0,2.0,5.0,10.0,15.0],\n",
    "        'multi_class'  : ['ovr','crammer_singer'],\n",
    "        'random_state' : [42],\n",
    "        'tol' : [1e-4],\n",
    "        'max_iter' : [1000],\n",
    "        }\n",
    "SVM = LinearSVC()\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train_04, labels_train_04)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train_04, labels_train_04)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test_04)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'Linear Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.svm.NuSVC Nu-Support Vector Classification\n",
    "# defaults: nu=0.5, kernel=rbf, degree=3, gamma=auto, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=ovr, random_state=None)\n",
    "parameters = {\n",
    "    'nu': [0.001,0.0025,0.005,0.0075,0.1,0.15], #other larger nu values is \"not fleasible\"\n",
    "    'kernel': ['linear','poly','rbf'],\n",
    "    'degree': [2,3,4,5,6],\n",
    "    'gamma': [1e-5,1e-4,1e-3,1e-2,'auto'],\n",
    "    'tol' : [1e-5],\n",
    "    'decision_function_shape' : ['ovo','ovr'],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "SVM = NuSVC()\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train_04, labels_train_04)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train_04, labels_train_04)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test_04)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'Nu-Support Vector Classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Important Features with threshold >= 0.09 (XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.7s remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score: 0.714554875177 with parameters: {'class_weight': 'balanced'}\n",
      "The accuracy of  C-Support Vector Classification is:  0.712246018671\n",
      "Axes(0.125,0.125;0.62x0.755)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      1.00      0.83      1283\n",
      "        1.0       0.57      0.11      0.19       142\n",
      "        2.0       0.57      0.01      0.02       396\n",
      "\n",
      "avg / total       0.67      0.71      0.61      1821\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dde7ad4be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#svm.SVC  C-Support Vector Classification.\n",
    "parameters ={\n",
    "#          'C': [0.005,0.01,0.05,0.1,0.25,0.5,0.75,1.0,2.0],\n",
    "#          'kernel': ['linear','poly','rbf'],\n",
    "#          'degree': [2,3,4,5,6],\n",
    "#         'gamma': [1e-3,1e-2,1,2,'auto'],\n",
    "#         'tol' : [1e-4],\n",
    "#         'decision_function_shape' : ['ovo','ovr'],\n",
    "            'class_weight' : ['balanced'],\n",
    "\n",
    "        }\n",
    "SVM = svm.SVC()\n",
    "\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train_009, labels_train)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train_009, labels_train)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test_009)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'C-Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearSVC Linear Support Vector Classification\n",
    "\n",
    "parameters ={\n",
    "#       'penalty' : ['l2'], #Specifies the norm used in the penalization. The l2 penalty is the standard used in SVC. The l1 leads to coef_ vectors that are sparse.\n",
    "#       'loss' : ['squared_hinge'], #Specifies the loss function. hinge is the standard SVM loss (used e.g. by the SVC class) while squared_hinge is the square of the hinge loss.\n",
    "#        dual' : ['False'], #Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features.\n",
    "        'tol' : [1e-5,1e-4,1e-3],\n",
    "        'C': [0.05,0.1,0.5,0.8,1.0,2.0,5.0,10.0,15.0],\n",
    "        'multi_class'  : ['ovr','crammer_singer'],\n",
    "        'random_state' : [42],\n",
    "        'tol' : [1e-4],\n",
    "        'max_iter' : [1000],\n",
    "        }\n",
    "SVM = LinearSVC()\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train_009, labels_train_009)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train_009, labels_train_009)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test_009)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'Linear Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.svm.NuSVC Nu-Support Vector Classification\n",
    "# defaults: nu=0.5, kernel=rbf, degree=3, gamma=auto, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=ovr, random_state=None)\n",
    "parameters = {\n",
    "    'nu': [0.001,0.0025,0.005,0.0075,0.1], #other larger nu values is \"not fleasible\"\n",
    "    'kernel': ['linear','poly','rbf'],\n",
    "    'degree': [2,3,4,5,6],\n",
    "    'gamma': [1e-5,1e-4,1e-3,1e-2,'auto'],\n",
    "    'tol' : [1e-4],\n",
    "    'decision_function_shape' : ['ovo','ovr'],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "SVM = NuSVC()\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train_009, labels_train_009)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train_009, labels_train_009)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test_009)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'Nu-Support Vector Classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PCA with Important Features (threshold >= 0.09) (XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm.SVC  C-Support Vector Classification.\n",
    "parameters ={\n",
    "         'C': [0.005,0.01,0.05,0.1,0.25,0.5,0.75,1.0,2.0],\n",
    "         'kernel': ['linear','poly','rbf'],\n",
    "         'degree': [2,3,4,5,6],\n",
    "        'gamma': [1e-3,1e-2,1,2,'auto'],\n",
    "        'tol' : [1e-4],\n",
    "        'decision_function_shape' : ['ovo','ovr'],\n",
    "        }\n",
    "SVM = svm.SVC()\n",
    "\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train_009_pca, labels_train_009_pca)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train_009_pca, labels_train_009_pca)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test_009_pca)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'C-Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearSVC Linear Support Vector Classification\n",
    "\n",
    "parameters ={\n",
    "#       'penalty' : ['l2'], #Specifies the norm used in the penalization. The l2 penalty is the standard used in SVC. The l1 leads to coef_ vectors that are sparse.\n",
    "#       'loss' : ['squared_hinge'], #Specifies the loss function. hinge is the standard SVM loss (used e.g. by the SVC class) while squared_hinge is the square of the hinge loss.\n",
    "#        dual' : ['False'], #Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features.\n",
    "        'tol' : [1e-4],\n",
    "        'C': [0.05,0.1,0.5,0.8,1.0,2.0,5.0,10.0,15.0],\n",
    "        'multi_class'  : ['ovr','crammer_singer'],\n",
    "        'random_state' : [42],\n",
    "        'tol' : [1e-4],\n",
    "        'max_iter' : [1000],\n",
    "        }\n",
    "SVM = LinearSVC()\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train_009_pca, labels_train_009_pca)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train_009_pca, labels_train_009_pca)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test_009_pca)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'Linear Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.svm.NuSVC Nu-Support Vector Classification\n",
    "# defaults: nu=0.5, kernel=rbf, degree=3, gamma=auto, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=ovr, random_state=None)\n",
    "parameters = {\n",
    "    'nu': [0.001,0.0025,0.005,0.0075,0.1], #other larger nu values is \"not fleasible\"\n",
    "    'kernel': ['linear','poly','rbf'],\n",
    "    'degree': [2,3,4,5,6],\n",
    "    'gamma': [1e-5,1e-4,1e-3,1e-2,'auto'],\n",
    "    'tol' : [1e-4],\n",
    "    'decision_function_shape' : ['ovo','ovr'],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "SVM = NuSVC()\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train_009_pca, labels_train_009_pca)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train_009_pca, labels_train_009_pca)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test_009_pca)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'Nu-Support Vector Classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PCA with Important Features (threshold >= 0.4) (XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1350 candidates, totalling 6750 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 516 tasks      | elapsed:   28.8s\n"
     ]
    }
   ],
   "source": [
    "#svm.SVC  C-Support Vector Classification.\n",
    "parameters ={\n",
    "#          'C': [0.005,0.01,0.05,0.1,0.25,0.5,0.75,1.0,2.0],\n",
    "#          'kernel': ['linear','poly','rbf'],\n",
    "#          'degree': [2,3,4,5,6],\n",
    "#         'gamma': [1e-3,1e-2,1,2,'auto'],\n",
    "#         'tol' : [1e-3],\n",
    "#         'decision_function_shape' : ['ovo','ovr'],\n",
    "        }\n",
    "SVM = svm.SVC()\n",
    "\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train_04_pca, labels_train)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train_04_pca, labels_train)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test_04_pca)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'C-Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearSVC Linear Support Vector Classification\n",
    "\n",
    "parameters ={\n",
    "#       'penalty' : ['l2'], #Specifies the norm used in the penalization. The l2 penalty is the standard used in SVC. The l1 leads to coef_ vectors that are sparse.\n",
    "#       'loss' : ['squared_hinge'], #Specifies the loss function. hinge is the standard SVM loss (used e.g. by the SVC class) while squared_hinge is the square of the hinge loss.\n",
    "#        dual' : ['False'], #Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features.\n",
    "        'tol' : [1e-3],\n",
    "        'C': [0.05,0.1,0.5,0.8,1.0,2.0,5.0,10.0,15.0],\n",
    "        'multi_class'  : ['ovr','crammer_singer'],\n",
    "        'random_state' : [42],\n",
    "        'tol' : [1e-3],\n",
    "        'max_iter' : [1000],\n",
    "        }\n",
    "SVM = LinearSVC()\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train_04_pca, labels_train_04_pca)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train_04_pca, labels_train_04_pca)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test_04_pca)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'Linear Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.svm.NuSVC Nu-Support Vector Classification\n",
    "# defaults: nu=0.5, kernel=rbf, degree=3, gamma=auto, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=ovr, random_state=None)\n",
    "parameters = {\n",
    "    'nu': [0.001,0.0025,0.005,0.0075,0.1], #other larger nu values is \"not fleasible\"\n",
    "    'kernel': ['linear','poly','rbf'],\n",
    "    'degree': [2,3,4,5,6],\n",
    "    'gamma': [1e-5,1e-4,1e-3,1e-2,'auto'],\n",
    "    'tol' : [1e-3],\n",
    "    'decision_function_shape' : ['ovo','ovr'],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "SVM = NuSVC()\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train_04_pca, labels_train_04_pca)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train_04_pca, labels_train_04_pca)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test_04_pca)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'Nu-Support Vector Classification')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
