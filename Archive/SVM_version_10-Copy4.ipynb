{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/ggplot/utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/ggplot/stats/smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import SMOTE #for SMOTE -> install package using: conda install -c conda-forge imbalanced-learn \n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import ggplot\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "import pylab as pl\n",
    "from itertools import cycle\n",
    "from sklearn import cross_validation\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "features_list = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','class']\n",
    "dataset1=pd.read_csv(\"Heart_Disease_Data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# SVM requires that each data instance is represented as a vector of real numbers\n",
    "# If you already have numeric dtypes (int8|16|32|64,float64,boolean) you can convert it to another \"numeric\" dtype using Pandas .astype() method. Demo: In [90]: df = pd.DataFrame(np.random.randint(10**5,10**7,(5,3)),columns=list('abc'), dtype=np.int64) In [91]: df Out[91]: a b c 0 9059440 9590567 2076918 1 5861102 4566089 1947323 2 6636568 162770 2487991 3 6794572 5236903 5628779 4 470121 4044395 4546794 In [92]: df.dtypes Out[92]: a int64 b int64 c int64 dtype: object In [93]: df['a'] = df['a'].astype(float) In [94]: df.dtypes Out[94]: a float64 b int64 c int64 dtype: object It won't work for object (string) dtypes, that can't be converted to numbers: In [95]: df.loc[1, 'b'] = 'XXXXXX' In [96]: df Out[96]:...\n",
    "# Just make everything numeric for ease, later we will convert to ordinal/one-hot encoding.\n",
    "dataset1 = dataset1.convert_objects(convert_numeric=True)\n",
    "dataset1 = dataset1.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age               False\n",
      "sex               False\n",
      "cp                False\n",
      "trestbps          False\n",
      "chol              False\n",
      "fbs               False\n",
      "restecg           False\n",
      "thalach           False\n",
      "exang             False\n",
      "oldpeak           False\n",
      "slop              False\n",
      "ca                 True\n",
      "thal               True\n",
      "pred_attribute    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "#### count missing value in terms of colunms #######\n",
    "#dataset.shape[0] - dataset.count()\n",
    "print(dataset1.isnull().any())\n",
    "dataset1 = dataset1.replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplications\n",
    "dataset1.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://pdfs.semanticscholar.org/daa0/f01f96a89fcfc5f41a2da67fb2a8966900ab.pdf \n",
    "# we should pick these features:\n",
    "Genetic_Based_Decision = dataset1[['cp','trestbps', 'restecg', 'thalach', 'ca', 'thal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlier:  34.0 age\n",
      "outlier:  29.0 age\n",
      "outlier:  77.0 age\n",
      "outlier:  34.0 age\n",
      "outlier:  76.0 age\n",
      "outlier:  354.0 chol\n",
      "outlier:  340.0 chol\n",
      "outlier:  353.0 chol\n",
      "outlier:  417.0 chol\n",
      "outlier:  360.0 chol\n",
      "outlier:  141.0 chol\n",
      "outlier:  341.0 chol\n",
      "outlier:  407.0 chol\n",
      "outlier:  564.0 chol\n",
      "outlier:  394.0 chol\n",
      "outlier:  409.0 chol\n",
      "outlier:  126.0 chol\n",
      "outlier:  342.0 chol\n",
      "outlier:  131.0 chol\n",
      "outlier:  99.0 thalach\n",
      "outlier:  97.0 thalach\n",
      "outlier:  202.0 thalach\n",
      "outlier:  96.0 thalach\n",
      "outlier:  88.0 thalach\n",
      "outlier:  95.0 thalach\n",
      "outlier:  96.0 thalach\n",
      "outlier:  71.0 thalach\n",
      "outlier:  90.0 thalach\n",
      "outlier:  3.5 oldpeak\n",
      "outlier:  3.6 oldpeak\n",
      "outlier:  3.4 oldpeak\n",
      "outlier:  3.6 oldpeak\n",
      "outlier:  6.2 oldpeak\n",
      "outlier:  3.6 oldpeak\n",
      "outlier:  4.0 oldpeak\n",
      "outlier:  5.6 oldpeak\n",
      "outlier:  4.0 oldpeak\n",
      "outlier:  4.2 oldpeak\n",
      "outlier:  4.2 oldpeak\n",
      "outlier:  3.8 oldpeak\n",
      "outlier:  3.4 oldpeak\n",
      "outlier:  3.6 oldpeak\n",
      "outlier:  4.4 oldpeak\n",
      "outlier:  4.0 oldpeak\n",
      "outlier:  3.4 oldpeak\n",
      "(45, 'outliers. That is', 15.0, 'percent of the total list')\n"
     ]
    }
   ],
   "source": [
    "continuous_vars = dataset1[['age', 'restecg', 'chol', 'thalach', 'oldpeak']] \n",
    "\n",
    "def checkforoutlier(df):\n",
    "    outliersnumbers = 0\n",
    "    for column in df:\n",
    "        for number in df[column]:\n",
    "            if number < np.percentile(\n",
    "                df[column], 25)-(np.percentile(\n",
    "                df[column], 75)-np.percentile(\n",
    "                df[column], 25)) or number > np.percentile(\n",
    "                df[column], 75)+(np.percentile(\n",
    "                df[column], 75)-np.percentile(\n",
    "                df[column], 25)):\n",
    "                    print(\"outlier: \", number, column)\n",
    "                    outliersnumbers += 1\n",
    "    return outliersnumbers, 'outliers. That is', round(float(outliersnumbers)/float(len(df[column]))*100, 0), 'percent of the total list'\n",
    "\n",
    "print(checkforoutlier(continuous_vars))\n",
    "\n",
    "# Thalach seems very high, but after research a heartbeat of 202 is possible: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two variables are discrete/ordinal: ca (number of major vessels colored by fluoroscopy) and num (diagnosis of heart disease)\n",
    "# Three can be directly viewed as 1 hot (because binary): 'sex':'male', 'fbs':'fasting blood sugar', 'exang':'exercise induced angina'\n",
    "\n",
    "# which leaves 4 for one-hot encoding. problem is that the values aren't unique, so have to manually\n",
    "# make extra columns:\n",
    "\n",
    "dataset1[\"cp\"] = dataset1[\"cp\"].replace([1,2,3,4], [\"typical angina\", \"atypical angina\", \"non-angina\", \"asymptomatic angina\"])\n",
    "dataset1[\"restecg\"] = dataset1[\"restecg\"].replace([0,1,2], [\"normalresecg\", \"ST-T wave abnormality\", \"left ventricular hypertrophy\"])\n",
    "dataset1[\"slop\"] = dataset1[\"slop\"].replace([1,2,3], [\"upsloping\", \"flat\", \"downsloping\"])\n",
    "dataset1[\"thal\"] = dataset1[\"thal\"].replace([3,6,7], [\"normalthal\", \"fixed defect\", \"reversible defect\"])\n",
    "\n",
    "x = dataset1[['cp', 'restecg', 'slop', 'thal']]\n",
    "for column in ['cp', 'restecg', 'slop', 'thal']:\n",
    "    one_hot = pd.get_dummies(dataset1[column])\n",
    "    dataset1 = dataset1.drop(column, axis=1)\n",
    "    dataset1 = dataset1.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b689622a-a475-40a8-bd33-e2b5e018d528",
    "_uuid": "2e13a97aaee5c269ff8f21fd7b66135927b66156"
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing:\n",
    "dataset1.dropna(inplace=True, axis=0, how=\"any\")\n",
    "Y=dataset1[\"pred_attribute\"]\n",
    "dataset1 = dataset1.drop(\"pred_attribute\", axis=1)\n",
    "X=dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "2dacb7c2-6d4e-4a17-b5cb-d6ecd821c923",
    "_uuid": "c3355ae680b60b0daa713153f05b3709193af7f6"
   },
   "outputs": [],
   "source": [
    "# evaluate the model by splitting into train and test sets  #Edit by ryan, we aim to do 3 traditional sets in the end, this first split is 80/20\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "5065d9ba-be53-4431-bb86-152ee1673550",
    "_uuid": "00229a787842594dee105c79de5871548613a045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 131, 1.0: 42, 3.0: 32, 2.0: 26, 4.0: 8})\n",
      "Counter({0.0: 30, 1.0: 12, 2.0: 10, 4.0: 5, 3.0: 3})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "list1 = []\n",
    "for i in labels_train:\n",
    "    list1.append(i)\n",
    "counter=collections.Counter(list1)\n",
    "print(counter)\n",
    "\n",
    "list2 = []\n",
    "for i in labels_test:\n",
    "    list2.append(i)\n",
    "counter=collections.Counter(list2)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7993311036789298\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print(len(features_train)/(len(features_train)+ len(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_df = pd.DataFrame(features_train)\n",
    "features_train_df.to_csv('features_train.csv', index=False)\n",
    "\n",
    "features_test_df = pd.DataFrame(features_test)\n",
    "features_test_df.to_csv('features_test.csv', index=False)\n",
    "\n",
    "labels_train_df = pd.DataFrame(labels_train)\n",
    "labels_train_df.to_csv('labels_train.csv', index=False)\n",
    "\n",
    "labels_test_df = pd.DataFrame(labels_test)\n",
    "labels_test_df.to_csv('labels_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Scores based on XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.56662\ttest-mlogloss:1.58393\n",
      "[1]\ttrain-mlogloss:1.52295\ttest-mlogloss:1.55394\n",
      "[2]\ttrain-mlogloss:1.48109\ttest-mlogloss:1.52955\n",
      "[3]\ttrain-mlogloss:1.4421\ttest-mlogloss:1.50704\n",
      "[4]\ttrain-mlogloss:1.4071\ttest-mlogloss:1.48778\n",
      "[5]\ttrain-mlogloss:1.37186\ttest-mlogloss:1.46671\n",
      "[6]\ttrain-mlogloss:1.33841\ttest-mlogloss:1.4476\n",
      "[7]\ttrain-mlogloss:1.30748\ttest-mlogloss:1.43219\n",
      "[8]\ttrain-mlogloss:1.27943\ttest-mlogloss:1.41783\n",
      "[9]\ttrain-mlogloss:1.24884\ttest-mlogloss:1.40222\n",
      "[10]\ttrain-mlogloss:1.22033\ttest-mlogloss:1.38607\n",
      "[11]\ttrain-mlogloss:1.19511\ttest-mlogloss:1.37279\n",
      "[12]\ttrain-mlogloss:1.16805\ttest-mlogloss:1.36199\n",
      "[13]\ttrain-mlogloss:1.14312\ttest-mlogloss:1.35062\n",
      "[14]\ttrain-mlogloss:1.11934\ttest-mlogloss:1.34054\n",
      "[15]\ttrain-mlogloss:1.09597\ttest-mlogloss:1.33341\n",
      "[16]\ttrain-mlogloss:1.07423\ttest-mlogloss:1.32293\n",
      "[17]\ttrain-mlogloss:1.05318\ttest-mlogloss:1.31409\n",
      "[18]\ttrain-mlogloss:1.03206\ttest-mlogloss:1.30246\n",
      "[19]\ttrain-mlogloss:1.0118\ttest-mlogloss:1.29283\n",
      "[20]\ttrain-mlogloss:0.991582\ttest-mlogloss:1.28255\n",
      "[21]\ttrain-mlogloss:0.973253\ttest-mlogloss:1.27268\n",
      "[22]\ttrain-mlogloss:0.955516\ttest-mlogloss:1.26408\n",
      "[23]\ttrain-mlogloss:0.938946\ttest-mlogloss:1.25685\n",
      "[24]\ttrain-mlogloss:0.922083\ttest-mlogloss:1.24977\n",
      "[25]\ttrain-mlogloss:0.906018\ttest-mlogloss:1.24283\n",
      "[26]\ttrain-mlogloss:0.889762\ttest-mlogloss:1.23495\n",
      "[27]\ttrain-mlogloss:0.874799\ttest-mlogloss:1.23042\n",
      "[28]\ttrain-mlogloss:0.859813\ttest-mlogloss:1.22445\n",
      "[29]\ttrain-mlogloss:0.844751\ttest-mlogloss:1.21834\n",
      "[30]\ttrain-mlogloss:0.831131\ttest-mlogloss:1.21404\n",
      "[31]\ttrain-mlogloss:0.817295\ttest-mlogloss:1.20936\n",
      "[32]\ttrain-mlogloss:0.803818\ttest-mlogloss:1.20517\n",
      "[33]\ttrain-mlogloss:0.79095\ttest-mlogloss:1.20224\n",
      "[34]\ttrain-mlogloss:0.778564\ttest-mlogloss:1.19625\n",
      "[35]\ttrain-mlogloss:0.767255\ttest-mlogloss:1.19407\n",
      "[36]\ttrain-mlogloss:0.754615\ttest-mlogloss:1.18879\n",
      "[37]\ttrain-mlogloss:0.742767\ttest-mlogloss:1.18601\n",
      "[38]\ttrain-mlogloss:0.731528\ttest-mlogloss:1.18645\n",
      "[39]\ttrain-mlogloss:0.720507\ttest-mlogloss:1.1836\n",
      "[40]\ttrain-mlogloss:0.709259\ttest-mlogloss:1.18185\n",
      "[41]\ttrain-mlogloss:0.698689\ttest-mlogloss:1.17781\n",
      "[42]\ttrain-mlogloss:0.68876\ttest-mlogloss:1.1752\n",
      "[43]\ttrain-mlogloss:0.67943\ttest-mlogloss:1.17449\n",
      "[44]\ttrain-mlogloss:0.669108\ttest-mlogloss:1.17261\n",
      "[45]\ttrain-mlogloss:0.659974\ttest-mlogloss:1.17271\n",
      "[46]\ttrain-mlogloss:0.649842\ttest-mlogloss:1.17072\n",
      "[47]\ttrain-mlogloss:0.640976\ttest-mlogloss:1.1705\n",
      "[48]\ttrain-mlogloss:0.631841\ttest-mlogloss:1.16597\n",
      "[49]\ttrain-mlogloss:0.62288\ttest-mlogloss:1.16646\n",
      "[50]\ttrain-mlogloss:0.61504\ttest-mlogloss:1.16695\n",
      "[51]\ttrain-mlogloss:0.607514\ttest-mlogloss:1.16464\n",
      "[52]\ttrain-mlogloss:0.599817\ttest-mlogloss:1.16496\n",
      "[53]\ttrain-mlogloss:0.592358\ttest-mlogloss:1.16441\n",
      "[54]\ttrain-mlogloss:0.58447\ttest-mlogloss:1.16462\n",
      "[55]\ttrain-mlogloss:0.577239\ttest-mlogloss:1.16407\n",
      "[56]\ttrain-mlogloss:0.569483\ttest-mlogloss:1.16521\n",
      "[57]\ttrain-mlogloss:0.562566\ttest-mlogloss:1.16562\n",
      "[58]\ttrain-mlogloss:0.555557\ttest-mlogloss:1.1661\n",
      "[59]\ttrain-mlogloss:0.548648\ttest-mlogloss:1.16579\n",
      "[60]\ttrain-mlogloss:0.541743\ttest-mlogloss:1.16484\n",
      "[61]\ttrain-mlogloss:0.535186\ttest-mlogloss:1.16463\n",
      "[62]\ttrain-mlogloss:0.52869\ttest-mlogloss:1.16453\n",
      "[63]\ttrain-mlogloss:0.521718\ttest-mlogloss:1.1657\n",
      "[64]\ttrain-mlogloss:0.515626\ttest-mlogloss:1.1672\n",
      "[65]\ttrain-mlogloss:0.509129\ttest-mlogloss:1.16697\n",
      "[66]\ttrain-mlogloss:0.503328\ttest-mlogloss:1.16909\n",
      "[67]\ttrain-mlogloss:0.497624\ttest-mlogloss:1.17056\n",
      "[68]\ttrain-mlogloss:0.491111\ttest-mlogloss:1.16988\n",
      "[69]\ttrain-mlogloss:0.485491\ttest-mlogloss:1.17115\n",
      "[70]\ttrain-mlogloss:0.480099\ttest-mlogloss:1.17142\n",
      "[71]\ttrain-mlogloss:0.474988\ttest-mlogloss:1.17114\n",
      "[72]\ttrain-mlogloss:0.470117\ttest-mlogloss:1.17168\n",
      "[73]\ttrain-mlogloss:0.465118\ttest-mlogloss:1.1721\n",
      "[74]\ttrain-mlogloss:0.459888\ttest-mlogloss:1.17137\n",
      "[75]\ttrain-mlogloss:0.455395\ttest-mlogloss:1.1719\n",
      "[76]\ttrain-mlogloss:0.450749\ttest-mlogloss:1.17176\n",
      "[77]\ttrain-mlogloss:0.445507\ttest-mlogloss:1.17342\n",
      "[78]\ttrain-mlogloss:0.440719\ttest-mlogloss:1.17424\n",
      "[79]\ttrain-mlogloss:0.435844\ttest-mlogloss:1.17448\n",
      "[80]\ttrain-mlogloss:0.431085\ttest-mlogloss:1.17311\n",
      "[81]\ttrain-mlogloss:0.426331\ttest-mlogloss:1.17296\n",
      "[82]\ttrain-mlogloss:0.422469\ttest-mlogloss:1.17423\n",
      "[83]\ttrain-mlogloss:0.417968\ttest-mlogloss:1.17614\n",
      "[84]\ttrain-mlogloss:0.413288\ttest-mlogloss:1.1765\n",
      "[85]\ttrain-mlogloss:0.408745\ttest-mlogloss:1.17915\n",
      "[86]\ttrain-mlogloss:0.4045\ttest-mlogloss:1.18034\n",
      "[87]\ttrain-mlogloss:0.401073\ttest-mlogloss:1.18091\n",
      "[88]\ttrain-mlogloss:0.397091\ttest-mlogloss:1.18272\n",
      "[89]\ttrain-mlogloss:0.39337\ttest-mlogloss:1.1845\n",
      "[90]\ttrain-mlogloss:0.389175\ttest-mlogloss:1.18602\n",
      "[91]\ttrain-mlogloss:0.38539\ttest-mlogloss:1.18782\n",
      "[92]\ttrain-mlogloss:0.381571\ttest-mlogloss:1.18831\n",
      "[93]\ttrain-mlogloss:0.377512\ttest-mlogloss:1.18889\n",
      "[94]\ttrain-mlogloss:0.373989\ttest-mlogloss:1.18976\n",
      "[95]\ttrain-mlogloss:0.370191\ttest-mlogloss:1.1923\n",
      "[96]\ttrain-mlogloss:0.366622\ttest-mlogloss:1.19393\n",
      "[97]\ttrain-mlogloss:0.363592\ttest-mlogloss:1.19435\n",
      "[98]\ttrain-mlogloss:0.360117\ttest-mlogloss:1.19374\n",
      "[99]\ttrain-mlogloss:0.356721\ttest-mlogloss:1.19482\n",
      "[100]\ttrain-mlogloss:0.353552\ttest-mlogloss:1.19627\n",
      "[101]\ttrain-mlogloss:0.350476\ttest-mlogloss:1.19844\n",
      "[102]\ttrain-mlogloss:0.347312\ttest-mlogloss:1.19918\n",
      "[103]\ttrain-mlogloss:0.344178\ttest-mlogloss:1.20132\n",
      "[104]\ttrain-mlogloss:0.341073\ttest-mlogloss:1.20143\n",
      "[105]\ttrain-mlogloss:0.338037\ttest-mlogloss:1.20352\n",
      "[106]\ttrain-mlogloss:0.335063\ttest-mlogloss:1.20261\n",
      "[107]\ttrain-mlogloss:0.33208\ttest-mlogloss:1.20343\n",
      "[108]\ttrain-mlogloss:0.329133\ttest-mlogloss:1.20577\n",
      "[109]\ttrain-mlogloss:0.326231\ttest-mlogloss:1.2077\n",
      "[110]\ttrain-mlogloss:0.323814\ttest-mlogloss:1.21067\n",
      "[111]\ttrain-mlogloss:0.321113\ttest-mlogloss:1.21371\n",
      "[112]\ttrain-mlogloss:0.318085\ttest-mlogloss:1.21647\n",
      "[113]\ttrain-mlogloss:0.315549\ttest-mlogloss:1.21871\n",
      "[114]\ttrain-mlogloss:0.312778\ttest-mlogloss:1.22148\n",
      "[115]\ttrain-mlogloss:0.310431\ttest-mlogloss:1.22199\n",
      "[116]\ttrain-mlogloss:0.307818\ttest-mlogloss:1.22415\n",
      "[117]\ttrain-mlogloss:0.305156\ttest-mlogloss:1.2257\n",
      "[118]\ttrain-mlogloss:0.302514\ttest-mlogloss:1.22659\n",
      "[119]\ttrain-mlogloss:0.299945\ttest-mlogloss:1.22608\n",
      "[120]\ttrain-mlogloss:0.297353\ttest-mlogloss:1.22832\n",
      "[121]\ttrain-mlogloss:0.29501\ttest-mlogloss:1.2291\n",
      "[122]\ttrain-mlogloss:0.292881\ttest-mlogloss:1.2312\n",
      "[123]\ttrain-mlogloss:0.290524\ttest-mlogloss:1.23183\n",
      "[124]\ttrain-mlogloss:0.288116\ttest-mlogloss:1.23386\n",
      "[125]\ttrain-mlogloss:0.285833\ttest-mlogloss:1.23656\n",
      "[126]\ttrain-mlogloss:0.283519\ttest-mlogloss:1.238\n",
      "[127]\ttrain-mlogloss:0.281486\ttest-mlogloss:1.23982\n",
      "[128]\ttrain-mlogloss:0.279668\ttest-mlogloss:1.24032\n",
      "[129]\ttrain-mlogloss:0.277797\ttest-mlogloss:1.2435\n",
      "[130]\ttrain-mlogloss:0.275602\ttest-mlogloss:1.24556\n",
      "[131]\ttrain-mlogloss:0.273514\ttest-mlogloss:1.24814\n",
      "[132]\ttrain-mlogloss:0.271303\ttest-mlogloss:1.25003\n",
      "[133]\ttrain-mlogloss:0.269283\ttest-mlogloss:1.25198\n",
      "[134]\ttrain-mlogloss:0.267452\ttest-mlogloss:1.25459\n",
      "[135]\ttrain-mlogloss:0.265462\ttest-mlogloss:1.25602\n",
      "[136]\ttrain-mlogloss:0.263552\ttest-mlogloss:1.25693\n",
      "[137]\ttrain-mlogloss:0.261568\ttest-mlogloss:1.25785\n",
      "[138]\ttrain-mlogloss:0.259621\ttest-mlogloss:1.25884\n",
      "[139]\ttrain-mlogloss:0.257673\ttest-mlogloss:1.26032\n",
      "[140]\ttrain-mlogloss:0.255947\ttest-mlogloss:1.26359\n",
      "[141]\ttrain-mlogloss:0.254208\ttest-mlogloss:1.26405\n",
      "[142]\ttrain-mlogloss:0.252529\ttest-mlogloss:1.26399\n",
      "[143]\ttrain-mlogloss:0.251039\ttest-mlogloss:1.2654\n",
      "[144]\ttrain-mlogloss:0.249215\ttest-mlogloss:1.26645\n",
      "[145]\ttrain-mlogloss:0.247224\ttest-mlogloss:1.26662\n",
      "[146]\ttrain-mlogloss:0.245427\ttest-mlogloss:1.26827\n",
      "[147]\ttrain-mlogloss:0.243867\ttest-mlogloss:1.2699\n",
      "[148]\ttrain-mlogloss:0.24224\ttest-mlogloss:1.27006\n",
      "[149]\ttrain-mlogloss:0.240252\ttest-mlogloss:1.27224\n",
      "[150]\ttrain-mlogloss:0.238809\ttest-mlogloss:1.2734\n",
      "[151]\ttrain-mlogloss:0.23731\ttest-mlogloss:1.27493\n",
      "[152]\ttrain-mlogloss:0.235614\ttest-mlogloss:1.27654\n",
      "[153]\ttrain-mlogloss:0.234331\ttest-mlogloss:1.27932\n",
      "[154]\ttrain-mlogloss:0.232821\ttest-mlogloss:1.27941\n",
      "[155]\ttrain-mlogloss:0.231256\ttest-mlogloss:1.27961\n",
      "[156]\ttrain-mlogloss:0.229768\ttest-mlogloss:1.28086\n",
      "[157]\ttrain-mlogloss:0.228232\ttest-mlogloss:1.2821\n",
      "[158]\ttrain-mlogloss:0.226688\ttest-mlogloss:1.28411\n",
      "[159]\ttrain-mlogloss:0.225154\ttest-mlogloss:1.28383\n",
      "[160]\ttrain-mlogloss:0.223796\ttest-mlogloss:1.28481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttrain-mlogloss:0.222303\ttest-mlogloss:1.28751\n",
      "[162]\ttrain-mlogloss:0.220927\ttest-mlogloss:1.28899\n",
      "[163]\ttrain-mlogloss:0.219704\ttest-mlogloss:1.29039\n",
      "[164]\ttrain-mlogloss:0.218196\ttest-mlogloss:1.29284\n",
      "[165]\ttrain-mlogloss:0.216814\ttest-mlogloss:1.29336\n",
      "[166]\ttrain-mlogloss:0.215436\ttest-mlogloss:1.29504\n",
      "[167]\ttrain-mlogloss:0.214174\ttest-mlogloss:1.29697\n",
      "[168]\ttrain-mlogloss:0.212971\ttest-mlogloss:1.298\n",
      "[169]\ttrain-mlogloss:0.211838\ttest-mlogloss:1.30063\n",
      "[170]\ttrain-mlogloss:0.210456\ttest-mlogloss:1.30228\n",
      "[171]\ttrain-mlogloss:0.209322\ttest-mlogloss:1.3043\n",
      "[172]\ttrain-mlogloss:0.208049\ttest-mlogloss:1.3049\n",
      "[173]\ttrain-mlogloss:0.207018\ttest-mlogloss:1.30641\n",
      "[174]\ttrain-mlogloss:0.205864\ttest-mlogloss:1.30875\n",
      "[175]\ttrain-mlogloss:0.204727\ttest-mlogloss:1.31047\n",
      "[176]\ttrain-mlogloss:0.20352\ttest-mlogloss:1.31055\n",
      "[177]\ttrain-mlogloss:0.202312\ttest-mlogloss:1.31109\n",
      "[178]\ttrain-mlogloss:0.201124\ttest-mlogloss:1.31253\n",
      "[179]\ttrain-mlogloss:0.200133\ttest-mlogloss:1.31433\n",
      "[180]\ttrain-mlogloss:0.198996\ttest-mlogloss:1.31477\n",
      "[181]\ttrain-mlogloss:0.197831\ttest-mlogloss:1.31637\n",
      "[182]\ttrain-mlogloss:0.196693\ttest-mlogloss:1.31738\n",
      "[183]\ttrain-mlogloss:0.195641\ttest-mlogloss:1.31972\n",
      "[184]\ttrain-mlogloss:0.19449\ttest-mlogloss:1.32045\n",
      "[185]\ttrain-mlogloss:0.193397\ttest-mlogloss:1.32084\n",
      "[186]\ttrain-mlogloss:0.192248\ttest-mlogloss:1.32364\n",
      "[187]\ttrain-mlogloss:0.191268\ttest-mlogloss:1.32393\n",
      "[188]\ttrain-mlogloss:0.190384\ttest-mlogloss:1.32404\n",
      "[189]\ttrain-mlogloss:0.189474\ttest-mlogloss:1.32616\n",
      "[190]\ttrain-mlogloss:0.18861\ttest-mlogloss:1.32663\n",
      "[191]\ttrain-mlogloss:0.187511\ttest-mlogloss:1.32791\n",
      "[192]\ttrain-mlogloss:0.18645\ttest-mlogloss:1.33062\n",
      "[193]\ttrain-mlogloss:0.185479\ttest-mlogloss:1.33136\n",
      "[194]\ttrain-mlogloss:0.184405\ttest-mlogloss:1.33107\n",
      "[195]\ttrain-mlogloss:0.183402\ttest-mlogloss:1.33281\n",
      "[196]\ttrain-mlogloss:0.182514\ttest-mlogloss:1.3339\n",
      "[197]\ttrain-mlogloss:0.181568\ttest-mlogloss:1.33445\n",
      "[198]\ttrain-mlogloss:0.180718\ttest-mlogloss:1.33571\n",
      "[199]\ttrain-mlogloss:0.180035\ttest-mlogloss:1.33788\n",
      "[200]\ttrain-mlogloss:0.179218\ttest-mlogloss:1.33789\n",
      "[201]\ttrain-mlogloss:0.178359\ttest-mlogloss:1.33895\n",
      "[202]\ttrain-mlogloss:0.177675\ttest-mlogloss:1.33969\n",
      "[203]\ttrain-mlogloss:0.17684\ttest-mlogloss:1.3409\n",
      "[204]\ttrain-mlogloss:0.175913\ttest-mlogloss:1.34131\n",
      "[205]\ttrain-mlogloss:0.174898\ttest-mlogloss:1.34328\n",
      "[206]\ttrain-mlogloss:0.173946\ttest-mlogloss:1.34373\n",
      "[207]\ttrain-mlogloss:0.173052\ttest-mlogloss:1.3452\n",
      "[208]\ttrain-mlogloss:0.172253\ttest-mlogloss:1.34763\n",
      "[209]\ttrain-mlogloss:0.171423\ttest-mlogloss:1.34821\n",
      "[210]\ttrain-mlogloss:0.17075\ttest-mlogloss:1.34856\n",
      "[211]\ttrain-mlogloss:0.170066\ttest-mlogloss:1.35029\n",
      "[212]\ttrain-mlogloss:0.169186\ttest-mlogloss:1.35106\n",
      "[213]\ttrain-mlogloss:0.168414\ttest-mlogloss:1.35258\n",
      "[214]\ttrain-mlogloss:0.167649\ttest-mlogloss:1.35293\n",
      "[215]\ttrain-mlogloss:0.16696\ttest-mlogloss:1.35384\n",
      "[216]\ttrain-mlogloss:0.166316\ttest-mlogloss:1.35615\n",
      "[217]\ttrain-mlogloss:0.165506\ttest-mlogloss:1.35681\n",
      "[218]\ttrain-mlogloss:0.164807\ttest-mlogloss:1.35751\n",
      "[219]\ttrain-mlogloss:0.16401\ttest-mlogloss:1.35745\n",
      "[220]\ttrain-mlogloss:0.163252\ttest-mlogloss:1.35947\n",
      "[221]\ttrain-mlogloss:0.162526\ttest-mlogloss:1.36095\n",
      "[222]\ttrain-mlogloss:0.161917\ttest-mlogloss:1.36142\n",
      "[223]\ttrain-mlogloss:0.161337\ttest-mlogloss:1.36364\n",
      "[224]\ttrain-mlogloss:0.160652\ttest-mlogloss:1.36582\n",
      "[225]\ttrain-mlogloss:0.159898\ttest-mlogloss:1.36679\n",
      "[226]\ttrain-mlogloss:0.159198\ttest-mlogloss:1.36707\n",
      "[227]\ttrain-mlogloss:0.158608\ttest-mlogloss:1.36831\n",
      "[228]\ttrain-mlogloss:0.158013\ttest-mlogloss:1.36941\n",
      "[229]\ttrain-mlogloss:0.157333\ttest-mlogloss:1.3701\n",
      "[230]\ttrain-mlogloss:0.156685\ttest-mlogloss:1.37025\n",
      "[231]\ttrain-mlogloss:0.15597\ttest-mlogloss:1.37068\n",
      "[232]\ttrain-mlogloss:0.155433\ttest-mlogloss:1.37235\n",
      "[233]\ttrain-mlogloss:0.154812\ttest-mlogloss:1.3734\n",
      "[234]\ttrain-mlogloss:0.154194\ttest-mlogloss:1.37487\n",
      "[235]\ttrain-mlogloss:0.153525\ttest-mlogloss:1.37664\n",
      "[236]\ttrain-mlogloss:0.152885\ttest-mlogloss:1.3777\n",
      "[237]\ttrain-mlogloss:0.152144\ttest-mlogloss:1.37894\n",
      "[238]\ttrain-mlogloss:0.151537\ttest-mlogloss:1.38021\n",
      "[239]\ttrain-mlogloss:0.150823\ttest-mlogloss:1.38032\n",
      "[240]\ttrain-mlogloss:0.150274\ttest-mlogloss:1.38125\n",
      "[241]\ttrain-mlogloss:0.149719\ttest-mlogloss:1.38352\n",
      "[242]\ttrain-mlogloss:0.149082\ttest-mlogloss:1.38476\n",
      "[243]\ttrain-mlogloss:0.148365\ttest-mlogloss:1.38545\n",
      "[244]\ttrain-mlogloss:0.14785\ttest-mlogloss:1.38636\n",
      "[245]\ttrain-mlogloss:0.147287\ttest-mlogloss:1.38717\n",
      "[246]\ttrain-mlogloss:0.146769\ttest-mlogloss:1.38819\n",
      "[247]\ttrain-mlogloss:0.146266\ttest-mlogloss:1.39085\n",
      "[248]\ttrain-mlogloss:0.14573\ttest-mlogloss:1.39154\n",
      "[249]\ttrain-mlogloss:0.145113\ttest-mlogloss:1.39305\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import operator\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4,\n",
    "    'silent': 1,\n",
    "    'num_class': 5\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(features_train, labels_train)\n",
    "dtest = xgb.DMatrix(features_test, labels_test)\n",
    "\n",
    "num_boost_rounds = 250\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'test')]\n",
    "\n",
    "# train model\n",
    "xgb_model = xgb.train(xgb_params, dtrain, num_boost_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1151aedd8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1036d85c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAt8AAAJCCAYAAAAY8qtZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3XuUXWV9//H3h0TDPaggK6XqCEYR\nCEYZEQQhKHUpUStCpRYraDWl6g8vRZvWqqhVY7UFlQpGy6UWKQqCSH4VvHAR5JKJCQwX0V8hrDZa\nWxQjdyF8f3+cHT0OM8nkMvvM5f1aa9bs8+znefZ3H/jjM0+es0+qCkmSJEljb4teFyBJkiRNFYZv\nSZIkqSWGb0mSJKklhm9JkiSpJYZvSZIkqSWGb0mSJKklhm9JkiSpJYZvSZIkqSWGb0mSJKkl03td\ngDScHXfcsfr6+npdhiRJ0notW7bsrqraaTR9Dd8al/r6+hgYGOh1GZIkSeuV5M7R9nXbiSRJktQS\nw7ckSZLUEsO3JEmS1BLDtyRJktQSw7ckSZLUEp92onFpcNVq+hYu6XUZkiRpAlu5aH6vS3gMV74l\nSZKklhi+JUmSpJYYvrVJkpyZ5MgN6N+X5KaxrEmSJGm8MnxLkiRJLTF8a4MkeUOSG5PckORLTfNB\nSb6f5Pa1q+Dp+GSSm5IMJjmqh2VLkiSNCz7tRKOWZE/gfcABVXVXkicC/wjMAg4EdgcuAs4DXgPM\nBZ4D7AgsTXLleuZfACwAmLb9TmN1G5IkST3jyrc2xIuB86rqLoCq+kXTfmFVPVpVtwA7N20HAudU\n1Zqq+hlwBfD8dU1eVYurqr+q+qdtPXOMbkGSJKl3DN/aEAFqmPaHhvTp/i1JkqSG4Vsb4jvAa5M8\nCaDZdjKSK4GjkkxLshNwEHB9CzVKkiSNW+751qhV1c1JPgpckWQNsHwd3S8A9gduoLNa/t6q+u8k\nfWNeqCRJ0jhl+NYGqaqzgLPWcX7b5ncB72l+us+vBPYawxIlSZLGLbedSJIkSS1x5Vvj0pxdZjKw\naH6vy5AkSdqsXPmWJEmSWmL4liRJklpi+JYkSZJaYviWJEmSWmL4liRJklpi+JYkSZJaYviWJEmS\nWmL4liRJklpi+JYkSZJaYviWJEmSWmL4liRJklpi+JYkSZJaMr3XBUjDGVy1mr6FS3pdhiRJGqWV\ni+b3uoQJwZVvSZIkqSWGb0mSJKklhu8pIskOSd7aHM9LcvEGjj8zyZEbcd0NvpYkSdJkZfieOnYA\n3trrIiRJkqYyw/fUsQjYLckK4JPAtknOS/LDJGcnCUCSDyRZmuSmJIvXtncbqU+SZyT5dpIbkvwg\nyW7NkGGvJUmSNNUYvqeOhcB/VNVc4D3Ac4F3AnsAuwIHNP1OqarnV9VewFbAK4aZa6Q+ZwP/VFXP\nAV4I/LRpH+lavyPJgiQDSQbW3L960+5WkiRpHDJ8T13XV9V/VdWjwAqgr2k/JMl1SQaBFwN7DjP2\nMX2SbAfsUlUXAFTVg1V1/3qu9TuqanFV9VdV/7StZ26u+5QkSRo3fM731PVQ1/EaYHqSLYHPAf1V\n9Z9JTgS27B60jj7r2krymGttevmSJEkTjyvfU8c9wHbr6bM2aN+VZFtguKebDNunqn4F/FeSVwMk\nmZFk600vW5IkafJwBXKKqKqfJ7k6yU3AA8DPhunzyyRfAAaBlcDSDezzp8Dnk3wYeBj4o819H5Ik\nSRNZqqrXNUiPMWPW7Jp1zMm9LkOSJI3SVP56+STLqqp/NH3ddiJJkiS1xG0nGpfm7DKTgSn8F7Qk\nSZqcXPmWJEmSWmL4liRJklpi+JYkSZJaYviWJEmSWmL4liRJklpi+JYkSZJaYviWJEmSWmL4liRJ\nklpi+JYkSZJaYviWJEmSWmL4liRJkloyvdcFSMMZXLWavoVLel2GJEljZuWi+b0uQT3gyrckSZLU\nEsO3JEmS1BLDtyRJktQSw7ckSZLUEsO3NkqSC5MsS3JzkgVN258l+VGSy5N8IckpTftOSc5PsrT5\nOaC31UuSJPWGTzvRxnpTVf0iyVbA0iRLgPcDzwPuAb4L3ND0/TRwUlVdleSpwCXAs3tRtCRJUi8Z\nvrWxjk9yeHP8FOBPgSuq6hcASb4KPLM5fyiwR5K1Y7dPsl1V3dM9YbOCvgBg2vY7jXH5kiRJ7TN8\na4MlmUcnUO9fVfcnuRy4jZFXs7do+j6wrnmrajGwGGDGrNm12QqWJEkaJ9zzrY0xE7i7Cd67A/sB\nWwMHJ3lCkunAEV39LwXevvZFkrmtVitJkjROGL61Mb4JTE9yI/AR4FpgFfAx4Drg28AtwOqm//FA\nf5Ibk9wCHNd+yZIkSb3nthNtsKp6CHj50PYkA1W1uFn5voDOijdVdRdwVLtVSpIkjT+ufGtzOjHJ\nCuAm4A7gwh7XI0mSNK6kys+1afzp7++vgYGBXpchSZK0XkmWVVX/aPq68i1JkiS1xPAtSZIktcTw\nLUmSJLXE8C1JkiS1xPAtSZIktcTwLUmSJLXE8C1JkiS1xPAtSZIktcTwLUmSJLXE8C1JkiS1xPAt\nSZIktcTwLUmSJLVkeq8LkIYzuGo1fQuX9LoMSVKPrFw0v9clSGPClW9JkiSpJYZvSZIkqSWG70ks\nyQ5J3rqZ5jo2ye91vV6ZZMfNMbckSdJUYfie3HYAHhO+k0zbiLmOBX5vfZ0kSZI0Mj9wObktAnZL\nsgJ4GLgX+CkwF9gjyeuB44HHA9fx26D+z0A/UMDpwH82r89O8gCwf9PvPUkOaY7/pKr+X5IzgQeB\nPYGdgXdX1cVJ9gTOaK61BXBEVf14zO5ckiRpHDJ8T24Lgb2qam6SecCS5vUdSZ4NHAUcUFUPJ/kc\ncDRwM7BLVe0Fna0rVfXLJG8HTqiqgaYd4FdVtW+SNwAnA69ortsHHAzsBlyW5BnAccCnq+rsJI8H\nNmb1XZIkaUJz28nUcn1V3dEcvwTYB1jarIy/BNgVuB3YNclnk7wM+NU65jun6/f+Xe1fqapHm5Xt\n24HdgWuAv0nyV8DTquqBoZMlWZBkIMnAmvtXb8JtSpIkjU+G76nlvq7jAGdV1dzm51lVdWJV3Q08\nB7gceBvwxXXMV6M4Bqiq+jLwKuAB4JIkL37MZFWLq6q/qvqnbT1z9HclSZI0QRi+J7d7gO1GOPcd\n4MgkTwZI8sQkT2ueYLJFVZ0PvB943jrmOqrr9zVd7X+UZIsku9FZTb8tya7A7VX1GeAiYO9NvDdJ\nkqQJxz3fk1hV/TzJ1UluorPi/LOuc7ck+Vvg0iRb0PlA5tuafmc0bQB/3fw+EzhtyAcuZyS5js4f\nca/ruvRtwBV0PnB5XFU9mOQo4PVJHgb+G/jw5r9jSZKk8S1VQ3cISBuvedrJxVV13qbMM2PW7Jp1\nzMmbpyhJ0oTj18trIkmyrKr6R9PXbSeSJElSS1z51rjU399fAwMDvS5DkiRpvVz5liRJksYhw7ck\nSZLUEsO3JEmS1BLDtyRJktQSw7ckSZLUEsO3JEmS1BLDtyRJktQSw7ckSZLUEsO3JEmS1BLDtyRJ\nktQSw7ckSZLUEsO3JEmS1JLpvS5AGs7gqtX0LVzS6zIkSS1buWh+r0uQxpQr35IkSVJLDN+SJElS\nSwzfU1SSe0doPzPJkZv5WscmOWVzzilJkjQRGb4lSZKklhi+p4Ak705yU/PzziHnkuSUJLckWQI8\nuevcyiSfSHJ98/OMpn2nJOcnWdr8HNC075vk+0mWN7+fNUwt85Nck2THMb5tSZKkccennUxySfYB\n3gi8AAhwXZIrurocDjwLmAPsDNwCnN51/ldVtW+SNwAnA68APg2cVFVXJXkqcAnwbOCHwEFV9UiS\nQ4GPAUd01XI48G7gsKq6e0xuWJIkaRwzfE9+BwIXVNV9AEm+Bryo6/xBwDlVtQb4SZLvDhl/Ttfv\nk5rjQ4E9kqzts32S7YCZwFlJZgMFPK5rnkOAfuClVfWr4QpNsgBYADBt+5029D4lSZLGPbedTH5Z\nfxdqlOfWHm8B7F9Vc5ufXarqHuAjwGVVtRfwSmDLrrG3A9sBzxzxQlWLq6q/qvqnbT1zFGVLkiRN\nLIbvye9K4NVJtk6yDZ1tJt8bcv6Pk0xLMovOCnW3o7p+X9McXwq8fW2HJHObw5nAqub42CHz3Am8\nBviXJHtu/O1IkiRNXIbvSa6qfgCcCVwPXAd8saqWd3W5APgxMAicClwxZIoZSa4D3gG8q2k7HuhP\ncmOSW4Djmva/Bz6e5Gpg2jC13AYcDXw1yW6b4fYkSZImlFSta8eBprIkK4H+qrqr7WvPmDW7Zh1z\nctuXlST1mF8vr4koybKq6h9NX1e+JUmSpJb4tBONqKr6enXtObvMZMDVD0mSNMm48i1JkiS1xPAt\nSZIktcTwLUmSJLXE8C1JkiS1xPAtSZIktcTwLUmSJLXE8C1JkiS1xPAtSZIktcTwLUmSJLXE8C1J\nkiS1xPAtSZIktcTwLUmSJLVkeq8LkIYzuGo1fQuX9LoMaVJYuWh+r0uQJDVc+ZYkSZJaYviWJEmS\nWmL4liRJklpi+JYkSZJaYvjWZpfkDUluTHJDki8leWWS65IsT/LtJDv3ukZJkqRe8Gkn2qyS7Am8\nDzigqu5K8kSggP2qqpK8GXgv8Je9rFOSJKkXDN/a3F4MnFdVdwFU1S+SzAHOTTILeDxwx3ADkywA\nFgBM236nlsqVJElqj9tOtLmFzkp3t88Cp1TVHODPgS2HG1hVi6uqv6r6p209c4zLlCRJap/hW5vb\nd4DXJnkSQLPtZCawqjl/TK8KkyRJ6jW3nWizqqqbk3wUuCLJGmA5cCLw1SSrgGuBp/ewREmSpJ4x\nfGuzq6qzgLOGNH+9F7VIkiSNJ247kSRJklriyrfGpTm7zGRg0fxelyFJkrRZufItSZIktcTwLUmS\nJLXE8C1JkiS1xPAtSZIktcTwLUmSJLXE8C1JkiS1xPAtSZIktcTwLUmSJLXE8C1JkiS1xPAtSZIk\ntcTwLUmSJLXE8C1JkiS1ZHqvC5CGM7hqNX0Ll/S6jElp5aL5vS5BkqQpy5VvSZIkqSWGb0mSJKkl\nhm9JkiSpJYZvSZIkqSWGb220JNskWZLkhiQ3JTkqyT5JrkiyLMklSWYlmZ5kaZJ5zbiPJ/loj8uX\nJElqnU870aZ4GfCTqpoPkGQm8O/AH1bV/yY5CvhoVb0pybHAeUmOb8a9YOhkSRYACwCmbb9TS7cg\nSZLUHsO3NsUg8KkknwAuBu4G9gK+lQRgGvBTgKq6OcmXgG8A+1fVr4dOVlWLgcUAM2bNrlbuQJIk\nqUWGb220qvpRkn2Aw4CPA98Cbq6q/UcYMgf4JbBzSyVKkiSNK+751kZL8nvA/VX1r8Cn6Gwl2SnJ\n/s35xyXZszl+DfAk4CDgM0l26FHZkiRJPePKtzbFHOCTSR4FHgb+AniETrieSef/r5OT/AxYBLyk\nqv4zySnAp4FjelS3JElSTxi+tdGq6hLgkmFOHTRM2zO7xn1mzIqSJEkax9x2IkmSJLXElW+NS3N2\nmcnAovm9LkOSJGmzcuVbkiRJaonhW5IkSWqJ4VuSJElqieFbkiRJaonhW5IkSWqJ4VuSJElqieFb\nkiRJaonhW5IkSWqJ4VuSJElqieFbkiRJaonhW5IkSWqJ4VuSJElqyfReFyANZ3DVavoWLul1GZPG\nykXze12CJEnClW9JkiSpNYZvSZIkqSWG7wkkyauSLGyOz0xy5DB95iW5eBOuMey8Q/rsnmRFkuVJ\ndtvA+ecleeHG1idJkjSRGb43UjrG5P1LMm249qq6qKoWjcU1N9Crga9X1XOr6j82cOw8wPAtSZKm\nJMP3BkjSl+TWJJ8DfgA8JclLk1yT5AdJvppk2yQvT/KVrnHzknyjOX5M/6Z9ZZIPJLkK+KMkxye5\nJcmNSf6t6XNsklO6Sjo0yfeS/CjJK4apd5skpydZ2qxS/+EwfZLklOZaS4And53bJ8kVSZYluSTJ\nrCSHAe8E3pzksqbf65Nc36yGf37tHw9JXtbc5w1JvpOkDzgOeFfT90Wb9B9EkiRpgvFpJxvuWcAb\nq+qtSXYE/hY4tKruS/JXwLuBjwGfT7JNVd0HHAWcu47+H27mfrCqDgRI8hPg6VX1UJIdRqilDzgY\n2A24LMkzhpx/H/DdqnpTM8f1Sb7d1LTW4c09zQF2Bm4BTk/yOOCzwB9W1f8mOQr4aDPXacC9VfWp\nJM9u7u+Aqnq4+cPk6CT/DnwBOKiq7kjyxKr6RffYoTeTZAGwAGDa9jut8z+CJEnSRGT43nB3VtW1\nzfF+wB7A1UkAHg9cU1WPJPkm8Mok5wHzgffSCcqP6d8197ldxzcCZye5ELhwhFq+UlWPAj9Ocjuw\n+5DzLwVeleSE5vWWwFOBW7v6HAScU1VrgJ8k+W7T/ixgL+BbTa3TgJ8OU8NLgH2ApU2/rYD/ad6b\nK6vqDoCq+sUI9/AbVbUYWAwwY9bsWl9/SZKkicbwveG6V40DfKuqXjdMv3OBtwG/AJZW1T3ppNOR\n+g+dez6dYPwq4P1J9hym/9CAOvR1gCOq6rYRrjfSuLVjb66q/dczNsBZVfXXv9OYvGqEeSVJkqYs\n93xvmmuBA9Zu90iydZJnNucuB54HvIXfrmivq/9vNB/kfEpVXUZnxXwHYNthrv9HSbZonjiyKzA0\nZF8C/J8m9JPkucPMcSXwx0mmJZkFHNK03wbslGT/ZuzjRvgD4DvAkUme3PR7YpKn0VnRPzjJ09e2\nN/3vAbYbZh5JkqRJz/C9Carqf4FjgXOS3EgnXO/enFsDXAy8vPm9zv5DTAP+NckgsBw4qap+OUy/\n24ArgH8HjquqB4ec/wjwOODGJDc1r4e6APgxMAic2sxHVf0aOBL4RJIbgBUM85SSqrqFzj72S5t7\n+hYwq7nXBcDXmvFr/wD5BnC4H7iUJElTUarcGaDxZ8as2TXrmJN7Xcak4dfLS5I0dpIsq6r+0fR1\n5VuSJElqiR+41Lg0Z5eZDLhaK0mSJhlXviVJkqSWGL4lSZKklhi+JUmSpJYYviVJkqSWGL4lSZKk\nlhi+JUmSpJYYviVJkqSWGL4lSZKklhi+JUmSpJYYviVJkqSWGL4lSZKklhi+JUmSpJZM73UB0nAG\nV62mb+GSXpcxIa1cNL/XJUiSpBG48i1JkiS1xPAtSZIktWTMwneSe0fR5/gktyY5O8m8JC8cq3qa\n6+2Q5K3r6fP9jZx7XpKLN64ySNKX5KaNHb+R15yb5LDNOF/r9yBJkjSR9Hrl+63AYVV1NDAPGNPw\nDezQXPMxkkwDqKqxrmHt9Xq63765/lxg2PDd6/okSZImo1bCd5L3JFma5MYkH2raTgN2BS5K8i7g\nOOBdSVYkeVHX2C2SrEyyQ1fb/0uyc5KdkpzfzL00yQHN+ROTnJ7k8iS3Jzm+GboI2K25xieb1erL\nknwZGGzG3tt1nfcmGUxyQ5JFTdvlSfqb4x2TrBzmfvdN8v0ky5vfz2raj03y1STfAC4d5q2aluQL\nSW5OcmmSrZLsluQHXXPPTrKsOV6Z5BNJrm9+ntG0r+t9WZzkUuBfgA8DRzXvx1FDzyfZMskZzXuw\nPMkhXffx9STfTHJbkg9u7D1IkiRNJWO+upnkpcBsYF8gdML2QVV1XJKXAYdU1V1JZgL3VtWnusdX\n1aNJvg4cDpyR5AXAyqr6WROaT6qqq5I8FbgEeHYzdHfgEGA74LYkpwILgb2qam5T27ymrr2q6o4h\ndb8ceDXwgqq6P8kTN+C2fwgcVFWPJDkU+BhwRHNuf2DvqvrFMONmA6+rqrck+QpwRFX9a5LVSeZW\n1QrgjcCZXWN+VVX7JnkDcDLwCuDT63hf9gEOrKoHkhwL9FfV25t7PnHI+b8EqKo5SXYHLk3yzGae\nfYG9gPuBpUmWAHdt5D3QXH8BsABg2vY7jeZ9liRJmlDa2Frw0uZnefN6WzoB7coNmONc4APAGcAf\nN68BDgX2SLK23/ZJtmuOl1TVQ8BDSf4H2HmEua8fGry75j6jqu4HGCEsj2QmcFaS2UABj+s69611\nzHVHE04BlgF9zfEXgTcmeTdwFJ3gu9Y5Xb9P6qp9pPfloqp6YB21d58/EPgsQFX9MMmdwNrw/a2q\n+jlAkq81fS/cyHugucZiYDHAjFmzax01SpIkTUhthO8AH6+qz2/CHNcAz0iyE53V6L9r2rcA9h8a\nJpvQ+VBX0xpGvtf7RmgPneA81CP8drvOliOM/QhwWVUdnqQPuHwU14PH1rxVc3w+8EHgu8CytaG3\nUcMcr+t9Wdf1h57PiL0e+96sfb0x9yBJkjQltLHn+xLgTUm2BUiyS5InD9PvHjpbRB6jqgq4APhH\n4Nau4HYp8Pa1/ZLMXU8tI15jGJc2dW/dzL1228lKOlszAI4cYexMYFVzfOworzeiqnqQzvt4Kp3V\n/25Hdf2+pjke7fuyvvfjSuDoZo5nAk8FbmvO/UGSJybZis4fRFdvwj1IkiRNCWMevqvqUuDLwDVJ\nBoHzGD7wfQM4PEM+cNnlXOD1/HbLCcDxQH86H+S8hc6HNtdVy8+Bq5PclOST6+n7TeAiYCDJCuCE\n5tSngL9I55GEO44w/O+Bjye5Gpi2rutsgLPprC4P/aDmjCTXAe8A3tW0jfZ9uYzO9pQVSY4a5vzn\n6HyAcpDO+35ss5UH4CrgS8AK4PyqGtiEe5AkSZoS0llU1niX5ARgZlW9v6ttJZ0PTN7Vci3H0vVB\nzQ0Y95h7GMmMWbNr1jEnb2SFU5tfLy9JUruSLKuq/tH09VnOE0CSC4DdgBf3upaNNRnuQZIkaVO5\n8q1xqb+/vwYGRrOTRZIkqbc2ZOW7199wKUmSJE0Zhm9JkiSpJYZvSZIkqSWGb0mSJKklhm9JkiSp\nJYZvSZIkqSWGb0mSJKklhm9JkiSpJYZvSZIkqSWGb0mSJKklhm9JkiSpJYZvSZIkqSXTe12ANJzB\nVavpW7ik12WMeysXze91CZIkaQO48i1JkiS1xPAtSZIktcTwrU2S5PgktyZZleSU9fSdl+SFbdUm\nSZI03hi+taneChwGvG8UfecBhm9JkjRlGb610ZKcBuwKXAQ8oav9lUmuS7I8ybeT7JykDzgOeFeS\nFUle1JOiJUmSesjwrY1WVccBPwEOAe7uOnUVsF9VPRf4N+C9VbUSOA04qarmVtX3hs6XZEGSgSQD\na+5fPfY3IEmS1DIfNaix8PvAuUlmAY8H7hjNoKpaDCwGmDFrdo1deZIkSb3hyrfGwmeBU6pqDvDn\nwJY9rkeSJGlcMHxrLMwEVjXHx3S13wNs1345kiRJ44PhW2PhROCrSb4H3NXV/g3gcD9wKUmSpir3\nfGuTVFVfc3hm80NVfR34+jB9fwTs3VJpkiRJ444r35IkSVJLXPnWuDRnl5kMLJrf6zIkSZI2K1e+\nJUmSpJYYviVJkqSWGL4lSZKklqw3fCfZOck/J/n35vUeSf5s7EuTJEmSJpfRrHyfCVwC/F7z+kfA\nO8eqIEmSJGmyGk343rGqvgI8ClBVjwBrxrQqSZIkaRIaTfi+L8mTgAJIsh+wekyrkiRJkiah0Tzn\n+93ARcBuSa4GdgKOHNOqJEmSpEloneE7yRbAlsDBwLOAALdV1cMt1CZJkiRNKusM31X1aJJ/qKr9\ngZtbqkmSJEmalEaz5/vSJEckyZhXI0mSJE1io93zvQ3wSJIH6Ww9qarafkwrkyRJkiaZ9Ybvqtqu\njUKkboOrVtO3cEmvyxjXVi6a3+sSJEnSBlpv+E5y0HDtVXXl5i9HkiRJmrxGs+3kPV3HWwL7AsuA\nF49JRZIkSdIkNZptJ6/sfp3kKcDfj1lFkiRJ0iQ1mqedDPVfwF6buxD1VpLXJ7k+yYokn0/ytCQ/\nTrJjki2SfC/JS5u+FyZZluTmJAu65rg3yUeT3JDk2iQ7N+27Na+XJvlwknt7dZ+SJEm9NJo935+l\n+Wp5OmF9LnDDWBaldiV5NnAUcEBVPZzkc3S+WOkTwGnAdcAtVXVpM+RNVfWLJFsBS5OcX1U/p/NU\nnGur6n1J/h54C/B3wKeBT1fVOUmOa/n2JEmSxo3R7Pke6Dp+BDinqq4eo3rUGy8B9qETpAG2Av6n\nqk5M8kfAcXT+6Frr+CSHN8dPAWYDPwd+DVzctC8D/qA53h94dXP8ZeBTwxXRrKIvAJi2/U6bfleS\nJEnjzGjC9w5V9enuhiTvGNqmCS3AWVX117/TmGwN/H7zclvgniTzgEOB/avq/iSX0/kgLsDDVbX2\nX0nWMLr/v36jqhYDiwFmzJpd6+kuSZI04Yxmz/cxw7Qdu5nrUG99BzgyyZMBkjwxydPobDs5G/gA\n8IWm70zg7iZ47w7sN4r5rwWOaI7/eLNWLkmSNIGMuDKZ5HXAnwBPT3JR16nt6Gwx0CRRVbck+Vvg\n0iRbAA/T+WbT59PZB74myRFJ3khn28hxSW4EbqMTrNfnncC/JvlLYAmwekxuRJIkaZxb17aA7wM/\nBXYE/qGr/R7gxrEsSu2rqnOBc4c079d1/jVd7S8fYY5tu47PA85rXq4C9quqSvLH/O7nCCRJkqaM\nEcN3Vd0J3Ennw3LSptgHOCWdT3P+EnhTj+uRJEnqifz283EjdEj2Az4LPBt4PDANuK+qth/78jRV\n9ff318CAC+SSJGn8S7KsqvpH03c0H7g8BXgd8GM6j6B7M50wLkmSJGkDjOpRcFX1/5JMq6o1wBlJ\nvj/GdUmSJEmTzmjC9/1JHg+saL618Kd0vslQkiRJ0gYYzbaTP236vR24j843Gh6xzhGSJEmSHmO9\nK99VdWeSrYBZVfWhFmqSJEmSJqX1rnwneSWwAvhm83rukC/dkSRJkjQKo9l2ciKwL53nM1NVK4C+\nsStJkiRJmpxGE74fqSq/DlySJEnaRKN52slNSf4EmJZkNnA8na+elyRJkrQBRlz5TvKl5vA/gD2B\nh4BzgF8B7xz70iRJkqTJZV0r3/skeRpwFHAI8A9d57YGHhzLwiRJkqTJZl3h+zQ6TzjZFRjoag9Q\nTbskSZKkUUpVrbtDcmpV/UVL9UgAzJg1u2Ydc3KvyxiXVi6a3+sSJElSlyTLqqp/NH3X+7QTg7ck\nSZK0eYzmUYOSJEmSNgPD9xguZHYhAAAgAElEQVRI8jctXGNuksO6Xr8qycKxvu4ItfjoSUmSpFEw\nfI+NMQ/fwFzgN+G7qi6qqkUtXPcxquqFvbiuJEnSRDPpwneSC5MsS3JzkgVN27QkZya5Kclgkncl\n2S3JD7rGzU6yrDlemeRjSa5JMpDkeUkuSfIfSY5r+sxLcmWSC5LckuS0JFskWQRslWRFkrObvu9u\nrn1Tknc2bX1Jfpjki0372UkOTXJ1kh8n2bfpt2+S7ydZ3vx+VpLHAx8Gjmquc1SSY5Oc0ozZuanr\nhubnMeE4yanNvd2c5ENd7SuTfCjJD5r3avemfack32raP5/kziQ7Nufu7XpPLk9yXnNvZydJc+4D\nSZY297p4bbskSdJUMunCN/CmqtoH6AeOT/IkOqvEu1TVXlU1Bzijqv4DWJ1kbjPujcCZXfP8Z1Xt\nD3yvaT8S2I9O6F1rX+AvgTnAbsBrqmoh8EBVza2qo5Ps08z9gmb8W5I8txn/DODTwN7A7sCfAAcC\nJ/Db1fMfAgdV1XOBDwAfq6pfN8fnNtc5d8h78Bngiqp6DvA84OZh3qf3NZ/K3Rs4OMneXefuqqrn\nAac2tQB8EPhu034B8NRh5gR4Lp0vYdqDzuMoD2jaT6mq51fVXsBWwCtGGC9JkjRpTcbwfXySG4Br\ngacAs4HbgV2TfDbJy+h8SyfAF4E3JplG58uEvtw1z0XN70Hguqq6p6r+F3gwyQ7Nueur6vaqWkPn\n2z8PHKaeA4ELquq+qroX+BrwoubcHVU1WFWP0gnI36nOsx8Hgb6mz0zgq0luAk6i822j6/NiOsGZ\nqlpTVauH6fPaZuV/eTPnHl3nvtb8XtZVx4HAvzVzfhO4e4RrX19V/9Xc04qu8YckuS7JYFPfY+4j\nyYJmNX5gzf3DlSxJkjSxTarwnWQecCiwf7PquxzYsqruBp4DXA68jU7oBjgfeDmdVdhlVfXzruke\nan4/2nW89vXaLyca+pD04R6avq7tFUPn7b7m2mt8BLisWTF+JbDlOuYblSRPp7Oi/ZKq2htYMmTe\ntXWs6apjtNtEuu9pDTA9yZbA54Ajm395+ALD3EdVLa6q/qrqn7b1zFHfjyRJ0kQxqcI3nVXiu6vq\n/mav8n4Azd7kLarqfOD9dLZiUFUPApfQWSU+YyOut2+SpyfZgs7K+VVN+8NJHtccXwm8OsnWSbYB\nDqezlWVD7mlVc3xsV/s9wHYjjPkO8Bfwm/3u2w85vz1wH51tNzvT+QNkfa4CXtvM+VLgCaMpvrE2\naN+VZFs6W3gkSZKmnMkWvr9JZ6X1Rjorxtc27bsAlydZQWf/9l93jTmbzor1pRtxvWuARcBNwB10\n9kIDLAZuTHJ2Vf2gueb1wHXAF6tq+QZc4++Bjye5GpjW1X4ZsMfaD1wOGfMOOts8BulsHfmdLR5V\ndQOdfxW4GTgduHoUdXwIeGmzVeXlwE/p/AGwXlX1Szqr3YPAhcDS0YyTJEmabNb79fKTXZITgJlV\n9f4NHDcPOKGqpsQHB5PMANZU1SNJ9gdOraq56xu3sfx6+ZH59fKSJI0v2YCvl5++/i6TV5IL6Dyl\n5MW9rmUCeCrwlWaLza+Bt/S4HkmSpAlnyq98a3zq7++vgYGBXpchSZK0Xhuy8j3Z9nxLkiRJ45bh\nW5IkSWqJ4VuSJElqieFbkiRJaonhW5IkSWqJ4VuSJElqieFbkiRJaonhW5IkSWqJ4VuSJElqieFb\nkiRJaonhW5IkSWqJ4VuSJElqyfReFyANZ3DVavoWLul1Ga1auWh+r0uQJEljzJVvSZIkqSWGb0mS\nJKklhm+NSpLLk/Q3x3/T1d6X5KYNnOvEJCds7holSZLGO8P3FJBkc+/t/5v1d5EkSdJQhu8Jollh\nvjXJF5LcnOTSJFslmZvk2iQ3JrkgyROa/pcn+ViSK4B3JDkzyalJLktye5KDk5zezHlm13VOTTLQ\nXONDw9SxCNgqyYokZzfN04bW1fR9S5KlSW5Icn6Srcf+nZIkSRq/DN8Ty2zgn6pqT+CXwBHAvwB/\nVVV7A4PAB7v671BVB1fVPzSvnwC8GHgX8A3gJGBPYE6SuU2f91VVP7A3cHCSvbsLqKqFwANVNbeq\njl5HXQBfq6rnV9VzgFuBP9s8b4MkSdLEZPieWO6oqhXN8TJgNzoB+4qm7SzgoK7+5w4Z/42qKjoh\n/WdVNVhVjwI3A31Nn9cm+QGwnE4w32Mj6lo7115JvpdkEDi6mW9ESRY0q+4Da+5fPYrLSpIkTSyG\n74nloa7jNcAO6+l/3wjjHx0y16PA9CRPB04AXtKspC8BttyIutbuMT8TeHtVzQE+tL65qmpxVfVX\nVf+0rWeO4rKSJEkTi+F7YlsN3J3kRc3rPwWuWEf/9dmeTmBfnWRn4OUj9Hs4yeNGMd92wE+bvkev\nr7MkSdJk5zdcTnzHAKc1H2a8HXjjxk5UVTckWU5nG8rtwNUjdF0M3NhsT3nfOqZ8P3AdcCedrS7b\nbWxtkiRJk0E6W4Cl8WXGrNk165iTe11Gq/x6eUmSJqYky5oHVqyX204kSZKklrjtROPSnF1mMuBK\nsCRJmmRc+ZYkSZJaYviWJEmSWmL4liRJklpi+JYkSZJaYviWJEmSWmL4liRJklpi+JYkSZJaYviW\nJEmSWmL4liRJklpi+JYkSZJaYviWJEmSWmL4liRJkloyvdcFSMMZXLWavoVLel3GmFm5aH6vS5Ak\nST3gyrckSZLUEsO3JEmS1BLDtzaLJMcnuTXJ2UlO6HU9kiRJ45HhW5vLW4HDgB/3uhBJkqTxyvCt\nTZbkNGBX4CLgXcBzknw3yY+TvKXpMyvJlUlWJLkpyYt6WbMkSVIv+LQTbbKqOi7Jy4BDgLcDhwP7\nAdsAy5MsAV4HXFJVH00yDdi6ZwVLkiT1iOFbY+HrVfUA8ECSy4B9gaXA6UkeB1xYVSuGDkqyAFgA\nMG37ndqsV5IkqRVuO9FYqKGvq+pK4CBgFfClJG94zKCqxVXVX1X907ae2UadkiRJrTJ8ayz8YZIt\nkzwJmAcsTfI04H+q6gvAPwPP62WBkiRJveC2E42F64ElwFOBj1TVT5IcA7wnycPAvcBjVr4lSZIm\nO8O3Nouq6msOTxzh/FnAWW3VI0mSNB657USSJElqiSvfGpfm7DKTgUXze12GJEnSZuXKtyRJktQS\nw7ckSZLUEsO3JEmS1BLDtyRJktQSw7ckSZLUEsO3JEmS1BLDtyRJktQSw7ckSZLUEsO3JEmS1BLD\ntyRJktQSw7ckSZLUEsO3JEmS1JLpvS5AGs7gqtX0LVzSs+uvXDS/Z9eWJEmTlyvfkiRJUksM35Ik\nSVJLDN8iSV+SmzZh/Pc3Zz2SJEmTleFbm6yqXtjrGiRJkiYCw/cEN3TVOskJSU5McnmSk5N8P8lN\nSfZtzh+cZEXzszzJdkPm2zLJGUkGm/OHNO3HJvl6km8muS3JB7vG3Nv8ntdc97wkP0xydpI05w5r\n2q5K8pkkF7fx/kiSJI0nPu1kctumql6Y5CDgdGAv4ATgbVV1dZJtgQeHjHkbQFXNSbI7cGmSZzbn\n9m3muB9YmmRJVQ0MGf9cYE/gJ8DVwAFJBoDPAwdV1R1Jztn8typJkjT+ufI9uZ0DUFVXAtsn2YFO\nIP7HJMcDO1TVI0PGHAh8qRn3Q+BOYG34/lZV/byqHgC+1vQd6vqq+q+qehRYAfQBuwO3V9Ud3XUN\nlWRBkoEkA2vuX71xdyxJkjSOGb4nvkf43f+OW3Yd15C+VVWLgDcDWwHXNqvb3bKOaz1mvmH6PNR1\nvIbOv66sa87u4hZXVX9V9U/beuZohkiSJE0ohu+J72fAk5M8KckM4BVd544CSHIgsLqqVifZraoG\nq+oTwACdVeluVwJHN+OeCTwVuK059wdJnphkK+DVdFbRR+OHwK5J+rrrkiRJmmrc8z3BVdXDST4M\nXAfcQSfornV38xjA7YE3NW3vbD5EuQa4Bfh3YFbXmM8BpyUZpLOqfmxVPdR8bvIqOltSngF8eZj9\n3iPV+ECStwLfTHIXcP3G3a0kSdLElqrhdg5ooktyOXDCaAPyKOY7Fuivqrdv5Phtq+re5ukn/wT8\nuKpOGqn/jFmza9YxJ29csZuBXy8vSZJGK8myquofTV+3nagtb0myArgZmEnn6SeSJElTiivfGpf6\n+/trYGCzLNpLkiSNKVe+JUmSpHHI8C1JkiS1xPAtSZIktcTwLUmSJLXE8C1JkiS1xPAtSZIktcTw\nLUmSJLXE8C1JkiS1xPAtSZIktcTwLUmSJLXE8C1JkiS1xPAtSZIktWR6rwuQhjO4ajV9C5e0dr2V\ni+a3di1JkjR1ufItSZIktcTwLUmSJLXE8K1NluSLSfbodR2SJEnjnXu+tcmq6s29rkGSJGkicOV7\nnEvSl+TWJF9IcnOSS5NslWRukmuT3JjkgiRPaPpfnuQTSa5P8qMkLxph3rckWZrkhiTnJ9m6aT8z\nyWeSfD/J7UmObNq3SPK5poaLk/zfrnOXJ+lvju9N8tFm3muT7Ny0vzLJdUmWJ/n22nZJkqSpxPA9\nMcwG/qmq9gR+CRwB/AvwV1W1NzAIfLCr//Sq2hd455D2bl+rqudX1XOAW4E/6zo3CzgQeAWwqGl7\nDdAHzAHeDOw/wrzbANc2814JvKVpvwrYr6qeC/wb8N6hA5MsSDKQZGDN/atHmF6SJGnictvJxHBH\nVa1ojpcBuwE7VNUVTdtZwFe7+n+tq2/fCHPuleTvgB2AbYFLus5dWFWPArd0rVAfCHy1af/vJJeN\nMO+vgYu7rv8HzfHvA+cmmQU8Hrhj6MCqWgwsBpgxa3aNML8kSdKE5cr3xPBQ1/EaOoF5NP3X0PyB\nleSMJCuS/N/m3JnA26tqDvAhYMsRrpchv9fn4apaG5x/c33gs8ApzfX+fMj1JEmSpgTD98S0Gri7\naz/3nwJXrKM/VfXGqppbVYc1TdsBP03yOODoUVzzKuCIZu/3zsC8Dax5JrCqOT5mA8dKkiRNCm47\nmbiOAU5rPih5O/DGDRz/fuA64E46e8a3W0//84GXADcBP2rGbsjG7BOBryZZBVwLPH0D65UkSZrw\n8tsdAtK6Jdm2qu5N8iTgeuCAqvrvsbjWjFmza9YxJ4/F1MPy6+UlSdLGSrKsqvpH09eVb22Ii5Ps\nQOcDkx8Zq+AtSZI0WRm+NWpVNa+ta83ZZSYDrkZLkqRJxg9cSpIkSS0xfEuSJEktMXxLkiRJLTF8\nS5IkSS0xfEuSJEktMXxLkiRJLTF8S5IkSS0xfEuSJEktMXxLkiRJLTF8S5IkSS0xfEuSJEktMXxL\nkiRJLZne6wKk4QyuWk3fwiUbPX7lovmbsRpJkqTNw5VvSZIkqSWGb0mSJKklhu8pJsnlSfp7XYck\nSdJUZPieQJKM2R79JNPGam5JkiR1GL5blqQvya1JvpDk5iSXJtkqydwk1ya5MckFSZ7Q9L88yceS\nXAG8I8mZSU5NclmS25McnOT0Zs4zu65zapKB5hofGqGWe5N8OMl1wP5J9klyRZJlSS5JMqvpd3yS\nW5ra/q1p26a57tIky5P8YdM+Lcmnkgw2/f9P035Ykh8muSrJZ5JcPKZvtCRJ0jhk+O6N2cA/VdWe\nwC+BI4B/Af6qqvYGBoEPdvXfoaoOrqp/aF4/AXgx8C7gG8BJwJ7AnCRzmz7vq6p+YG/g4CR7D1PH\nNsBNVfUC4Drgs8CRVbUPcDrw0abfQuC5TW3HrZ0f+G5VPR84BPhkkm2ABcDTu/qfnWRL4PPAy6vq\nQGCn4d6UJAuaPxgG1ty/er1voiRJ0kRj+O6NO6pqRXO8DNiNTsC+omk7Czioq/+5Q8Z/o6qKTkj/\nWVUNVtWjwM1AX9PntUl+ACynE8z3GKaONcD5zfGzgL2AbyVZAfwt8PvNuRvphOjXA480bS8FFjZ9\nLwe2BJ4KHAqcVlWPAFTVL4Ddgdur6o5m7DnDvSlVtbiq+quqf9rWM4frIkmSNKH5nO/eeKjreA2w\nw3r63zfC+EeHzPUoMD3J04ETgOdX1d3NdpQth5n3wapa0xwHuLmq9h+m33w6fwy8Cnh/kj2b/kdU\n1W3dHZMEqCHj8//bu/toyaryzuPfX7qhFYEGArI6vjU6LS4iocELooJjDFFBEfEl4DjSoBFd6qhR\noswwakdHg7gmQcXIoEEQiTIoKBMMCg7yKi/dTTcvAoLQrgQZGEBbEAcVn/mj9rWL9r523z51X76f\ntWrVqX32PufZ51TdemrffarG6pwkSdJc4cj39LAO+GmS/dvjNwKXjFF/PNvSS9jXJdkZOHACbW4F\ndkryPIAkWyT54yR/ADylqi4G3k/vg8LWwLeB/9SSbZLs2bbzHeBtwxeHJtkBuAV4epLFrc5hm9A3\nSZKkGcuR7+ljGXBykq2AO4CjNnZDVbUmyXX0pqHcAVwxgTa/SvJa4NNJFtJ7bpwI/BD4cisL8PdV\n9bMkH23rr28J+FrgFcAXgGe28l8Dn6+qk5K8HbggyX3ANRvbN0mSpJksvanD0uaVZOuqeqgl6p8F\nbquqvx+t/oJFS2rRshM3en/+vLwkSepKkpXtiy7G5bQTdeUt7eLMm4CF9L79RJIkaU5x5FvT0tDQ\nUK1YsWLQYUiSJI3LkW9JkiRpGjL5liRJkjpi8i1JkiR1xORbkiRJ6ojJtyRJktQRk29JkiSpIybf\nkiRJUkdMviVJkqSOmHxLkiRJHTH5liRJkjpi8i1JkiR1xORbkiRJ6sj8QQcgjeSGu9ax+NjzJ1x/\n7fEv34zRSJIkTQ1HviVJkqSOmHxLkiRJHTH57kiS/7KJ7b+VZLuNaHdkkpM2Zd8T2MfbkhyxOfch\nSZI0G5h8d2eTku+qOqiqfjZVwUylqjq5qr406DgkSZKmO5PvKZbkG0lWJrkpydGt7Hjg8UlWJzkz\nyUeTvLuvzceSvCvJi5JcmuTcJD9IcnKSP2h11ibZsS0fkeT6JGuSnNHKDk5ydZLrklyUZOdx4twn\nyZWt/pVJdm3lRyY5J8kFSW5LckJfmzcn+WGS7yX5/PCIepLlSY5py99L8okk17S6+7fyxUkuS7Kq\n3Z4/hYddkiRpRvDbTqbem6rqgSSPB65N8vWqOjbJO6tqKfQSUeAc4FMtuT4c2AfYvd3vBvwYuAB4\nNfC14Y0n+WPgOOAFVXVfkh3aqsuBfauqkvwl8H7gfWPEeQvwwqr6TZIDgI8Dr2nrlgJ7Ao8Atyb5\nDPAo8EFgL+BB4H8Da0bZ9vyq2ifJQcCHgQOAe4E/r6r/l2QJ8BVgqL9R+7ByNMC8bXcaI3RJkqSZ\nyeR76r0ryaFt+SnAEuD+/gpVtTbJ/Un2BHYGrquq+5MAXFNVdwAk+QqwH33JN/Bi4GtVdV/b1gOt\n/MnAWUkWAVsCd44T50Lg9JYIF7BF37rvVtW6FsMPgKcBOwKXDO8vydnAM0fZ9jntfiWwuC1vAZyU\nZCm9RP732lbVKcApAAsWLalx4pckSZpxnHYyhZK8iN4o7/Oqag/gOuBxo1T/AnAkcBRwal/5hknn\nho8zQhnAZ4CTqmp34K1j7HfYR4GLq+rZwMEb1H+kb/lReh/SMs72+g23H24L8FfAPcAe9Ea8t5zE\n9iRJkmYFk++ptRD4aVU9nORZwL59636dpH90+VzgZcDewLf7yvdJskubjnIYvekk/b4L/EWSPwTo\nm3ayELirLS+bYKzD9Y+cQP1rgH+fZPsk81k/RWWiFgJ3V9VvgTcC8ybZXpIkacYz+Z5aFwDzk1xP\nb2T5qr51pwDXJzkToKp+BVwM/M+qerSv3veB44Eb6U0dObd/B1V1E/Ax4JIka4C/a6uWA2cnuQy4\nbwKxngD8bZIrmEAiXFV30ZsXfjVwEfADYN0E9jPsH4BlSa6iN+XkF5NoK0mSNCukyqm1g9BGtlcB\nr6uq21rZi4BjquoVg4xtNEm2rqqH2sj3ucCpVXXueO02xoJFS2rRshMnXN+fl5ckSYOSZGVVDY1f\n05HvgUiyG3A7vQsbbxt0PJOwPMlq1o/Kf2PA8UiSJM0ojnxrWhoaGqoVK1YMOgxJkqRxOfItSZIk\nTUMm35IkSVJHTL4lSZKkjph8S5IkSR0x+ZYkSZI6YvItSZIkdcTkW5IkSeqIybckSZLUEZNvSZIk\nqSMm35IkSVJHTL4lSZKkjph8S5IkSR2ZP+gApJHccNc6Fh97/oTrrz3+5ZsxGkmSpKnhyLckSZLU\nEZNvSZIkqSMm39NQkuVJjulwf2uT7LiRbb+QZLepjkmSJGk2cs63NklV/eWgY5AkSZopHPmeJpIc\nl+TWJBcBu7aypUmuSnJ9knOTbJ/kiUlWtvV7JKkkT22Pf5RkqySnJfl0kiuT3JHktW39oiSXJlmd\n5MYk+48Qx3vbuhuTvKeVLU5yS5LTWyxfS7JVW/e9JENt+aEkH0uypsW9cyt/Rnt8bZKPJHmog0Mq\nSZI07Zh8TwNJngMcDuwJvBrYu636EvCBqvoT4Abgw1V1L/C4JNsC+wMrgP2TPA24t6oebm0XAfsB\nrwCOb2X/Afh2VS0F9gBWjxDHUcBzgX2BtyTZs63eFTilxfJz4O0jdOUJwFVVtQdwKfCWVv4p4FNV\ntTfwkzGOw9FJViRZ8ejD60Y/YJIkSTOUyff0sD9wblU9XFU/B86jl8huV1WXtDqnAy9sy1cCL2iP\nP97u9wcu69vmN6rqt1X1A2DnVnYtcFSS5cDuVfXgBnHs1+L4RVU9BJzTtgvwr1V1RVv+cqu7oV8B\n/9yWVwKL2/LzgLPb8j+NdhCq6pSqGqqqoXlbLRytmiRJ0oxl8j191CTqXkYvKX4a8E16o9j70Rtt\nHvZI33IAqupSeon6XcAZSY7YYLuZRHwjxfvrqhoufxSvKZAkSXoMk+/p4VLg0CSPT7INcDDwC+Cn\nffOy3whc0lf/PwK3VdVvgQeAg4ArGEPf1JTPA/8I7DVCHK9q88afABzK+tH0pyZ5Xlt+PXD5JPp3\nFfCatnz4JNpJkiTNKo5MTgNVtSrJWfTmYP+Y9QnvMuDkdnHjHfTmY1NVa5PA+pHuy4EnV9VPx9nV\ni4C/TvJr4CHgMSPfLY7TgGta0Req6roki4GbgWVJ/gdwG/C5SXTxPcCXk7wPOB9wQrckSZqTsn6W\ngDSylnz/c1U9eyPbbwX8sqoqyeHA66vqkLHaLFi0pBYtO3HC+/Dn5SVJ0qAkWVlVQxOp68i3uvAc\n4KT0hut/BrxpwPFIkiQNhCPfmpaGhoZqxYoVgw5DkiRpXJMZ+faCS0mSJKkjJt+SJElSR0y+JUmS\npI6YfEuSJEkdMfmWJEmSOmLyLUmSJHXE5FuSJEnqiMm3JEmS1BGTb0mSJKkjJt+SJElSR0y+JUmS\npI6YfEuSJEkdmT/oAKSR3HDXOhYfe/6YddYe//KOopEkSZoajnxLkiRJHTH5noaSvCvJzUnOTPLK\nJMdO0XYfmkCd5UmOGafOTkmuTnJdkv0nGcPSJAdNpo0kSdJs4bST6entwIFVdWd7fN4ggxnBnwG3\nVNWyjWi7FBgCvjW1IUmSJE1/jnxPM0lOBp4OnJfkr5IcmeSktu6bSY5oy29NcmZbfkaSC5KsTHJZ\nkme18l2SfD/JtUk+OsY+j0tya5KLgF37yn9vu0mWAicAByVZneTxSV7S9rMqydlJtm7t905yZZI1\nSa5JshD4CHBYa3vYZjmIkiRJ05Qj39NMVb0tycuAP62q+5Ic2bf6aOCKJHcC7wP2beWnAG+rqtuS\nPBf4B+DFwKeAz1XVl5K8Y6T9JXkOcDiwJ73nwypg5WjbraoXJ/kQMFRV70yyI/BfgQOq6hdJPgC8\nN8nxwFnAYVV1bZJtgYeB37WdgsMlSZI0o5h8zyBVdU9LfC8GDq2qB9oo8/OBs5MMV13Q7l8AvKYt\nnwF8YoTN7g+cW1UPAyQ5r92Ptd1++wK70ftQALAl8H16I+h3V9W1Lfaft+2O2r8kR9P7gMG8bXca\ntZ4kSdJMZfI98+wO3A/8UXv8B8DPqmrpKPVrAtscqc542x0W4MKqev1jCpM/meC+1wdRdQq90XYW\nLFoyqbaSJEkzgXO+Z5Ak+wAH0psickySXdqI8p1JXtfqJMkerckV9KaUALxhlM1eChza5m5vAxwM\nvxupHm27/a4CXpDk37V6WyV5JnAL8EdJ9m7l2ySZDzwIbLMJh0GSJGnGMvmeIZIsAD4PvKmqfkJv\nzvep6c3jeAPw5iRrgJuAQ1qzdwPvSHItsHCk7VbVKnpzs1cDXwcu61s92nb72/9f4EjgK0mup5eM\nP6uqfgUcBnymtb8QeBy9KTO7ecGlJEmai1Llf/c1/SxYtKQWLTtxzDr+wqUkSZoOkqysqqGJ1HXk\nW5IkSeqIF1xqWtr9SQtZ4ci2JEmaZRz5liRJkjpi8i1JkiR1xORbkiRJ6ojJtyRJktQRk29JkiSp\nIybfkiRJUkdMviVJkqSOmHxLkiRJHTH5liRJkjpi8i1JkiR1xORbkiRJ6ojJtyRJktSR+YMOQBrJ\nDXetY/Gx549ZZ+3xL+8oGkmSpKnhyLckSZLUEZNvSZIkqSMm3x1Isl2St2/iNq7cyHbLkxyzKfue\nwD4+kuSAzbkPSZKk2cDkuxvbAZuUfFfV86colilXVR+qqosGHYckSdJ0Z/LdjeOBZyRZneSTSc5I\ncsjwyiRnJnllkiOTfDPJBUluTfLhvjoP9S2/P8kNSdYkOb6VvSXJta3s60m2GiugJAcnuTrJdUku\nSrJzK1+e5NQk30tyR5J39bX5YJJbklyY5CvDI+pJTkvy2ra8NsnfJFnVYnxWK98nyZVtf1cm2XVK\njqwkSdIMYvLdjWOBH1XV0qr6a+ALwFEASRYCzwe+1eruA7wBWAq8LslQ/4aSHAi8CnhuVe0BnNBW\nnVNVe7eym4E3jxPT5cC+VbUn8FXg/X3rngW8tMXy4SRbtDheA+wJvBoYYnT3VdVewOeA4SkvtwAv\nbPv7EPDxceKTJEmadbJTn20AAAgoSURBVPyqwQGoqkuSfDbJE+klsl+vqt8kAbiwqu4HSHIOsB+w\noq/5AcAXq+rhtq0HWvmzk/w3elNctga+PU4YTwbOSrII2BK4s2/d+VX1CPBIknuBnVsc36yqX7bY\n/tcY2z6n3a9s/QNYCJyeZAlQwBYbNkpyNHA0wLxtdxonfEmSpJnHke/BOYPeCPdRwBf7ymuDehs+\nzghlAKcB76yq3YG/AR43zv4/A5zU6r91g/qP9C0/Su9DWsbZXr/h9sNtAT4KXFxVzwYOHim+qjql\nqoaqamjeVgsnsTtJkqSZweS7Gw8C22xQdhrwHoCquqmv/M+T7JDk8fSml1yxQbvvAG8antOdZIdW\nvg1wd5It6CX141kI3NWWl02g/uXAwUkel2RrYLK/cNO/vyMn2VaSJGlWMPnuQJtGckWSG5N8spXd\nQ29u9hc3qH45vVHx1fSmo6zYYFsXAOcBK5KsZv2c6g8CVwMX0ptfPZ7lwNlJLgPum0Afrm37XUNv\nWskKYN0E9jPsBOBvk1wBzJtEO0mSpFkjVSPNYNDm1kaubwD2qqp1rexIYKiq3jnI2EaTZOuqeqjF\nfilwdFWt2hz7WrBoSS1aduKYdfx5eUmSNB0kWVlVY30Zxe848j0A7QdpbgE+M5x4zxCntNH2VfRG\n5TdL4i1JkjRbOfKtaWloaKhWrFgxfkVJkqQBc+RbkiRJmoZMviVJkqSOmHxLkiRJHTH5liRJkjpi\n8i1JkiR1xORbkiRJ6ohfNahpKcmDwK2DjmOAdmQCvzw6S83lvsPc7r99n7vmcv/nct9h9vT/aVW1\n00Qqzt/ckUgb6daJfl/mbJRkxVzt/1zuO8zt/tv3udl3mNv9n8t9h7nZf6edSJIkSR0x+ZYkSZI6\nYvKt6eqUQQcwYHO5/3O57zC3+2/f56653P+53HeYg/33gktJkiSpI458S5IkSR0x+da0k+RlSW5N\ncnuSYwcdz1RL8pQkFye5OclNSd7dypcnuSvJ6nY7qK/Nf27H49YkLx1c9FMjydokN7R+rmhlOyS5\nMMlt7X77Vp4kn279vz7JXoONfuMl2bXv/K5O8vMk75nN5z7JqUnuTXJjX9mkz3WSZa3+bUmWDaIv\nkzVK3z+Z5JbWv3OTbNfKFyf5Zd9z4OS+Ns9pr5fb2/HJIPozGaP0fdLP85n6fjBK/8/q6/vaJKtb\n+Ww796O9x82J1/2EVJU3b9PmBswDfgQ8HdgSWAPsNui4priPi4C92vI2wA+B3YDlwDEj1N+tHYcF\nwC7t+MwbdD828RisBXbcoOwE4Ni2fCzwibZ8EPAvQIB9gasHHf8UHYN5wP8Bnjabzz3wQmAv4MaN\nPdfADsAd7X77trz9oPu2kX1/CTC/LX+ir++L++ttsJ1rgOe14/IvwIGD7ttG9n1Sz/OZ/H4wUv83\nWP/fgQ/N0nM/2nvcnHjdT+TmyLemm32A26vqjqr6FfBV4JABxzSlquruqlrVlh8EbgaeNEaTQ4Cv\nVtUjVXUncDu94zTbHAKc3pZPB17VV/6l6rkK2C7JokEEOMX+DPhRVf14jDoz/txX1aXAAxsUT/Zc\nvxS4sKoeqKqfAhcCL9v80W+akfpeVd+pqt+0h1cBTx5rG63/21bV96uXkXyJ9cdr2hrlvI9mtOf5\njH0/GKv/bfT6L4CvjLWNGXzuR3uPmxOv+4kw+dZ08yTgX/se/xtjJ6YzWpLFwJ7A1a3one3fbqcO\n/0uO2XlMCvhOkpVJjm5lO1fV3dD74w08sZXPxv4DHM5j33znyrmHyZ/r2Xoc3kRvxG/YLkmuS3JJ\nkv1b2ZPo9XfYTO/7ZJ7ns/W87w/cU1W39ZXNynO/wXucr/vG5FvTzUjz2WblV/Ik2Rr4OvCeqvo5\n8DngGcBS4G56/5aE2XlMXlBVewEHAu9I8sIx6s66/ifZEnglcHYrmkvnfiyj9XfWHYckxwG/Ac5s\nRXcDT62qPYH3Av+UZFtmV98n+zyfTX3v93oe+8F7Vp77Ed7jRq06QtlsPv8m35p2/g14St/jJwM/\nGVAsm02SLej9UTqzqs4BqKp7qurRqvot8HnWTy+Ydcekqn7S7u8FzqXX13uGp5O0+3tb9VnXf3of\nOlZV1T0wt859M9lzPauOQ7tw7BXAG9p0AtqUi/vb8kp6c52fSa/v/VNTZmzfN+J5PqvOO0CS+cCr\ngbOGy2bjuR/pPY45/rrvZ/Kt6eZaYEmSXdro4OHAeQOOaUq1+X7/CNxcVX/XV94/j/lQYPgq+fOA\nw5MsSLILsITeRTgzUpInJNlmeJneBWg30uvn8NXsy4BvtuXzgCPaFfH7AuuG/3U5gz1m5GuunPs+\nkz3X3wZekmT7NlXhJa1sxknyMuADwCur6uG+8p2SzGvLT6d3ru9o/X8wyb7tb8cRrD9eM8pGPM9n\n4/vBAcAtVfW76SSz7dyP9h7HHH7d/55BX/HpzduGN3pXPv+Q3qf/4wYdz2bo3370/nV2PbC63Q4C\nzgBuaOXnAYv62hzXjsetzICr3cfp/9PpfWvBGuCm4XMM/CHwXeC2dr9DKw/w2db/G4ChQfdhE/u/\nFXA/sLCvbNaee3ofMu4Gfk1vJOvNG3Ou6c2Pvr3djhp0vzah77fTm8c6/No/udV9TXs9rAFWAQf3\nbWeIXqL6I+Ak2g/kTefbKH2f9PN8pr4fjNT/Vn4a8LYN6s62cz/ae9yceN1P5OYvXEqSJEkdcdqJ\nJEmS1BGTb0mSJKkjJt+SJElSR0y+JUmSpI6YfEuSJEkdMfmWJEmSOmLyLUmSJHXE5FuSJEnqyP8H\nFQrXumVvb60AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114a45f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importance = xgb_model.get_fscore()\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "importance_df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "\n",
    "# Plot Feature Importance\n",
    "plt.figure()\n",
    "importance_df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an relatively small dataset. Therefore, we should do our feature selection based on a cross-\n",
    "validated set. We will check this assumption by comparing the scores on a cross-validated set vs the simple split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_cross, features_test_cross, labels_train_cross, labels_test_cross = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE for SVM - Balancing only on the training set, not the validation set  [This is for the traditional training -not the cross validated one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:75: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#further divide the 'traditional' non-cross set into training 80/20  for pure training and cross validation  \n",
    "features_train_notoversampled, features_validate, labels_train_notoversampled, labels_validate = train_test_split(features_train, labels_train, test_size = .2, random_state=0)\n",
    "\n",
    "sm = SMOTE(random_state=0, ratio = 1.0, kind= 'svm' )\n",
    "#x_train_res, y_train_res = sm.fit_sample(x_train, y_train)\n",
    "features_train_oversampled, labels_train_oversampled = sm.fit_sample(features_train_notoversampled, labels_train_notoversampled)\n",
    "\n",
    "#re-enter into original variables\n",
    "##features_train = features_train_oversampled\n",
    "##labels_train = labels_train_oversampled\n",
    "\n",
    "#Below 2 lines if we want to want to force the array back into dataframe    \n",
    "##features_train = pd.DataFrame(features_train_oversampled,columns=[\"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\"thalach\",\"exang\",\"oldpeak\",\"slop\",\"ca\",\"thal\"])\n",
    "##labels_train = pd.DataFrame(labels_train_oversampled,columns=[\"pred_attribute\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.13185208,  0.67015058, -1.27885728, ..., -0.27487371,\n",
       "         0.99176941, -0.84635221],\n",
       "       [ 0.07286213,  0.67015058,  1.57668306, ..., -0.27487371,\n",
       "        -1.0082989 ,  1.1815412 ],\n",
       "       [-0.03665734,  0.67015058, -0.70774921, ..., -0.27487371,\n",
       "        -1.0082989 ,  1.1815412 ],\n",
       "       ..., \n",
       "       [-2.11752735, -1.49220195,  0.32024531, ..., -0.27487371,\n",
       "         0.99176941, -0.84635221],\n",
       "       [-0.47473524,  0.67015058,  1.00557499, ..., -0.27487371,\n",
       "        -1.0082989 ,  1.1815412 ],\n",
       "       [ 0.51094003, -1.49220195,  2.37623435, ..., -0.27487371,\n",
       "         0.99176941, -0.84635221]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVC Models are only any good when the data is scaled. Lets scale the data and build the model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing.data import QuantileTransformer\n",
    "scaler = MinMaxScaler()\n",
    "Standard_scaler = StandardScaler()\n",
    "Robust_scaler = preprocessing.RobustScaler(quantile_range=(25, 75))\n",
    "Quantile_scalar = preprocessing.QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "features_train = Standard_scaler.fit_transform(features_train)\n",
    "features_test = Standard_scaler.transform(features_test)\n",
    "\n",
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import grid_search\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def checkmetrics(pred, labels_test, name):\n",
    "    print('The accuracy is of a', name, 'is: ', accuracy_score(pred, labels_test))\n",
    "    # print 'if everyone had 0 score: ', float(float(len(pred))-float(numberpoi))/float(len(pred))\n",
    "    matrix = confusion_matrix(labels_test, pred)\n",
    "#  print('There are', matrix[0][0], 'healthy people correctly identified vs', matrix[2][2] +matrix[3][3] +matrix[4][4] +matrix[1][1], 'sick ones. See:\\n', matrix)\n",
    "    print(matrix)\n",
    "    print(classification_report(pred, labels_test))\n",
    "    final_mse = mean_squared_error(labels_test, pred)\n",
    "    final_rmse = np.sqrt(final_mse)\n",
    "    print('mean square error', final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score using lasso:\n",
      "[(0.505, 'oldpeak'), (0.5, 'exang'), (0.485, 'thalach'), (0.375, 'fbs'), (0.215, 'restecg'), (0.005, 'ca'), (0.0, 'trestbps'), (0.0, 'thal'), (0.0, 'slope'), (0.0, 'sex'), (0.0, 'cp'), (0.0, 'class'), (0.0, 'chol'), (0.0, 'age')]\n",
      "Features sorted by their score using Linear Regression:\n",
      "[(1, 'class'), (3, 'oldpeak'), (5, 'exang'), (7, 'restecg'), (8, 'sex'), (9, 'thal'), (10, 'thalach'), (12, 'ca'), (17, 'slope'), (18, 'chol'), (19, 'cp'), (20, 'age'), (21, 'fbs'), (22, 'trestbps')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class RandomizedLasso is deprecated; The class RandomizedLasso is deprecated in 0.19 and will be removed in 0.21.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Feature selection using RFECV to pick best features,\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.feature_selection import RFECV\n",
    "rlasso = RandomizedLasso(alpha=0.025)\n",
    "names = features_list\n",
    "rlasso.fit(features_train, labels_train)\n",
    " \n",
    "print(\"Features sorted by their score using lasso:\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), rlasso.scores_), \n",
    "                 names), reverse=True))\n",
    "\n",
    "#use linear regression as the model\n",
    "lr = LinearRegression()\n",
    "#rank all features, i.e continue the elimination until the last one\n",
    "rfe = RFE(lr, n_features_to_select=1)\n",
    "rfe.fit(X,Y)\n",
    " \n",
    "print(\"Features sorted by their score using Linear Regression:\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score: 0.739669421488 with parameters: {'C': 4, 'class_weight': None, 'gamma': 1e-05}\n",
      "The accuracy is of a No SMOTE - sq hinge - Validate - support vector machine linear is:  0.573770491803\n",
      "[[31  2  1  1  0]\n",
      " [ 7  2  2  2  0]\n",
      " [ 4  1  1  0  0]\n",
      " [ 0  2  0  1  1]\n",
      " [ 0  1  0  2  0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      0.74      0.81        42\n",
      "        1.0       0.15      0.25      0.19         8\n",
      "        2.0       0.17      0.25      0.20         4\n",
      "        3.0       0.25      0.17      0.20         6\n",
      "        4.0       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.67      0.57      0.61        61\n",
      "\n",
      "mean square error 1.06355420218\n",
      "The train score for ovo: 0.739669421488 with parameters: {'C': 4, 'class_weight': None, 'decision_function_shape': 'ovo', 'gamma': 1e-05}\n",
      "The accuracy is of a No SMOTE - sq hinge, one vs one - Validate - support vector machine linear is:  0.573770491803\n",
      "[[31  2  1  1  0]\n",
      " [ 7  2  2  2  0]\n",
      " [ 4  1  1  0  0]\n",
      " [ 0  2  0  1  1]\n",
      " [ 0  1  0  2  0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      0.74      0.81        42\n",
      "        1.0       0.15      0.25      0.19         8\n",
      "        2.0       0.17      0.25      0.20         4\n",
      "        3.0       0.25      0.17      0.20         6\n",
      "        4.0       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.67      0.57      0.61        61\n",
      "\n",
      "mean square error 1.06355420218\n"
     ]
    }
   ],
   "source": [
    "parameters ={'C': [0.1,0.2,0.5,1,2,3,4,5], \n",
    "             'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5], \n",
    "             \"class_weight\": ['balanced', None]}\n",
    "SVM = svm.SVC(kernel=\"linear\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score:\", str(grid_search.score(features_train, labels_train)), \"with parameters:\", grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge - Validate - support vector machine linear')\n",
    "\n",
    "# Compare with one-versus all:\n",
    "parameters ={'C': [0.1,0.2,0.5,1,2,3,4,5],\n",
    "             \"class_weight\": ['balanced', None], \n",
    "             'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5], \n",
    "             'decision_function_shape': ['ovo', 'ovr']}\n",
    "SVM = svm.SVC(kernel=\"linear\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score for ovo:\", str(grid_search.score(features_train, labels_train)), 'with parameters:', grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge, one vs one - Validate - support vector machine linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different model (standard one vs. rest) loss not automatically being squared hinge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score:  0.644628099174 with parameters: {'C': 0.1, 'class_weight': None}\n",
      "The accuracy is of a No SMOTE - hinge - Validate - support vector machine linear is:  0.573770491803\n",
      "[[33  0  1  1  0]\n",
      " [ 9  0  0  3  1]\n",
      " [ 4  1  0  1  0]\n",
      " [ 2  0  0  2  0]\n",
      " [ 1  0  0  2  0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.94      0.67      0.79        49\n",
      "        1.0       0.00      0.00      0.00         1\n",
      "        2.0       0.00      0.00      0.00         1\n",
      "        3.0       0.50      0.22      0.31         9\n",
      "        4.0       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.83      0.57      0.68        61\n",
      "\n",
      "mean square error 1.26101702384\n",
      "The train score:  0.669421487603 with parameters: {'C': 0.2, 'class_weight': None, 'multi_class': 'ovr'}\n",
      "The accuracy is of a No SMOTE - hinge, one vs rest - Validate - support vector machine linear is:  0.573770491803\n",
      "[[33  1  0  1  0]\n",
      " [ 8  0  0  4  1]\n",
      " [ 4  1  0  1  0]\n",
      " [ 1  1  0  2  0]\n",
      " [ 1  0  0  2  0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.94      0.70      0.80        47\n",
      "        1.0       0.00      0.00      0.00         3\n",
      "        2.0       0.00      0.00      0.00         0\n",
      "        3.0       0.50      0.20      0.29        10\n",
      "        4.0       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.81      0.57      0.67        61\n",
      "\n",
      "mean square error 1.22808660986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters ={'C': [0.1,0.2,0.5,1,2,3,4,5],\n",
    "            \"class_weight\": ['balanced', None]}\n",
    "SVM = LinearSVC(loss=\"hinge\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score: \", str(grid_search.score(features_train, labels_train)), 'with parameters:', grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - hinge - Validate - support vector machine linear')\n",
    "\n",
    "# Compare with Cramer:\n",
    "parameters ={'C': [0.1,0.2,0.5,1,2,3,4,5],\n",
    "            \"class_weight\": ['balanced', None],\n",
    "             'multi_class':['ovr', 'crammer_singer']}\n",
    "SVM = LinearSVC(loss=\"hinge\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score: \", str(grid_search.score(features_train, labels_train)), 'with parameters:', grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - hinge, one vs rest - Validate - support vector machine linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification\n",
    "Polynominal features\n",
    "\n",
    "Note that when there are multiple features, Polynomial Regression is capable of finding relationships\n",
    "between features (which is something a plain Linear Regression model cannot do). This is made possible\n",
    "by the fact that PolynomialFeatures also adds all combinations of features up to the given degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score: 0.801652892562 with parameters: {'C': 0.5, 'class_weight': None, 'coef0': 10, 'degree': 2, 'gamma': 0.05}\n",
      "The accuracy is of a No SMOTE - sq hinge - Validate - support vector machine linear is:  0.590163934426\n",
      "[[30  3  1  1  0]\n",
      " [ 7  3  0  3  0]\n",
      " [ 4  0  2  0  0]\n",
      " [ 0  1  1  1  1]\n",
      " [ 0  1  0  2  0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.86      0.73      0.79        41\n",
      "        1.0       0.23      0.38      0.29         8\n",
      "        2.0       0.33      0.50      0.40         4\n",
      "        3.0       0.25      0.14      0.18         7\n",
      "        4.0       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.66      0.59      0.62        61\n",
      "\n",
      "mean square error 1.05581915988\n"
     ]
    }
   ],
   "source": [
    "# HOW GRIDSEARCH SHOULD WORK\n",
    "parameters ={'C': [0.1,0.2,0.5,1,2,3,4,5], \n",
    "             'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5], \n",
    "             \"class_weight\": ['balanced', None],\n",
    "             \"degree\": [1,2,3],\n",
    "             \"coef0\": [1,10]}\n",
    "SVM = svm.SVC(kernel=\"poly\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score:\", str(grid_search.score(features_train, labels_train)), \"with parameters:\", grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge - Validate - support vector machine linear')\n",
    "\n",
    "# No need to compare with one vs all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score: 0.648760330579 with parameters: {'C': 1, 'class_weight': None, 'gamma': 0.01}\n",
      "The accuracy is of a No SMOTE - sq hinge - Validate - support vector machine linear is:  0.672131147541\n",
      "[[34  1  0  0  0]\n",
      " [ 8  5  0  0  0]\n",
      " [ 4  2  0  0  0]\n",
      " [ 0  2  0  2  0]\n",
      " [ 1  1  0  1  0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.72      0.83        47\n",
      "        1.0       0.38      0.45      0.42        11\n",
      "        2.0       0.00      0.00      0.00         0\n",
      "        3.0       0.50      0.67      0.57         3\n",
      "        4.0       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.84      0.67      0.74        61\n",
      "\n",
      "mean square error 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Just like the polynomial features method, the similarity features method can be useful with any Machine\n",
    "# Learning algorithm, but it may be computationally expensive to compute all the additional features,\n",
    "# especially on large training sets. However, once again the kernel trick does its SVM magic: it makes it\n",
    "# possible to obtain a similar result as if you had added many similarity features, without actually having to\n",
    "# add them\n",
    "\n",
    "parameters ={'C': [0.1,0.2,0.5,1,2,3,4,5], \n",
    "             'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5], \n",
    "             \"class_weight\": ['balanced', None]}\n",
    "SVM = svm.SVC(kernel=\"rbf\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score:\", str(grid_search.score(features_train, labels_train)), \"with parameters:\", grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge - Validate - support vector machine linear')\n",
    "\n",
    "# no need to compare with one vs all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
