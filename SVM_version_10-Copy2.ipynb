{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/ggplot/utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/ggplot/stats/smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE #for SMOTE -> install package using: conda install -c conda-forge imbalanced-learn \n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import ggplot\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "import pylab as pl\n",
    "from itertools import cycle\n",
    "from sklearn import cross_validation\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "features_list = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','class']\n",
    "dataset1=pd.read_csv(\"Heart_Disease_Data.csv\")\n",
    "\n",
    "dataset1 = dataset1.convert_objects(convert_numeric=True)\n",
    "dataset1.astype('float')\n",
    "\n",
    "dataset1 = dataset1.fillna(value=0)\n",
    "\n",
    "# based on https://pdfs.semanticscholar.org/daa0/f01f96a89fcfc5f41a2da67fb2a8966900ab.pdf\n",
    "Genetic_Based_Decision = dataset1[['cp','trestbps', 'restecg', 'thalach', 'ca', 'thal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# SVM requires that each data instance is represented as a vector of real numbers\n",
    "# If you already have numeric dtypes (int8|16|32|64,float64,boolean) you can convert it to another \"numeric\" dtype using Pandas .astype() method. Demo: In [90]: df = pd.DataFrame(np.random.randint(10**5,10**7,(5,3)),columns=list('abc'), dtype=np.int64) In [91]: df Out[91]: a b c 0 9059440 9590567 2076918 1 5861102 4566089 1947323 2 6636568 162770 2487991 3 6794572 5236903 5628779 4 470121 4044395 4546794 In [92]: df.dtypes Out[92]: a int64 b int64 c int64 dtype: object In [93]: df['a'] = df['a'].astype(float) In [94]: df.dtypes Out[94]: a float64 b int64 c int64 dtype: object It won't work for object (string) dtypes, that can't be converted to numbers: In [95]: df.loc[1, 'b'] = 'XXXXXX' In [96]: df Out[96]:...\n",
    "# Just make everything numeric for ease\n",
    "dataset1 = dataset1.convert_objects(convert_numeric=True)\n",
    "dataset1 = dataset1.astype('float')\n",
    "\n",
    "# Two variables are discrete/ordinal: ca (number of major vessels colored by fluoroscopy) and num (diagnosis of heart disease)\n",
    "# Three can be directly viewed as 1 hot (because binary): 'sex':'male', 'fbs':'fasting blood sugar', 'exang':'exercise induced angina'\n",
    "\n",
    "# which leaves 4 for one-hot encoding. problem is that the values aren't unique, so have to manually\n",
    "# make extra columns:\n",
    "\n",
    "dataset1[\"cp\"] = dataset1[\"cp\"].replace([1,2,3,4], [\"typical angina\", \"atypical angina\", \"non-angina\", \"asymptomatic angina\"])\n",
    "dataset1[\"restecg\"] = dataset1[\"restecg\"].replace([0,1,2], [\"normalresecg\", \"ST-T wave abnormality\", \"left ventricular hypertrophy\"])\n",
    "dataset1[\"slop\"] = dataset1[\"slop\"].replace([1,2,3], [\"upsloping\", \"flat\", \"downsloping\"])\n",
    "dataset1[\"thal\"] = dataset1[\"thal\"].replace([3,6,7], [\"normalthal\", \"fixed defect\", \"reversible defect\"])\n",
    "\n",
    "x = dataset1[['cp', 'restecg', 'slop', 'thal']]\n",
    "for column in ['cp', 'restecg', 'slop', 'thal']:\n",
    "    one_hot = pd.get_dummies(dataset1[column])\n",
    "    dataset1 = dataset1.drop(column, axis=1)\n",
    "    dataset1 = dataset1.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b689622a-a475-40a8-bd33-e2b5e018d528",
    "_uuid": "2e13a97aaee5c269ff8f21fd7b66135927b66156"
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing:\n",
    "dataset1.dropna(inplace=True, axis=0, how=\"any\")\n",
    "Y=dataset1[\"pred_attribute\"]\n",
    "dataset1 = dataset1.drop(\"pred_attribute\", axis=1)\n",
    "X=dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "2dacb7c2-6d4e-4a17-b5cb-d6ecd821c923",
    "_uuid": "c3355ae680b60b0daa713153f05b3709193af7f6"
   },
   "outputs": [],
   "source": [
    "# evaluate the model by splitting into train and test sets  #Edit by ryan, we aim to do 3 traditional sets in the end, this first split is 80/20\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "5065d9ba-be53-4431-bb86-152ee1673550",
    "_uuid": "00229a787842594dee105c79de5871548613a045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 129, 1.0: 42, 3.0: 31, 2.0: 30, 4.0: 10})\n",
      "Counter({0.0: 35, 1.0: 13, 2.0: 6, 3.0: 4, 4.0: 3})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "list1 = []\n",
    "for i in labels_train:\n",
    "    list1.append(i)\n",
    "counter=collections.Counter(list1)\n",
    "print(counter)\n",
    "\n",
    "list2 = []\n",
    "for i in labels_test:\n",
    "    list2.append(i)\n",
    "counter=collections.Counter(list2)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7986798679867987\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print(len(features_train)/(len(features_train)+ len(features_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an relatively small dataset. Therefore, we should do our feature selection based on a cross-\n",
    "validated set. We will check this assumption by comparing the scores on a cross-validated set vs the simple split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_cross, features_test_cross, labels_train_cross, labels_test_cross = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE for SVM - Balancing only on the training set, not the validation set  [This is for the traditional training -not the cross validated one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:75: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#further divide the 'traditional' non-cross set into training 80/20  for pure training and cross validation  \n",
    "features_train_notoversampled, features_validate, labels_train_notoversampled, labels_validate = train_test_split(features_train, labels_train, test_size = .2, random_state=0)\n",
    "\n",
    "sm = SMOTE(random_state=0, ratio = 1.0, kind= 'svm' )\n",
    "#x_train_res, y_train_res = sm.fit_sample(x_train, y_train)\n",
    "features_train_oversampled, labels_train_oversampled = sm.fit_sample(features_train_notoversampled, labels_train_notoversampled)\n",
    "\n",
    "#re-enter into original variables\n",
    "##features_train = features_train_oversampled\n",
    "##labels_train = labels_train_oversampled\n",
    "\n",
    "#Below 2 lines if we want to want to force the array back into dataframe    \n",
    "##features_train = pd.DataFrame(features_train_oversampled,columns=[\"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\"thalach\",\"exang\",\"oldpeak\",\"slop\",\"ca\",\"thal\"])\n",
    "##labels_train = pd.DataFrame(labels_train_oversampled,columns=[\"pred_attribute\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.13185208,  0.67015058, -1.27885728, ..., -0.27487371,\n",
       "         0.99176941, -0.84635221],\n",
       "       [ 0.07286213,  0.67015058,  1.57668306, ..., -0.27487371,\n",
       "        -1.0082989 ,  1.1815412 ],\n",
       "       [-0.03665734,  0.67015058, -0.70774921, ..., -0.27487371,\n",
       "        -1.0082989 ,  1.1815412 ],\n",
       "       ...,\n",
       "       [-2.11752735, -1.49220195,  0.32024531, ..., -0.27487371,\n",
       "         0.99176941, -0.84635221],\n",
       "       [-0.47473524,  0.67015058,  1.00557499, ..., -0.27487371,\n",
       "        -1.0082989 ,  1.1815412 ],\n",
       "       [ 0.51094003, -1.49220195,  2.37623435, ..., -0.27487371,\n",
       "         0.99176941, -0.84635221]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVC Models are only any good when the data is scaled. Lets scale the data and build the model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing.data import QuantileTransformer\n",
    "scaler = MinMaxScaler()\n",
    "Standard_scaler = StandardScaler()\n",
    "Robust_scaler = preprocessing.RobustScaler(quantile_range=(25, 75))\n",
    "Quantile_scalar = preprocessing.QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "features_train = Standard_scaler.fit_transform(features_train)\n",
    "features_test = Standard_scaler.transform(features_test)\n",
    "\n",
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import grid_search\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def checkmetrics(pred, labels_test, name):\n",
    "    print('The accuracy is of a', name, 'is: ', accuracy_score(pred, labels_test))\n",
    "    # print 'if everyone had 0 score: ', float(float(len(pred))-float(numberpoi))/float(len(pred))\n",
    "    matrix = confusion_matrix(labels_test, pred)\n",
    "#  print('There are', matrix[0][0], 'healthy people correctly identified vs', matrix[2][2] +matrix[3][3] +matrix[4][4] +matrix[1][1], 'sick ones. See:\\n', matrix)\n",
    "    print(matrix)\n",
    "    print(classification_report(pred, labels_test))\n",
    "    final_mse = mean_squared_error(labels_test, pred)\n",
    "    final_rmse = np.sqrt(final_mse)\n",
    "    print('mean square error', final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score using lasso:\n",
      "[(0.495, 'oldpeak'), (0.46, 'exang'), (0.45, 'thalach'), (0.41, 'restecg'), (0.245, 'fbs'), (0.005, 'ca'), (0.0, 'trestbps'), (0.0, 'thal'), (0.0, 'slope'), (0.0, 'sex'), (0.0, 'cp'), (0.0, 'class'), (0.0, 'chol'), (0.0, 'age')]\n",
      "Features sorted by their score using Linear Regression:\n",
      "[(1, 'oldpeak'), (2, 'class'), (6, 'exang'), (7, 'restecg'), (8, 'sex'), (9, 'thal'), (10, 'thalach'), (14, 'ca'), (18, 'slope'), (19, 'chol'), (20, 'cp'), (21, 'age'), (22, 'fbs'), (23, 'trestbps')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class RandomizedLasso is deprecated; The class RandomizedLasso is deprecated in 0.19 and will be removed in 0.21.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Feature selection using RFECV to pick best features,\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.feature_selection import RFECV\n",
    "rlasso = RandomizedLasso(alpha=0.025)\n",
    "names = features_list\n",
    "rlasso.fit(features_train, labels_train)\n",
    " \n",
    "print(\"Features sorted by their score using lasso:\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), rlasso.scores_), \n",
    "                 names), reverse=True))\n",
    "\n",
    "#use linear regression as the model\n",
    "lr = LinearRegression()\n",
    "#rank all features, i.e continue the elimination until the last one\n",
    "rfe = RFE(lr, n_features_to_select=1)\n",
    "rfe.fit(X,Y)\n",
    " \n",
    "print(\"Features sorted by their score using Linear Regression:\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score: 0.7396694214876033 with parameters: {'C': 4, 'class_weight': None, 'gamma': 1e-05}\n",
      "The accuracy is of a No SMOTE - sq hinge - Validate - support vector machine linear is:  0.5737704918032787\n",
      "[[31  2  1  1  0]\n",
      " [ 7  2  2  2  0]\n",
      " [ 4  1  1  0  0]\n",
      " [ 0  2  0  1  1]\n",
      " [ 0  1  0  2  0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      0.74      0.81        42\n",
      "        1.0       0.15      0.25      0.19         8\n",
      "        2.0       0.17      0.25      0.20         4\n",
      "        3.0       0.25      0.17      0.20         6\n",
      "        4.0       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.67      0.57      0.61        61\n",
      "\n",
      "mean square error 1.06355420218417\n",
      "The train score for ovo: 0.7396694214876033 with parameters: {'C': 4, 'class_weight': None, 'decision_function_shape': 'ovo', 'gamma': 1e-05}\n",
      "The accuracy is of a No SMOTE - sq hinge, one vs one - Validate - support vector machine linear is:  0.5737704918032787\n",
      "[[31  2  1  1  0]\n",
      " [ 7  2  2  2  0]\n",
      " [ 4  1  1  0  0]\n",
      " [ 0  2  0  1  1]\n",
      " [ 0  1  0  2  0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      0.74      0.81        42\n",
      "        1.0       0.15      0.25      0.19         8\n",
      "        2.0       0.17      0.25      0.20         4\n",
      "        3.0       0.25      0.17      0.20         6\n",
      "        4.0       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.67      0.57      0.61        61\n",
      "\n",
      "mean square error 1.06355420218417\n"
     ]
    }
   ],
   "source": [
    "parameters ={'C': [0.1,0.2,0.5,1,2,3,4,5], \n",
    "             'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5], \n",
    "             \"class_weight\": ['balanced', None]}\n",
    "SVM = svm.SVC(kernel=\"linear\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score:\", str(grid_search.score(features_train, labels_train)), \"with parameters:\", grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge - Validate - support vector machine linear')\n",
    "\n",
    "# Compare with one-versus all:\n",
    "parameters ={'C': [0.1,0.2,0.5,1,2,3,4,5],\n",
    "             \"class_weight\": ['balanced', None], \n",
    "             'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5], \n",
    "             'decision_function_shape': ['ovo', 'ovr']}\n",
    "SVM = svm.SVC(kernel=\"linear\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score for ovo:\", str(grid_search.score(features_train, labels_train)), 'with parameters:', grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge, one vs one - Validate - support vector machine linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different model (standard one vs. rest) loss not automatically being squared hinge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score:  0.6570247933884298 with parameters: {'C': 0.5, 'class_weight': None}\n",
      "The accuracy is of a No SMOTE - hinge - Validate - support vector machine linear is:  0.5573770491803278\n",
      "[[33  0  1  1  0]\n",
      " [ 8  0  1  3  1]\n",
      " [ 4  0  0  1  1]\n",
      " [ 0  1  0  1  2]\n",
      " [ 1  0  0  2  0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.94      0.72      0.81        46\n",
      "        1.0       0.00      0.00      0.00         1\n",
      "        2.0       0.00      0.00      0.00         2\n",
      "        3.0       0.25      0.12      0.17         8\n",
      "        4.0       0.00      0.00      0.00         4\n",
      "\n",
      "avg / total       0.74      0.56      0.64        61\n",
      "\n",
      "mean square error 1.201092398951751\n",
      "The train score:  0.6446280991735537 with parameters: {'C': 0.1, 'class_weight': None, 'multi_class': 'ovr'}\n",
      "The accuracy is of a No SMOTE - hinge, one vs rest - Validate - support vector machine linear is:  0.5737704918032787\n",
      "[[33  0  1  1  0]\n",
      " [ 9  0  1  3  0]\n",
      " [ 4  0  1  1  0]\n",
      " [ 2  0  1  1  0]\n",
      " [ 1  0  0  2  0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.94      0.67      0.79        49\n",
      "        1.0       0.00      0.00      0.00         0\n",
      "        2.0       0.17      0.25      0.20         4\n",
      "        3.0       0.25      0.12      0.17         8\n",
      "        4.0       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.80      0.57      0.67        61\n",
      "\n",
      "mean square error 1.2078975094943374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters ={'C': [0.1,0.2,0.5,1,2,3,4,5],\n",
    "            \"class_weight\": ['balanced', None]}\n",
    "SVM = LinearSVC(loss=\"hinge\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score: \", str(grid_search.score(features_train, labels_train)), 'with parameters:', grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - hinge - Validate - support vector machine linear')\n",
    "\n",
    "# Compare with Cramer:\n",
    "parameters ={'C': [0.1,0.2,0.5,1,2,3,4,5],\n",
    "            \"class_weight\": ['balanced', None],\n",
    "             'multi_class':['ovr', 'crammer_singer']}\n",
    "SVM = LinearSVC(loss=\"hinge\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score: \", str(grid_search.score(features_train, labels_train)), 'with parameters:', grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - hinge, one vs rest - Validate - support vector machine linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification\n",
    "Polynominal features\n",
    "\n",
    "Note that when there are multiple features, Polynomial Regression is capable of finding relationships\n",
    "between features (which is something a plain Linear Regression model cannot do). This is made possible\n",
    "by the fact that PolynomialFeatures also adds all combinations of features up to the given degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score: 0.8016528925619835 with parameters: {'C': 0.5, 'class_weight': None, 'coef0': 10, 'degree': 2, 'gamma': 0.05}\n",
      "The accuracy is of a No SMOTE - sq hinge - Validate - support vector machine linear is:  0.5901639344262295\n",
      "[[30  3  1  1  0]\n",
      " [ 7  3  0  3  0]\n",
      " [ 4  0  2  0  0]\n",
      " [ 0  1  1  1  1]\n",
      " [ 0  1  0  2  0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.86      0.73      0.79        41\n",
      "        1.0       0.23      0.38      0.29         8\n",
      "        2.0       0.33      0.50      0.40         4\n",
      "        3.0       0.25      0.14      0.18         7\n",
      "        4.0       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.66      0.59      0.62        61\n",
      "\n",
      "mean square error 1.0558191598757127\n"
     ]
    }
   ],
   "source": [
    "# HOW GRIDSEARCH SHOULD WORK\n",
    "parameters ={'C': [0.1,0.2,0.5,1,2,3,4,5], \n",
    "             'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5], \n",
    "             \"class_weight\": ['balanced', None],\n",
    "             \"degree\": [1,2,3],\n",
    "             \"coef0\": [1,10]}\n",
    "SVM = svm.SVC(kernel=\"poly\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score:\", str(grid_search.score(features_train, labels_train)), \"with parameters:\", grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge - Validate - support vector machine linear')\n",
    "\n",
    "# No need to compare with one vs all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score: 0.6487603305785123 with parameters: {'C': 1, 'class_weight': None, 'gamma': 0.01}\n",
      "The accuracy is of a No SMOTE - sq hinge - Validate - support vector machine linear is:  0.6721311475409836\n",
      "[[34  1  0  0  0]\n",
      " [ 8  5  0  0  0]\n",
      " [ 4  2  0  0  0]\n",
      " [ 0  2  0  2  0]\n",
      " [ 1  1  0  1  0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.72      0.83        47\n",
      "        1.0       0.38      0.45      0.42        11\n",
      "        2.0       0.00      0.00      0.00         0\n",
      "        3.0       0.50      0.67      0.57         3\n",
      "        4.0       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.84      0.67      0.74        61\n",
      "\n",
      "mean square error 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidleonardi/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Just like the polynomial features method, the similarity features method can be useful with any Machine\n",
    "# Learning algorithm, but it may be computationally expensive to compute all the additional features,\n",
    "# especially on large training sets. However, once again the kernel trick does its SVM magic: it makes it\n",
    "# possible to obtain a similar result as if you had added many similarity features, without actually having to\n",
    "# add them\n",
    "\n",
    "parameters ={'C': [0.1,0.2,0.5,1,2,3,4,5], \n",
    "             'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5], \n",
    "             \"class_weight\": ['balanced', None]}\n",
    "SVM = svm.SVC(kernel=\"rbf\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score:\", str(grid_search.score(features_train, labels_train)), \"with parameters:\", grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge - Validate - support vector machine linear')\n",
    "\n",
    "# no need to compare with one vs all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
