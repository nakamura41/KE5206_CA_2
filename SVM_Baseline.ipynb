{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\ggplot\\utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\ggplot\\stats\\smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import SMOTE #for SMOTE -> install package using: conda install -c conda-forge imbalanced-learn\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import ggplot\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC\n",
    "#from sklearn.svm import SVR #just Testing for regression on other continous data of dataset\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "\n",
    "import pylab as pl\n",
    "from itertools import cycle\n",
    "from sklearn import cross_validation\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "features_list = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','class']\n",
    "dataset1=pd.read_csv(\"data/Heart_Disease_Data.csv\")\n",
    "#dataset1=pd.read_csv(\"Balanced_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# SVM requires that each data instance is represented as a vector of real numbers\n",
    "# If you already have numeric dtypes (int8|16|32|64,float64,boolean) you can convert it to another \"numeric\" dtype using Pandas .astype() method. Demo: In [90]: df = pd.DataFrame(np.random.randint(10**5,10**7,(5,3)),columns=list('abc'), dtype=np.int64) In [91]: df Out[91]: a b c 0 9059440 9590567 2076918 1 5861102 4566089 1947323 2 6636568 162770 2487991 3 6794572 5236903 5628779 4 470121 4044395 4546794 In [92]: df.dtypes Out[92]: a int64 b int64 c int64 dtype: object In [93]: df['a'] = df['a'].astype(float) In [94]: df.dtypes Out[94]: a float64 b int64 c int64 dtype: object It won't work for object (string) dtypes, that can't be converted to numbers: In [95]: df.loc[1, 'b'] = 'XXXXXX' In [96]: df Out[96]:...\n",
    "# Just make everything numeric for ease, later we will convert to ordinal/one-hot encoding.\n",
    "dataset1 = dataset1.convert_objects(convert_numeric=True)\n",
    "dataset1 = dataset1.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count missing value in terms of colunms #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age               False\n",
      "sex               False\n",
      "cp                False\n",
      "trestbps          False\n",
      "chol              False\n",
      "fbs               False\n",
      "restecg           False\n",
      "thalach           False\n",
      "exang             False\n",
      "oldpeak           False\n",
      "slop              False\n",
      "ca                 True\n",
      "thal               True\n",
      "pred_attribute    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "#dataset.shape[0] - dataset.count()\n",
    "print(dataset1.isnull().any())\n",
    "dataset1 = dataset1.replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://pdfs.semanticscholar.org/daa0/f01f96a89fcfc5f41a2da67fb2a8966900ab.pdf \n",
    "# these features, based on reading, may be important but have to be confirmed too by statistical methods:\n",
    "Genetic_Based_Decision = dataset1[['cp','trestbps', 'restecg', 'thalach', 'ca', 'thal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two variables are discrete/ordinal: ca (number of major vessels colored by fluoroscopy) and num (diagnosis of heart disease)\n",
    "# Three can be directly viewed as 1 hot (because binary): 'sex':'male', 'fbs':'fasting blood sugar', 'exang':'exercise induced angina'\n",
    "\n",
    "# which leaves 4 for one-hot encoding. problem is that the values aren't unique, so have to manually\n",
    "# make extra columns:\n",
    "\n",
    "dataset1[\"cp\"] = dataset1[\"cp\"].replace([1,2,3,4], [\"typical angina\", \"atypical angina\", \"non-angina\", \"asymptomatic angina\"])\n",
    "dataset1[\"restecg\"] = dataset1[\"restecg\"].replace([0,1,2], [\"normalresecg\", \"ST-T wave abnormality\", \"left ventricular hypertrophy\"])\n",
    "dataset1[\"slop\"] = dataset1[\"slop\"].replace([1,2,3], [\"upsloping\", \"flat\", \"downsloping\"])\n",
    "dataset1[\"thal\"] = dataset1[\"thal\"].replace([3,6,7], [\"normalthal\", \"fixed defect\", \"reversible defect\"])\n",
    "\n",
    "x = dataset1[['cp', 'restecg', 'slop', 'thal']]\n",
    "for column in ['cp', 'restecg', 'slop', 'thal']:\n",
    "    one_hot = pd.get_dummies(dataset1[column])\n",
    "    dataset1 = dataset1.drop(column, axis=1)\n",
    "    dataset1 = dataset1.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b689622a-a475-40a8-bd33-e2b5e018d528",
    "_uuid": "2e13a97aaee5c269ff8f21fd7b66135927b66156"
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing:\n",
    "dataset1.dropna(inplace=True, axis=0, how=\"any\")\n",
    "Y=dataset1[\"pred_attribute\"]\n",
    "dataset1 = dataset1.drop(\"pred_attribute\", axis=1)\n",
    "X=dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "2dacb7c2-6d4e-4a17-b5cb-d6ecd821c923",
    "_uuid": "c3355ae680b60b0daa713153f05b3709193af7f6"
   },
   "outputs": [],
   "source": [
    "# evaluate the model by splitting into train and test sets  #Edit by ryan, we aim to do 3 traditional sets in the end, this first split is 80/20\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_df = pd.DataFrame(features_train)\n",
    "#features_train_df.to_csv('features_train.csv', index=False)\n",
    "\n",
    "features_test_df = pd.DataFrame(features_test)\n",
    "#features_test_df.to_csv('features_test.csv', index=False)\n",
    "\n",
    "labels_train_df = pd.DataFrame(labels_train)\n",
    "labels_train_df.to_csv('data/labels_train.csv', index=False)\n",
    "\n",
    "labels_test_df = pd.DataFrame(labels_test)\n",
    "labels_test_df.to_csv('data/labels_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling And Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an relatively small dataset. Therefore, we should do our feature selection based on a cross-\n",
    "validated set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler (by David)\n",
    "SVC Models are only any good when the data is scaled. Lets scale the data and build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8045168 ,  0.69445626, -1.08367795, ..., -0.23570226,\n",
       "         0.93062959, -0.81975606],\n",
       "       [ 0.53527611,  0.69445626,  2.08476008, ..., -0.23570226,\n",
       "        -1.07454138,  1.21987509],\n",
       "       [ 0.98187375, -1.43997549, -1.30219091, ..., -0.23570226,\n",
       "         0.93062959, -0.81975606],\n",
       "       ..., \n",
       "       [ 0.53527611,  0.69445626,  0.44591283, ..., -0.23570226,\n",
       "        -1.07454138,  1.21987509],\n",
       "       [-0.9161662 ,  0.69445626,  0.44591283, ..., -0.23570226,\n",
       "        -1.07454138,  1.21987509],\n",
       "       [ 0.3119773 , -1.43997549, -0.20962608, ..., -0.23570226,\n",
       "         0.93062959, -0.81975606]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing.data import QuantileTransformer\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "Standard_scaler = StandardScaler()\n",
    "Robust_scaler = preprocessing.RobustScaler(quantile_range=(25, 75))\n",
    "Quantile_scalar = preprocessing.QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "features_train = Standard_scaler.fit_transform(features_train)\n",
    "features_test = Standard_scaler.transform(features_test)\n",
    "\n",
    "\n",
    "features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing only on the training set, not the validation set\n",
    "Unfortunately SMOTE categorial implementation is not really implemented\n",
    "We will do simple oversampling -> Done using external program SPSS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Export data to files for external program to balance the data\n",
    "\n",
    "#without features selection\n",
    "merged = np.concatenate((features_train, labels_train.reshape((-1, 1))), axis=1)\n",
    "merged_df = pd.DataFrame(merged) \n",
    "merged_df.to_csv(\"data/train_NoEng_NB.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "#features_train_009\n",
    " \n",
    "#features_train_04 \n",
    "\n",
    "#features_train_009_pca \n",
    "\n",
    "#features_train_04_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import balanced train data back\n",
    "\n",
    "#without features selection\n",
    "\n",
    "df=pd.read_csv(\"data/train_noEng_Balanced.csv\")\n",
    "\n",
    "#df.drop(df.columns[[-1,]], axis=1, inplace=True)\n",
    "test_df = df.iloc[:, 22:23]\n",
    "df.drop(df.columns[[-1,]], axis=1, inplace=True)\n",
    "\n",
    "features_train = df.as_matrix\n",
    "\n",
    "#test_df = df.iloc[:, 22:23]\n",
    "#df = df.iloc[:, :-1]\n",
    "\n",
    "df\n",
    "\n",
    "features_test\n",
    "\n",
    "\n",
    "\n",
    "test_df.to_csv(\"data/train.csv\", index=False)\n",
    "#features_train_009\n",
    " \n",
    "#features_train_04 \n",
    "\n",
    "#features_train_009_pca \n",
    "\n",
    "#features_train_04_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.as_matrix of             0         1         2         3         4         5         6  \\\n",
       "0   -0.804517  0.694456 -1.083678 -0.798812 -0.401386 -0.233713 -0.748132   \n",
       "1    0.535276  0.694456  2.084760  1.477208 -0.401386 -0.361359  1.336663   \n",
       "2    0.535276  0.694456  2.084760  1.477208 -0.401386 -0.361359  1.336663   \n",
       "3    0.535276  0.694456  2.084760  1.477208 -0.401386 -0.361359  1.336663   \n",
       "4    0.535276  0.694456  2.084760  1.477208 -0.401386 -0.361359  1.336663   \n",
       "5    0.981874 -1.439975 -1.302191  0.413822 -0.401386  0.872555  1.336663   \n",
       "6    0.981874 -1.439975 -1.302191  0.413822 -0.401386  0.872555  1.336663   \n",
       "7    0.311977  0.694456  0.992195  0.544413 -0.401386 -1.552725  1.336663   \n",
       "8    0.311977  0.694456  0.992195  0.544413 -0.401386 -1.552725  1.336663   \n",
       "9   -0.692867  0.694456 -1.192934 -0.332414 -0.401386  0.830006 -0.748132   \n",
       "10  -0.692867  0.694456 -1.192934 -0.332414 -0.401386  0.830006 -0.748132   \n",
       "11  -0.692867  0.694456 -1.192934 -0.332414 -0.401386  0.830006 -0.748132   \n",
       "12   1.205173  0.694456  0.172772  0.133983 -0.401386 -0.914493 -0.748132   \n",
       "13   1.205173  0.694456  0.172772  0.133983 -0.401386 -0.914493 -0.748132   \n",
       "14   1.205173  0.694456  0.172772  0.133983 -0.401386 -0.914493 -0.748132   \n",
       "15   1.205173  0.694456  0.172772  0.133983 -0.401386 -0.914493 -0.748132   \n",
       "16   0.758575  0.694456  0.445913 -0.742844 -0.401386 -0.446457  1.336663   \n",
       "17   0.758575  0.694456  0.445913 -0.742844 -0.401386 -0.446457  1.336663   \n",
       "18   0.758575  0.694456  0.445913 -0.742844 -0.401386 -0.446457  1.336663   \n",
       "19   1.316822  0.694456  1.538478 -0.351070 -0.401386 -0.446457 -0.748132   \n",
       "20  -0.022971  0.694456  0.992195 -0.276447 -0.401386  0.702360 -0.748132   \n",
       "21  -2.255959  0.694456 -0.755908 -1.209242 -0.401386  1.085299 -0.748132   \n",
       "22  -0.916166  0.694456  0.992195 -0.295102 -0.401386 -0.063518 -0.748132   \n",
       "23  -0.916166  0.694456  0.992195 -0.295102 -0.401386 -0.063518 -0.748132   \n",
       "24  -0.916166  0.694456  0.992195 -0.295102 -0.401386 -0.063518 -0.748132   \n",
       "25   0.646926  0.694456 -0.810537 -0.313758  2.491364  0.489616  1.336663   \n",
       "26   0.646926  0.694456 -0.810537 -0.313758  2.491364  0.489616  1.336663   \n",
       "27   0.646926  0.694456 -0.810537 -0.313758  2.491364  0.489616  1.336663   \n",
       "28   0.646926  0.694456 -0.810537 -0.313758  2.491364  0.489616  1.336663   \n",
       "29   0.870224 -1.439975  0.445913  2.745810 -0.401386  0.361970 -0.748132   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "517  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n",
       "518  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n",
       "519  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n",
       "520  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n",
       "521  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n",
       "522  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n",
       "523  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n",
       "524  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n",
       "525  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n",
       "526  0.311977 -1.439975 -0.100370 -0.201823 -0.401386  1.085299 -0.748132   \n",
       "527  0.311977 -1.439975 -0.100370 -0.201823 -0.401386  1.085299 -0.748132   \n",
       "528  0.311977 -1.439975 -0.100370 -0.201823 -0.401386  1.085299 -0.748132   \n",
       "529  1.093523  0.694456 -1.192934 -0.668220 -0.401386 -0.191164  1.336663   \n",
       "530  1.651770  0.694456  0.445913  0.133983 -0.401386 -0.106067 -0.748132   \n",
       "531  1.651770  0.694456  0.445913  0.133983 -0.401386 -0.106067 -0.748132   \n",
       "532  1.651770  0.694456  0.445913  0.133983 -0.401386 -0.106067 -0.748132   \n",
       "533  1.651770  0.694456  0.445913  0.133983 -0.401386 -0.106067 -0.748132   \n",
       "534  1.428471  0.694456 -0.373511  0.133983  2.491364  0.617263 -0.748132   \n",
       "535  1.428471  0.694456 -0.373511  0.133983  2.491364  0.617263 -0.748132   \n",
       "536  1.428471  0.694456 -0.373511  0.133983  2.491364  0.617263 -0.748132   \n",
       "537  1.428471  0.694456 -0.373511  0.133983  2.491364  0.617263 -0.748132   \n",
       "538  0.535276  0.694456  0.445913 -1.302521 -0.401386  0.574714  1.336663   \n",
       "539  0.535276  0.694456  0.445913 -1.302521 -0.401386  0.574714  1.336663   \n",
       "540  0.535276  0.694456  0.445913 -1.302521 -0.401386  0.574714  1.336663   \n",
       "541  0.535276  0.694456  0.445913 -1.302521 -0.401386  0.574714  1.336663   \n",
       "542 -0.916166  0.694456  0.445913  1.197370 -0.401386 -1.212335  1.336663   \n",
       "543 -0.916166  0.694456  0.445913  1.197370 -0.401386 -1.212335  1.336663   \n",
       "544 -0.916166  0.694456  0.445913  1.197370 -0.401386 -1.212335  1.336663   \n",
       "545 -0.916166  0.694456  0.445913  1.197370 -0.401386 -1.212335  1.336663   \n",
       "546  0.311977 -1.439975 -0.209626  1.048122 -0.401386  0.447067 -0.748132   \n",
       "\n",
       "            7         8         9    ...           12        13        14  \\\n",
       "0   -0.792141 -0.705669  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "1    2.044514 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "2    2.044514 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "3    2.044514 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "4    2.044514 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "5    0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "6    0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "7   -0.362345  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "8   -0.362345  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "9   -0.018508 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n",
       "10  -0.018508 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n",
       "11  -0.018508 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n",
       "12   1.528759  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "13   1.528759  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "14   1.528759  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "15   1.528759  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "16   0.755125  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "17   0.755125  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "18   0.755125  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "19   1.098962 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "20   0.497248 -0.705669 -0.995227    ...    -0.287926 -0.069338  1.014459   \n",
       "21  -0.878101 -0.705669 -0.995227    ...     3.473111 -0.069338  1.014459   \n",
       "22   2.216433 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n",
       "23   2.216433 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n",
       "24   2.216433 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n",
       "25   0.325329  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "26   0.325329  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "27   0.325329  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "28   0.325329  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "29   0.153411 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "..        ...       ...       ...    ...          ...       ...       ...   \n",
       "517  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "518  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "519  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "520  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "521  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "522  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "523  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "524  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "525  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "526 -0.878101  0.420169 -0.995227    ...    -0.287926 -0.069338  1.014459   \n",
       "527 -0.878101  0.420169 -0.995227    ...    -0.287926 -0.069338  1.014459   \n",
       "528 -0.878101  0.420169 -0.995227    ...    -0.287926 -0.069338  1.014459   \n",
       "529  0.669166 -0.705669 -0.995227    ...     3.473111 -0.069338  1.014459   \n",
       "530  0.841085  2.671846 -0.995227    ...    -0.287926 -0.069338  1.014459   \n",
       "531  0.841085  2.671846 -0.995227    ...    -0.287926 -0.069338  1.014459   \n",
       "532  0.841085  2.671846 -0.995227    ...    -0.287926 -0.069338  1.014459   \n",
       "533  0.841085  2.671846 -0.995227    ...    -0.287926 -0.069338  1.014459   \n",
       "534 -0.706182  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "535 -0.706182  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "536 -0.706182  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "537 -0.706182  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "538 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "539 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "540 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "541 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "542  0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "543  0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "544  0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "545  0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n",
       "546 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n",
       "\n",
       "           15        16        17        18        19        20        21  \n",
       "0    0.995227 -0.278064 -0.912871  1.054093 -0.235702  0.930630 -0.819756  \n",
       "1   -1.004796  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "2   -1.004796  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "3   -1.004796  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "4   -1.004796  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "5    0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n",
       "6    0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n",
       "7   -1.004796 -0.278064  1.095445 -0.948683  4.242641 -1.074541 -0.819756  \n",
       "8   -1.004796 -0.278064  1.095445 -0.948683  4.242641 -1.074541 -0.819756  \n",
       "9    0.995227  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "10   0.995227  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "11   0.995227  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "12  -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "13  -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "14  -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "15  -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "16  -1.004796 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n",
       "17  -1.004796 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n",
       "18  -1.004796 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n",
       "19  -1.004796 -0.278064 -0.912871  1.054093  4.242641 -1.074541 -0.819756  \n",
       "20  -1.004796 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n",
       "21  -1.004796 -0.278064 -0.912871  1.054093 -0.235702  0.930630 -0.819756  \n",
       "22   0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n",
       "23   0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n",
       "24   0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n",
       "25   0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n",
       "26   0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n",
       "27   0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n",
       "28   0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n",
       "29  -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "517 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "518 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "519 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "520 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "521 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "522 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "523 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "524 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "525 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "526 -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n",
       "527 -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n",
       "528 -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n",
       "529 -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n",
       "530 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "531 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "532 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "533 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "534  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "535  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "536  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "537  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "538  0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n",
       "539  0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n",
       "540  0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n",
       "541  0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n",
       "542  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "543  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "544  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "545  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n",
       "546 -1.004796 -0.278064 -0.912871  1.054093 -0.235702  0.930630 -0.819756  \n",
       "\n",
       "[547 rows x 22 columns]>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn import grid_search\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "\n",
    "def checkmetrics(pred, labels_test, name):\n",
    "    sns.set()\n",
    "    print('The accuracy of ', name, 'is: ', accuracy_score(pred, labels_test))\n",
    "    matrix = confusion_matrix(labels_test, pred)\n",
    "    ax = sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    print(ax)\n",
    "    print(classification_report(pred, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Appoarch - Default Params - 5 fold X-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(<bound method NDFrame.as_matrix of             0         1         2         3         4         5         6  \\\n0   -0.804517  0.694456 -1.083678 -0.798812 -0.401386 -0.233713 -0.748132   \n1    0.535276  0.694456  2.084760  1.477208 -0.401386 -0.361359  1.336663   \n2    0.535276  0.694456  2.084760  1.477208 -0.401386 -0.361359  1.336663   \n3    0.535276  0.694456  2.084760  1.477208 -0.401386 -0.361359  1.336663   \n4    0.535276  0.694456  2.084760  1.477208 -0.401386 -0.361359  1.336663   \n5    0.981874 -1.439975 -1.302191  0.413822 -0.401386  0.872555  1.336663   \n6    0.981874 -1.439975 -1.302191  0.413822 -0.401386  0.872555  1.336663   \n7    0.311977  0.694456  0.992195  0.544413 -0.401386 -1.552725  1.336663   \n8    0.311977  0.694456  0.992195  0.544413 -0.401386 -1.552725  1.336663   \n9   -0.692867  0.694456 -1.192934 -0.332414 -0.401386  0.830006 -0.748132   \n10  -0.692867  0.694456 -1.192934 -0.332414 -0.401386  0.830006 -0.748132   \n11  -0.692867  0.694456 -1.192934 -0.332414 -0.401386  0.830006 -0.748132   \n12   1.205173  0.694456  0.172772  0.133983 -0.401386 -0.914493 -0.748132   \n13   1.205173  0.694456  0.172772  0.133983 -0.401386 -0.914493 -0.748132   \n14   1.205173  0.694456  0.172772  0.133983 -0.401386 -0.914493 -0.748132   \n15   1.205173  0.694456  0.172772  0.133983 -0.401386 -0.914493 -0.748132   \n16   0.758575  0.694456  0.445913 -0.742844 -0.401386 -0.446457  1.336663   \n17   0.758575  0.694456  0.445913 -0.742844 -0.401386 -0.446457  1.336663   \n18   0.758575  0.694456  0.445913 -0.742844 -0.401386 -0.446457  1.336663   \n19   1.316822  0.694456  1.538478 -0.351070 -0.401386 -0.446457 -0.748132   \n20  -0.022971  0.694456  0.992195 -0.276447 -0.401386  0.702360 -0.748132   \n21  -2.255959  0.694456 -0.755908 -1.209242 -0.401386  1.085299 -0.748132   \n22  -0.916166  0.694456  0.992195 -0.295102 -0.401386 -0.063518 -0.748132   \n23  -0.916166  0.694456  0.992195 -0.295102 -0.401386 -0.063518 -0.748132   \n24  -0.916166  0.694456  0.992195 -0.295102 -0.401386 -0.063518 -0.748132   \n25   0.646926  0.694456 -0.810537 -0.313758  2.491364  0.489616  1.336663   \n26   0.646926  0.694456 -0.810537 -0.313758  2.491364  0.489616  1.336663   \n27   0.646926  0.694456 -0.810537 -0.313758  2.491364  0.489616  1.336663   \n28   0.646926  0.694456 -0.810537 -0.313758  2.491364  0.489616  1.336663   \n29   0.870224 -1.439975  0.445913  2.745810 -0.401386  0.361970 -0.748132   \n..        ...       ...       ...       ...       ...       ...       ...   \n517  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n518  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n519  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n520  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n521  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n522  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n523  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n524  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n525  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n526  0.311977 -1.439975 -0.100370 -0.201823 -0.401386  1.085299 -0.748132   \n527  0.311977 -1.439975 -0.100370 -0.201823 -0.401386  1.085299 -0.748132   \n528  0.311977 -1.439975 -0.100370 -0.201823 -0.401386  1.085299 -0.748132   \n529  1.093523  0.694456 -1.192934 -0.668220 -0.401386 -0.191164  1.336663   \n530  1.651770  0.694456  0.445913  0.133983 -0.401386 -0.106067 -0.748132   \n531  1.651770  0.694456  0.445913  0.133983 -0.401386 -0.106067 -0.748132   \n532  1.651770  0.694456  0.445913  0.133983 -0.401386 -0.106067 -0.748132   \n533  1.651770  0.694456  0.445913  0.133983 -0.401386 -0.106067 -0.748132   \n534  1.428471  0.694456 -0.373511  0.133983  2.491364  0.617263 -0.748132   \n535  1.428471  0.694456 -0.373511  0.133983  2.491364  0.617263 -0.748132   \n536  1.428471  0.694456 -0.373511  0.133983  2.491364  0.617263 -0.748132   \n537  1.428471  0.694456 -0.373511  0.133983  2.491364  0.617263 -0.748132   \n538  0.535276  0.694456  0.445913 -1.302521 -0.401386  0.574714  1.336663   \n539  0.535276  0.694456  0.445913 -1.302521 -0.401386  0.574714  1.336663   \n540  0.535276  0.694456  0.445913 -1.302521 -0.401386  0.574714  1.336663   \n541  0.535276  0.694456  0.445913 -1.302521 -0.401386  0.574714  1.336663   \n542 -0.916166  0.694456  0.445913  1.197370 -0.401386 -1.212335  1.336663   \n543 -0.916166  0.694456  0.445913  1.197370 -0.401386 -1.212335  1.336663   \n544 -0.916166  0.694456  0.445913  1.197370 -0.401386 -1.212335  1.336663   \n545 -0.916166  0.694456  0.445913  1.197370 -0.401386 -1.212335  1.336663   \n546  0.311977 -1.439975 -0.209626  1.048122 -0.401386  0.447067 -0.748132   \n\n            7         8         9    ...           12        13        14  \\\n0   -0.792141 -0.705669  1.004796    ...    -0.287926 -0.069338 -0.985747   \n1    2.044514 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n2    2.044514 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n3    2.044514 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n4    2.044514 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n5    0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n6    0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n7   -0.362345  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n8   -0.362345  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n9   -0.018508 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n10  -0.018508 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n11  -0.018508 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n12   1.528759  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n13   1.528759  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n14   1.528759  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n15   1.528759  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n16   0.755125  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n17   0.755125  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n18   0.755125  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n19   1.098962 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n20   0.497248 -0.705669 -0.995227    ...    -0.287926 -0.069338  1.014459   \n21  -0.878101 -0.705669 -0.995227    ...     3.473111 -0.069338  1.014459   \n22   2.216433 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n23   2.216433 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n24   2.216433 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n25   0.325329  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n26   0.325329  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n27   0.325329  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n28   0.325329  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n29   0.153411 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n..        ...       ...       ...    ...          ...       ...       ...   \n517  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n518  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n519  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n520  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n521  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n522  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n523  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n524  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n525  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n526 -0.878101  0.420169 -0.995227    ...    -0.287926 -0.069338  1.014459   \n527 -0.878101  0.420169 -0.995227    ...    -0.287926 -0.069338  1.014459   \n528 -0.878101  0.420169 -0.995227    ...    -0.287926 -0.069338  1.014459   \n529  0.669166 -0.705669 -0.995227    ...     3.473111 -0.069338  1.014459   \n530  0.841085  2.671846 -0.995227    ...    -0.287926 -0.069338  1.014459   \n531  0.841085  2.671846 -0.995227    ...    -0.287926 -0.069338  1.014459   \n532  0.841085  2.671846 -0.995227    ...    -0.287926 -0.069338  1.014459   \n533  0.841085  2.671846 -0.995227    ...    -0.287926 -0.069338  1.014459   \n534 -0.706182  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n535 -0.706182  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n536 -0.706182  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n537 -0.706182  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n538 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338 -0.985747   \n539 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338 -0.985747   \n540 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338 -0.985747   \n541 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338 -0.985747   \n542  0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n543  0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n544  0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n545  0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n546 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n\n           15        16        17        18        19        20        21  \n0    0.995227 -0.278064 -0.912871  1.054093 -0.235702  0.930630 -0.819756  \n1   -1.004796  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n2   -1.004796  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n3   -1.004796  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n4   -1.004796  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n5    0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n6    0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n7   -1.004796 -0.278064  1.095445 -0.948683  4.242641 -1.074541 -0.819756  \n8   -1.004796 -0.278064  1.095445 -0.948683  4.242641 -1.074541 -0.819756  \n9    0.995227  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n10   0.995227  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n11   0.995227  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n12  -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n13  -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n14  -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n15  -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n16  -1.004796 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n17  -1.004796 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n18  -1.004796 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n19  -1.004796 -0.278064 -0.912871  1.054093  4.242641 -1.074541 -0.819756  \n20  -1.004796 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n21  -1.004796 -0.278064 -0.912871  1.054093 -0.235702  0.930630 -0.819756  \n22   0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n23   0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n24   0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n25   0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n26   0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n27   0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n28   0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n29  -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n..        ...       ...       ...       ...       ...       ...       ...  \n517 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n518 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n519 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n520 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n521 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n522 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n523 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n524 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n525 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n526 -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n527 -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n528 -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n529 -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n530 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n531 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n532 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n533 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n534  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n535  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n536  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n537  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n538  0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n539  0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n540  0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n541  0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n542  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n543  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n544  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n545  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n546 -1.004796 -0.278064 -0.912871  1.054093 -0.235702  0.930630 -0.819756  \n\n[547 rows x 22 columns]>, dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-1fa4119f4169>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mgrid_search_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSVM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mgrid_search_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mresultsdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_search_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The train score:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_search_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"with parameters:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_search_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[0mrefit_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'score'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[1;31m# Regenerate parameter iterable for each fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \"\"\"\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \"\"\"\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[1;32m--> 119\u001b[1;33m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array array(<bound method NDFrame.as_matrix of             0         1         2         3         4         5         6  \\\n0   -0.804517  0.694456 -1.083678 -0.798812 -0.401386 -0.233713 -0.748132   \n1    0.535276  0.694456  2.084760  1.477208 -0.401386 -0.361359  1.336663   \n2    0.535276  0.694456  2.084760  1.477208 -0.401386 -0.361359  1.336663   \n3    0.535276  0.694456  2.084760  1.477208 -0.401386 -0.361359  1.336663   \n4    0.535276  0.694456  2.084760  1.477208 -0.401386 -0.361359  1.336663   \n5    0.981874 -1.439975 -1.302191  0.413822 -0.401386  0.872555  1.336663   \n6    0.981874 -1.439975 -1.302191  0.413822 -0.401386  0.872555  1.336663   \n7    0.311977  0.694456  0.992195  0.544413 -0.401386 -1.552725  1.336663   \n8    0.311977  0.694456  0.992195  0.544413 -0.401386 -1.552725  1.336663   \n9   -0.692867  0.694456 -1.192934 -0.332414 -0.401386  0.830006 -0.748132   \n10  -0.692867  0.694456 -1.192934 -0.332414 -0.401386  0.830006 -0.748132   \n11  -0.692867  0.694456 -1.192934 -0.332414 -0.401386  0.830006 -0.748132   \n12   1.205173  0.694456  0.172772  0.133983 -0.401386 -0.914493 -0.748132   \n13   1.205173  0.694456  0.172772  0.133983 -0.401386 -0.914493 -0.748132   \n14   1.205173  0.694456  0.172772  0.133983 -0.401386 -0.914493 -0.748132   \n15   1.205173  0.694456  0.172772  0.133983 -0.401386 -0.914493 -0.748132   \n16   0.758575  0.694456  0.445913 -0.742844 -0.401386 -0.446457  1.336663   \n17   0.758575  0.694456  0.445913 -0.742844 -0.401386 -0.446457  1.336663   \n18   0.758575  0.694456  0.445913 -0.742844 -0.401386 -0.446457  1.336663   \n19   1.316822  0.694456  1.538478 -0.351070 -0.401386 -0.446457 -0.748132   \n20  -0.022971  0.694456  0.992195 -0.276447 -0.401386  0.702360 -0.748132   \n21  -2.255959  0.694456 -0.755908 -1.209242 -0.401386  1.085299 -0.748132   \n22  -0.916166  0.694456  0.992195 -0.295102 -0.401386 -0.063518 -0.748132   \n23  -0.916166  0.694456  0.992195 -0.295102 -0.401386 -0.063518 -0.748132   \n24  -0.916166  0.694456  0.992195 -0.295102 -0.401386 -0.063518 -0.748132   \n25   0.646926  0.694456 -0.810537 -0.313758  2.491364  0.489616  1.336663   \n26   0.646926  0.694456 -0.810537 -0.313758  2.491364  0.489616  1.336663   \n27   0.646926  0.694456 -0.810537 -0.313758  2.491364  0.489616  1.336663   \n28   0.646926  0.694456 -0.810537 -0.313758  2.491364  0.489616  1.336663   \n29   0.870224 -1.439975  0.445913  2.745810 -0.401386  0.361970 -0.748132   \n..        ...       ...       ...       ...       ...       ...       ...   \n517  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n518  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n519  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n520  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n521  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n522  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n523  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n524  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n525  0.981874 -1.439975  0.992195  2.988336 -0.401386  0.234324 -0.748132   \n526  0.311977 -1.439975 -0.100370 -0.201823 -0.401386  1.085299 -0.748132   \n527  0.311977 -1.439975 -0.100370 -0.201823 -0.401386  1.085299 -0.748132   \n528  0.311977 -1.439975 -0.100370 -0.201823 -0.401386  1.085299 -0.748132   \n529  1.093523  0.694456 -1.192934 -0.668220 -0.401386 -0.191164  1.336663   \n530  1.651770  0.694456  0.445913  0.133983 -0.401386 -0.106067 -0.748132   \n531  1.651770  0.694456  0.445913  0.133983 -0.401386 -0.106067 -0.748132   \n532  1.651770  0.694456  0.445913  0.133983 -0.401386 -0.106067 -0.748132   \n533  1.651770  0.694456  0.445913  0.133983 -0.401386 -0.106067 -0.748132   \n534  1.428471  0.694456 -0.373511  0.133983  2.491364  0.617263 -0.748132   \n535  1.428471  0.694456 -0.373511  0.133983  2.491364  0.617263 -0.748132   \n536  1.428471  0.694456 -0.373511  0.133983  2.491364  0.617263 -0.748132   \n537  1.428471  0.694456 -0.373511  0.133983  2.491364  0.617263 -0.748132   \n538  0.535276  0.694456  0.445913 -1.302521 -0.401386  0.574714  1.336663   \n539  0.535276  0.694456  0.445913 -1.302521 -0.401386  0.574714  1.336663   \n540  0.535276  0.694456  0.445913 -1.302521 -0.401386  0.574714  1.336663   \n541  0.535276  0.694456  0.445913 -1.302521 -0.401386  0.574714  1.336663   \n542 -0.916166  0.694456  0.445913  1.197370 -0.401386 -1.212335  1.336663   \n543 -0.916166  0.694456  0.445913  1.197370 -0.401386 -1.212335  1.336663   \n544 -0.916166  0.694456  0.445913  1.197370 -0.401386 -1.212335  1.336663   \n545 -0.916166  0.694456  0.445913  1.197370 -0.401386 -1.212335  1.336663   \n546  0.311977 -1.439975 -0.209626  1.048122 -0.401386  0.447067 -0.748132   \n\n            7         8         9    ...           12        13        14  \\\n0   -0.792141 -0.705669  1.004796    ...    -0.287926 -0.069338 -0.985747   \n1    2.044514 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n2    2.044514 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n3    2.044514 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n4    2.044514 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n5    0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n6    0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n7   -0.362345  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n8   -0.362345  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n9   -0.018508 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n10  -0.018508 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n11  -0.018508 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n12   1.528759  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n13   1.528759  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n14   1.528759  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n15   1.528759  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n16   0.755125  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n17   0.755125  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n18   0.755125  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n19   1.098962 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n20   0.497248 -0.705669 -0.995227    ...    -0.287926 -0.069338  1.014459   \n21  -0.878101 -0.705669 -0.995227    ...     3.473111 -0.069338  1.014459   \n22   2.216433 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n23   2.216433 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n24   2.216433 -0.705669 -0.995227    ...    -0.287926 -0.069338 -0.985747   \n25   0.325329  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n26   0.325329  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n27   0.325329  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n28   0.325329  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n29   0.153411 -0.705669  1.004796    ...    -0.287926 -0.069338  1.014459   \n..        ...       ...       ...    ...          ...       ...       ...   \n517  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n518  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n519  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n520  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n521  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n522  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n523  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n524  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n525  2.560270  2.671846  1.004796    ...    -0.287926 -0.069338  1.014459   \n526 -0.878101  0.420169 -0.995227    ...    -0.287926 -0.069338  1.014459   \n527 -0.878101  0.420169 -0.995227    ...    -0.287926 -0.069338  1.014459   \n528 -0.878101  0.420169 -0.995227    ...    -0.287926 -0.069338  1.014459   \n529  0.669166 -0.705669 -0.995227    ...     3.473111 -0.069338  1.014459   \n530  0.841085  2.671846 -0.995227    ...    -0.287926 -0.069338  1.014459   \n531  0.841085  2.671846 -0.995227    ...    -0.287926 -0.069338  1.014459   \n532  0.841085  2.671846 -0.995227    ...    -0.287926 -0.069338  1.014459   \n533  0.841085  2.671846 -0.995227    ...    -0.287926 -0.069338  1.014459   \n534 -0.706182  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n535 -0.706182  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n536 -0.706182  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n537 -0.706182  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n538 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338 -0.985747   \n539 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338 -0.985747   \n540 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338 -0.985747   \n541 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338 -0.985747   \n542  0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n543  0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n544  0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n545  0.669166  1.546008  1.004796    ...    -0.287926 -0.069338 -0.985747   \n546 -0.878101  0.420169  1.004796    ...    -0.287926 -0.069338  1.014459   \n\n           15        16        17        18        19        20        21  \n0    0.995227 -0.278064 -0.912871  1.054093 -0.235702  0.930630 -0.819756  \n1   -1.004796  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n2   -1.004796  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n3   -1.004796  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n4   -1.004796  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n5    0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n6    0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n7   -1.004796 -0.278064  1.095445 -0.948683  4.242641 -1.074541 -0.819756  \n8   -1.004796 -0.278064  1.095445 -0.948683  4.242641 -1.074541 -0.819756  \n9    0.995227  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n10   0.995227  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n11   0.995227  3.596294 -0.912871 -0.948683 -0.235702 -1.074541  1.219875  \n12  -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n13  -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n14  -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n15  -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n16  -1.004796 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n17  -1.004796 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n18  -1.004796 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n19  -1.004796 -0.278064 -0.912871  1.054093  4.242641 -1.074541 -0.819756  \n20  -1.004796 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n21  -1.004796 -0.278064 -0.912871  1.054093 -0.235702  0.930630 -0.819756  \n22   0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n23   0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n24   0.995227 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n25   0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n26   0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n27   0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n28   0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n29  -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n..        ...       ...       ...       ...       ...       ...       ...  \n517 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n518 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n519 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n520 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n521 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n522 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n523 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n524 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n525 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n526 -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n527 -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n528 -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n529 -1.004796 -0.278064  1.095445 -0.948683 -0.235702  0.930630 -0.819756  \n530 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n531 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n532 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n533 -1.004796 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n534  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n535  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n536  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n537  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n538  0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n539  0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n540  0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n541  0.995227 -0.278064 -0.912871  1.054093 -0.235702 -1.074541  1.219875  \n542  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n543  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n544  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n545  0.995227 -0.278064  1.095445 -0.948683 -0.235702 -1.074541  1.219875  \n546 -1.004796 -0.278064 -0.912871  1.054093 -0.235702  0.930630 -0.819756  \n\n[547 rows x 22 columns]>, dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "#svm.SVC  C-Support Vector Classification.\n",
    "parameters ={\n",
    "#    'C': [0.1,1], \n",
    "#    'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5],\n",
    "#    \"class_weight\": ['balanced', None]\n",
    "}\n",
    "SVM = svm.SVC()\n",
    "\n",
    "#param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\n",
    "#grid_search_cv = RandomizedSearchCV(SVM, param_distributions, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train, labels_train)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train, labels_train)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'C-Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearSVC Linear Support Vector Classification\n",
    "\n",
    "parameters = {\n",
    "#   'C': [0.1,0.2,0.5,1,2,3,4,5],\n",
    "#   \"class_weight\": ['balanced', None]\n",
    "}\n",
    "SVM = LinearSVC()\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train, labels_train)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train, labels_train)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'Linear Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.svm.NuSVC Nu-Support Vector Classification\n",
    "# defaults: nu=0.5, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)\n",
    "parameters = {\n",
    "    'nu': [0.1] #other larger nu values is \"not fleasible\"\n",
    "#   'C': [0.1,0.2,0.5,1,2,3,4,5],\n",
    "#    \"class_weight\": ['balanced', None]\n",
    "}\n",
    "SVM = NuSVC()\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train, labels_train)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train, labels_train)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'Nu-Support Vector Classification')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
