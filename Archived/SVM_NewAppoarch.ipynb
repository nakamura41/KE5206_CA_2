{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\ggplot\\utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\ggplot\\stats\\smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import SMOTE #for SMOTE -> install package using: conda install -c conda-forge imbalanced-learn\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import ggplot\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC\n",
    "#from sklearn.svm import SVR #just Testing for regression on other continous data of dataset\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "\n",
    "import pylab as pl\n",
    "from itertools import cycle\n",
    "from sklearn import cross_validation\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "features_list = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','class']\n",
    "dataset1=pd.read_csv(\"data\\Heart_Disease_Data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# SVM requires that each data instance is represented as a vector of real numbers\n",
    "# If you already have numeric dtypes (int8|16|32|64,float64,boolean) you can convert it to another \"numeric\" dtype using Pandas .astype() method. Demo: In [90]: df = pd.DataFrame(np.random.randint(10**5,10**7,(5,3)),columns=list('abc'), dtype=np.int64) In [91]: df Out[91]: a b c 0 9059440 9590567 2076918 1 5861102 4566089 1947323 2 6636568 162770 2487991 3 6794572 5236903 5628779 4 470121 4044395 4546794 In [92]: df.dtypes Out[92]: a int64 b int64 c int64 dtype: object In [93]: df['a'] = df['a'].astype(float) In [94]: df.dtypes Out[94]: a float64 b int64 c int64 dtype: object It won't work for object (string) dtypes, that can't be converted to numbers: In [95]: df.loc[1, 'b'] = 'XXXXXX' In [96]: df Out[96]:...\n",
    "# Just make everything numeric for ease, later we will convert to ordinal/one-hot encoding.\n",
    "dataset1 = dataset1.convert_objects(convert_numeric=True)\n",
    "dataset1 = dataset1.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slop</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>pred_attribute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0    63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
       "1    67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
       "2    67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
       "3    37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
       "4    41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
       "5    56.0  1.0  2.0     120.0  236.0  0.0      0.0    178.0    0.0      0.8   \n",
       "6    62.0  0.0  4.0     140.0  268.0  0.0      2.0    160.0    0.0      3.6   \n",
       "7    57.0  0.0  4.0     120.0  354.0  0.0      0.0    163.0    1.0      0.6   \n",
       "8    63.0  1.0  4.0     130.0  254.0  0.0      2.0    147.0    0.0      1.4   \n",
       "9    53.0  1.0  4.0     140.0  203.0  1.0      2.0    155.0    1.0      3.1   \n",
       "10   57.0  1.0  4.0     140.0  192.0  0.0      0.0    148.0    0.0      0.4   \n",
       "11   56.0  0.0  2.0     140.0  294.0  0.0      2.0    153.0    0.0      1.3   \n",
       "12   56.0  1.0  3.0     130.0  256.0  1.0      2.0    142.0    1.0      0.6   \n",
       "13   44.0  1.0  2.0     120.0  263.0  0.0      0.0    173.0    0.0      0.0   \n",
       "14   52.0  1.0  3.0     172.0  199.0  1.0      0.0    162.0    0.0      0.5   \n",
       "15   57.0  1.0  3.0     150.0  168.0  0.0      0.0    174.0    0.0      1.6   \n",
       "16   48.0  1.0  2.0     110.0  229.0  0.0      0.0    168.0    0.0      1.0   \n",
       "17   54.0  1.0  4.0     140.0  239.0  0.0      0.0    160.0    0.0      1.2   \n",
       "18   48.0  0.0  3.0     130.0  275.0  0.0      0.0    139.0    0.0      0.2   \n",
       "19   49.0  1.0  2.0     130.0  266.0  0.0      0.0    171.0    0.0      0.6   \n",
       "20   64.0  1.0  1.0     110.0  211.0  0.0      2.0    144.0    1.0      1.8   \n",
       "21   58.0  0.0  1.0     150.0  283.0  1.0      2.0    162.0    0.0      1.0   \n",
       "22   58.0  1.0  2.0     120.0  284.0  0.0      2.0    160.0    0.0      1.8   \n",
       "23   58.0  1.0  3.0     132.0  224.0  0.0      2.0    173.0    0.0      3.2   \n",
       "24   60.0  1.0  4.0     130.0  206.0  0.0      2.0    132.0    1.0      2.4   \n",
       "25   50.0  0.0  3.0     120.0  219.0  0.0      0.0    158.0    0.0      1.6   \n",
       "26   58.0  0.0  3.0     120.0  340.0  0.0      0.0    172.0    0.0      0.0   \n",
       "27   66.0  0.0  1.0     150.0  226.0  0.0      0.0    114.0    0.0      2.6   \n",
       "28   43.0  1.0  4.0     150.0  247.0  0.0      0.0    171.0    0.0      1.5   \n",
       "29   40.0  1.0  4.0     110.0  167.0  0.0      2.0    114.0    1.0      2.0   \n",
       "..    ...  ...  ...       ...    ...  ...      ...      ...    ...      ...   \n",
       "273  71.0  0.0  4.0     112.0  149.0  0.0      0.0    125.0    0.0      1.6   \n",
       "274  59.0  1.0  1.0     134.0  204.0  0.0      0.0    162.0    0.0      0.8   \n",
       "275  64.0  1.0  1.0     170.0  227.0  0.0      2.0    155.0    0.0      0.6   \n",
       "276  66.0  0.0  3.0     146.0  278.0  0.0      2.0    152.0    0.0      0.0   \n",
       "277  39.0  0.0  3.0     138.0  220.0  0.0      0.0    152.0    0.0      0.0   \n",
       "278  57.0  1.0  2.0     154.0  232.0  0.0      2.0    164.0    0.0      0.0   \n",
       "279  58.0  0.0  4.0     130.0  197.0  0.0      0.0    131.0    0.0      0.6   \n",
       "280  57.0  1.0  4.0     110.0  335.0  0.0      0.0    143.0    1.0      3.0   \n",
       "281  47.0  1.0  3.0     130.0  253.0  0.0      0.0    179.0    0.0      0.0   \n",
       "282  55.0  0.0  4.0     128.0  205.0  0.0      1.0    130.0    1.0      2.0   \n",
       "283  35.0  1.0  2.0     122.0  192.0  0.0      0.0    174.0    0.0      0.0   \n",
       "284  61.0  1.0  4.0     148.0  203.0  0.0      0.0    161.0    0.0      0.0   \n",
       "285  58.0  1.0  4.0     114.0  318.0  0.0      1.0    140.0    0.0      4.4   \n",
       "286  58.0  0.0  4.0     170.0  225.0  1.0      2.0    146.0    1.0      2.8   \n",
       "287  58.0  1.0  2.0     125.0  220.0  0.0      0.0    144.0    0.0      0.4   \n",
       "288  56.0  1.0  2.0     130.0  221.0  0.0      2.0    163.0    0.0      0.0   \n",
       "289  56.0  1.0  2.0     120.0  240.0  0.0      0.0    169.0    0.0      0.0   \n",
       "290  67.0  1.0  3.0     152.0  212.0  0.0      2.0    150.0    0.0      0.8   \n",
       "291  55.0  0.0  2.0     132.0  342.0  0.0      0.0    166.0    0.0      1.2   \n",
       "292  44.0  1.0  4.0     120.0  169.0  0.0      0.0    144.0    1.0      2.8   \n",
       "293  63.0  1.0  4.0     140.0  187.0  0.0      2.0    144.0    1.0      4.0   \n",
       "294  63.0  0.0  4.0     124.0  197.0  0.0      0.0    136.0    1.0      0.0   \n",
       "295  41.0  1.0  2.0     120.0  157.0  0.0      0.0    182.0    0.0      0.0   \n",
       "296  59.0  1.0  4.0     164.0  176.0  1.0      2.0     90.0    0.0      1.0   \n",
       "297  57.0  0.0  4.0     140.0  241.0  0.0      0.0    123.0    1.0      0.2   \n",
       "298  45.0  1.0  1.0     110.0  264.0  0.0      0.0    132.0    0.0      1.2   \n",
       "299  68.0  1.0  4.0     144.0  193.0  1.0      0.0    141.0    0.0      3.4   \n",
       "300  57.0  1.0  4.0     130.0  131.0  0.0      0.0    115.0    1.0      1.2   \n",
       "301  57.0  0.0  2.0     130.0  236.0  0.0      2.0    174.0    0.0      0.0   \n",
       "302  38.0  1.0  3.0     138.0  175.0  0.0      0.0    173.0    0.0      0.0   \n",
       "\n",
       "     slop   ca  thal  pred_attribute  \n",
       "0     3.0  0.0   6.0             0.0  \n",
       "1     2.0  3.0   3.0             2.0  \n",
       "2     2.0  2.0   7.0             1.0  \n",
       "3     3.0  0.0   3.0             0.0  \n",
       "4     1.0  0.0   3.0             0.0  \n",
       "5     1.0  0.0   3.0             0.0  \n",
       "6     3.0  2.0   3.0             3.0  \n",
       "7     1.0  0.0   3.0             0.0  \n",
       "8     2.0  1.0   7.0             2.0  \n",
       "9     3.0  0.0   7.0             1.0  \n",
       "10    2.0  0.0   6.0             0.0  \n",
       "11    2.0  0.0   3.0             0.0  \n",
       "12    2.0  1.0   6.0             2.0  \n",
       "13    1.0  0.0   7.0             0.0  \n",
       "14    1.0  0.0   7.0             0.0  \n",
       "15    1.0  0.0   3.0             0.0  \n",
       "16    3.0  0.0   7.0             1.0  \n",
       "17    1.0  0.0   3.0             0.0  \n",
       "18    1.0  0.0   3.0             0.0  \n",
       "19    1.0  0.0   3.0             0.0  \n",
       "20    2.0  0.0   3.0             0.0  \n",
       "21    1.0  0.0   3.0             0.0  \n",
       "22    2.0  0.0   3.0             1.0  \n",
       "23    1.0  2.0   7.0             3.0  \n",
       "24    2.0  2.0   7.0             4.0  \n",
       "25    2.0  0.0   3.0             0.0  \n",
       "26    1.0  0.0   3.0             0.0  \n",
       "27    3.0  0.0   3.0             0.0  \n",
       "28    1.0  0.0   3.0             0.0  \n",
       "29    2.0  0.0   7.0             3.0  \n",
       "..    ...  ...   ...             ...  \n",
       "273   2.0  0.0   3.0             0.0  \n",
       "274   1.0  2.0   3.0             1.0  \n",
       "275   2.0  0.0   7.0             0.0  \n",
       "276   2.0  1.0   3.0             0.0  \n",
       "277   2.0  0.0   3.0             0.0  \n",
       "278   1.0  1.0   3.0             1.0  \n",
       "279   2.0  0.0   3.0             0.0  \n",
       "280   2.0  1.0   7.0             2.0  \n",
       "281   1.0  0.0   3.0             0.0  \n",
       "282   2.0  1.0   7.0             3.0  \n",
       "283   1.0  0.0   3.0             0.0  \n",
       "284   1.0  1.0   7.0             2.0  \n",
       "285   3.0  3.0   6.0             4.0  \n",
       "286   2.0  2.0   6.0             2.0  \n",
       "287   2.0  NaN   7.0             0.0  \n",
       "288   1.0  0.0   7.0             0.0  \n",
       "289   3.0  0.0   3.0             0.0  \n",
       "290   2.0  0.0   7.0             1.0  \n",
       "291   1.0  0.0   3.0             0.0  \n",
       "292   3.0  0.0   6.0             2.0  \n",
       "293   1.0  2.0   7.0             2.0  \n",
       "294   2.0  0.0   3.0             1.0  \n",
       "295   1.0  0.0   3.0             0.0  \n",
       "296   2.0  2.0   6.0             3.0  \n",
       "297   2.0  0.0   7.0             1.0  \n",
       "298   2.0  0.0   7.0             1.0  \n",
       "299   2.0  2.0   7.0             2.0  \n",
       "300   2.0  1.0   7.0             3.0  \n",
       "301   2.0  1.0   3.0             1.0  \n",
       "302   1.0  NaN   3.0             0.0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count missing value in terms of colunms #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age               False\n",
      "sex               False\n",
      "cp                False\n",
      "trestbps          False\n",
      "chol              False\n",
      "fbs               False\n",
      "restecg           False\n",
      "thalach           False\n",
      "exang             False\n",
      "oldpeak           False\n",
      "slop              False\n",
      "ca                 True\n",
      "thal               True\n",
      "pred_attribute    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "#dataset.shape[0] - dataset.count()\n",
    "print(dataset1.isnull().any())\n",
    "dataset1 = dataset1.replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check for duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://pdfs.semanticscholar.org/daa0/f01f96a89fcfc5f41a2da67fb2a8966900ab.pdf \n",
    "# we should pick these features:\n",
    "Genetic_Based_Decision = dataset1[['cp','trestbps', 'restecg', 'thalach', 'ca', 'thal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlier:  34.0 age\n",
      "outlier:  29.0 age\n",
      "outlier:  77.0 age\n",
      "outlier:  34.0 age\n",
      "outlier:  76.0 age\n",
      "outlier:  354.0 chol\n",
      "outlier:  340.0 chol\n",
      "outlier:  353.0 chol\n",
      "outlier:  417.0 chol\n",
      "outlier:  360.0 chol\n",
      "outlier:  141.0 chol\n",
      "outlier:  341.0 chol\n",
      "outlier:  407.0 chol\n",
      "outlier:  564.0 chol\n",
      "outlier:  394.0 chol\n",
      "outlier:  409.0 chol\n",
      "outlier:  126.0 chol\n",
      "outlier:  342.0 chol\n",
      "outlier:  131.0 chol\n",
      "outlier:  99.0 thalach\n",
      "outlier:  97.0 thalach\n",
      "outlier:  202.0 thalach\n",
      "outlier:  96.0 thalach\n",
      "outlier:  88.0 thalach\n",
      "outlier:  95.0 thalach\n",
      "outlier:  96.0 thalach\n",
      "outlier:  71.0 thalach\n",
      "outlier:  90.0 thalach\n",
      "outlier:  3.5 oldpeak\n",
      "outlier:  3.6 oldpeak\n",
      "outlier:  3.4 oldpeak\n",
      "outlier:  3.6 oldpeak\n",
      "outlier:  6.2 oldpeak\n",
      "outlier:  3.6 oldpeak\n",
      "outlier:  4.0 oldpeak\n",
      "outlier:  5.6 oldpeak\n",
      "outlier:  4.0 oldpeak\n",
      "outlier:  4.2 oldpeak\n",
      "outlier:  4.2 oldpeak\n",
      "outlier:  3.8 oldpeak\n",
      "outlier:  3.4 oldpeak\n",
      "outlier:  3.6 oldpeak\n",
      "outlier:  4.4 oldpeak\n",
      "outlier:  4.0 oldpeak\n",
      "outlier:  3.4 oldpeak\n",
      "(45, 'outliers. That is', 15.0, 'percent of the total list')\n"
     ]
    }
   ],
   "source": [
    "continuous_vars = dataset1[['age', 'restecg', 'chol', 'thalach', 'oldpeak']] \n",
    "\n",
    "def checkforoutlier(df):\n",
    "    outliersnumbers = 0\n",
    "    for column in df:\n",
    "        for number in df[column]:\n",
    "            if number < np.percentile(\n",
    "                df[column], 25)-(np.percentile(\n",
    "                df[column], 75)-np.percentile(\n",
    "                df[column], 25)) or number > np.percentile(\n",
    "                df[column], 75)+(np.percentile(\n",
    "                df[column], 75)-np.percentile(\n",
    "                df[column], 25)):\n",
    "                    print(\"outlier: \", number, column)\n",
    "                    outliersnumbers += 1\n",
    "    return outliersnumbers, 'outliers. That is', round(float(outliersnumbers)/float(len(df[column]))*100, 0), 'percent of the total list'\n",
    "\n",
    "print(checkforoutlier(continuous_vars))\n",
    "\n",
    "# Thalach seems very high, but after research a heartbeat of 202 is possible: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two variables are discrete/ordinal: ca (number of major vessels colored by fluoroscopy) and num (diagnosis of heart disease)\n",
    "# Three can be directly viewed as 1 hot (because binary): 'sex':'male', 'fbs':'fasting blood sugar', 'exang':'exercise induced angina'\n",
    "\n",
    "# which leaves 4 for one-hot encoding. problem is that the values aren't unique, so have to manually\n",
    "# make extra columns:\n",
    "\n",
    "dataset1[\"cp\"] = dataset1[\"cp\"].replace([1,2,3,4], [\"typical angina\", \"atypical angina\", \"non-angina\", \"asymptomatic angina\"])\n",
    "dataset1[\"restecg\"] = dataset1[\"restecg\"].replace([0,1,2], [\"normalresecg\", \"ST-T wave abnormality\", \"left ventricular hypertrophy\"])\n",
    "dataset1[\"slop\"] = dataset1[\"slop\"].replace([1,2,3], [\"upsloping\", \"flat\", \"downsloping\"])\n",
    "dataset1[\"thal\"] = dataset1[\"thal\"].replace([3,6,7], [\"normalthal\", \"fixed defect\", \"reversible defect\"])\n",
    "\n",
    "x = dataset1[['cp', 'restecg', 'slop', 'thal']]\n",
    "for column in ['cp', 'restecg', 'slop', 'thal']:\n",
    "    one_hot = pd.get_dummies(dataset1[column])\n",
    "    dataset1 = dataset1.drop(column, axis=1)\n",
    "    dataset1 = dataset1.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b689622a-a475-40a8-bd33-e2b5e018d528",
    "_uuid": "2e13a97aaee5c269ff8f21fd7b66135927b66156"
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing:\n",
    "dataset1.dropna(inplace=True, axis=0, how=\"any\")\n",
    "Y=dataset1[\"pred_attribute\"]\n",
    "dataset1 = dataset1.drop(\"pred_attribute\", axis=1)\n",
    "X=dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "2dacb7c2-6d4e-4a17-b5cb-d6ecd821c923",
    "_uuid": "c3355ae680b60b0daa713153f05b3709193af7f6"
   },
   "outputs": [],
   "source": [
    "# evaluate the model by splitting into train and test sets  #Edit by ryan, we aim to do 3 traditional sets in the end, this first split is 80/20\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "5065d9ba-be53-4431-bb86-152ee1673550",
    "_uuid": "00229a787842594dee105c79de5871548613a045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 131, 1.0: 42, 3.0: 32, 2.0: 26, 4.0: 8})\n",
      "Counter({0.0: 30, 1.0: 12, 2.0: 10, 4.0: 5, 3.0: 3})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "list1 = []\n",
    "for i in labels_train:\n",
    "    list1.append(i)\n",
    "counter=collections.Counter(list1)\n",
    "print(counter)\n",
    "\n",
    "list2 = []\n",
    "for i in labels_test:\n",
    "    list2.append(i)\n",
    "counter=collections.Counter(list2)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7993311036789298\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print(len(features_train)/(len(features_train)+ len(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_df = pd.DataFrame(features_train)\n",
    "features_train_df.to_csv('features_train.csv', index=False)\n",
    "\n",
    "features_test_df = pd.DataFrame(features_test)\n",
    "features_test_df.to_csv('features_test.csv', index=False)\n",
    "\n",
    "labels_train_df = pd.DataFrame(labels_train)\n",
    "labels_train_df.to_csv('labels_train.csv', index=False)\n",
    "\n",
    "labels_test_df = pd.DataFrame(labels_test)\n",
    "labels_test_df.to_csv('labels_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Scores based on XGBoost (by David)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.56559\ttest-mlogloss:1.58009\n",
      "[1]\ttrain-mlogloss:1.52178\ttest-mlogloss:1.55242\n",
      "[2]\ttrain-mlogloss:1.48283\ttest-mlogloss:1.53258\n",
      "[3]\ttrain-mlogloss:1.4436\ttest-mlogloss:1.50955\n",
      "[4]\ttrain-mlogloss:1.40645\ttest-mlogloss:1.48689\n",
      "[5]\ttrain-mlogloss:1.3696\ttest-mlogloss:1.46478\n",
      "[6]\ttrain-mlogloss:1.33668\ttest-mlogloss:1.4465\n",
      "[7]\ttrain-mlogloss:1.30398\ttest-mlogloss:1.42958\n",
      "[8]\ttrain-mlogloss:1.27417\ttest-mlogloss:1.41113\n",
      "[9]\ttrain-mlogloss:1.24508\ttest-mlogloss:1.39635\n",
      "[10]\ttrain-mlogloss:1.21776\ttest-mlogloss:1.38117\n",
      "[11]\ttrain-mlogloss:1.19122\ttest-mlogloss:1.36531\n",
      "[12]\ttrain-mlogloss:1.1649\ttest-mlogloss:1.35248\n",
      "[13]\ttrain-mlogloss:1.14094\ttest-mlogloss:1.3428\n",
      "[14]\ttrain-mlogloss:1.11766\ttest-mlogloss:1.33034\n",
      "[15]\ttrain-mlogloss:1.09389\ttest-mlogloss:1.32069\n",
      "[16]\ttrain-mlogloss:1.07048\ttest-mlogloss:1.30956\n",
      "[17]\ttrain-mlogloss:1.05001\ttest-mlogloss:1.30091\n",
      "[18]\ttrain-mlogloss:1.0295\ttest-mlogloss:1.29026\n",
      "[19]\ttrain-mlogloss:1.00962\ttest-mlogloss:1.28262\n",
      "[20]\ttrain-mlogloss:0.990484\ttest-mlogloss:1.27358\n",
      "[21]\ttrain-mlogloss:0.971149\ttest-mlogloss:1.26544\n",
      "[22]\ttrain-mlogloss:0.953481\ttest-mlogloss:1.25678\n",
      "[23]\ttrain-mlogloss:0.935497\ttest-mlogloss:1.24986\n",
      "[24]\ttrain-mlogloss:0.918648\ttest-mlogloss:1.24317\n",
      "[25]\ttrain-mlogloss:0.90213\ttest-mlogloss:1.23805\n",
      "[26]\ttrain-mlogloss:0.885533\ttest-mlogloss:1.2316\n",
      "[27]\ttrain-mlogloss:0.870491\ttest-mlogloss:1.22592\n",
      "[28]\ttrain-mlogloss:0.855923\ttest-mlogloss:1.21801\n",
      "[29]\ttrain-mlogloss:0.842444\ttest-mlogloss:1.21463\n",
      "[30]\ttrain-mlogloss:0.829412\ttest-mlogloss:1.21203\n",
      "[31]\ttrain-mlogloss:0.81613\ttest-mlogloss:1.20927\n",
      "[32]\ttrain-mlogloss:0.803078\ttest-mlogloss:1.20722\n",
      "[33]\ttrain-mlogloss:0.789958\ttest-mlogloss:1.20277\n",
      "[34]\ttrain-mlogloss:0.778328\ttest-mlogloss:1.20102\n",
      "[35]\ttrain-mlogloss:0.76604\ttest-mlogloss:1.19735\n",
      "[36]\ttrain-mlogloss:0.754662\ttest-mlogloss:1.19144\n",
      "[37]\ttrain-mlogloss:0.742833\ttest-mlogloss:1.1881\n",
      "[38]\ttrain-mlogloss:0.731974\ttest-mlogloss:1.18594\n",
      "[39]\ttrain-mlogloss:0.720413\ttest-mlogloss:1.18219\n",
      "[40]\ttrain-mlogloss:0.710244\ttest-mlogloss:1.17914\n",
      "[41]\ttrain-mlogloss:0.699368\ttest-mlogloss:1.17555\n",
      "[42]\ttrain-mlogloss:0.689502\ttest-mlogloss:1.17317\n",
      "[43]\ttrain-mlogloss:0.679082\ttest-mlogloss:1.17147\n",
      "[44]\ttrain-mlogloss:0.669106\ttest-mlogloss:1.16952\n",
      "[45]\ttrain-mlogloss:0.66005\ttest-mlogloss:1.16628\n",
      "[46]\ttrain-mlogloss:0.650964\ttest-mlogloss:1.16376\n",
      "[47]\ttrain-mlogloss:0.642051\ttest-mlogloss:1.16227\n",
      "[48]\ttrain-mlogloss:0.632879\ttest-mlogloss:1.16145\n",
      "[49]\ttrain-mlogloss:0.624328\ttest-mlogloss:1.16242\n",
      "[50]\ttrain-mlogloss:0.616987\ttest-mlogloss:1.1611\n",
      "[51]\ttrain-mlogloss:0.608507\ttest-mlogloss:1.16131\n",
      "[52]\ttrain-mlogloss:0.60098\ttest-mlogloss:1.16217\n",
      "[53]\ttrain-mlogloss:0.592635\ttest-mlogloss:1.15957\n",
      "[54]\ttrain-mlogloss:0.58489\ttest-mlogloss:1.15967\n",
      "[55]\ttrain-mlogloss:0.577324\ttest-mlogloss:1.15978\n",
      "[56]\ttrain-mlogloss:0.569788\ttest-mlogloss:1.16035\n",
      "[57]\ttrain-mlogloss:0.562883\ttest-mlogloss:1.16186\n",
      "[58]\ttrain-mlogloss:0.555598\ttest-mlogloss:1.16111\n",
      "[59]\ttrain-mlogloss:0.548747\ttest-mlogloss:1.16195\n",
      "[60]\ttrain-mlogloss:0.54192\ttest-mlogloss:1.16094\n",
      "[61]\ttrain-mlogloss:0.535529\ttest-mlogloss:1.16124\n",
      "[62]\ttrain-mlogloss:0.529332\ttest-mlogloss:1.162\n",
      "[63]\ttrain-mlogloss:0.523076\ttest-mlogloss:1.16281\n",
      "[64]\ttrain-mlogloss:0.517169\ttest-mlogloss:1.16098\n",
      "[65]\ttrain-mlogloss:0.511494\ttest-mlogloss:1.16303\n",
      "[66]\ttrain-mlogloss:0.506028\ttest-mlogloss:1.16184\n",
      "[67]\ttrain-mlogloss:0.500552\ttest-mlogloss:1.163\n",
      "[68]\ttrain-mlogloss:0.494452\ttest-mlogloss:1.16196\n",
      "[69]\ttrain-mlogloss:0.48899\ttest-mlogloss:1.16186\n",
      "[70]\ttrain-mlogloss:0.483822\ttest-mlogloss:1.16323\n",
      "[71]\ttrain-mlogloss:0.478879\ttest-mlogloss:1.16377\n",
      "[72]\ttrain-mlogloss:0.473463\ttest-mlogloss:1.16342\n",
      "[73]\ttrain-mlogloss:0.468376\ttest-mlogloss:1.1627\n",
      "[74]\ttrain-mlogloss:0.463501\ttest-mlogloss:1.16236\n",
      "[75]\ttrain-mlogloss:0.458471\ttest-mlogloss:1.16397\n",
      "[76]\ttrain-mlogloss:0.45445\ttest-mlogloss:1.16423\n",
      "[77]\ttrain-mlogloss:0.449867\ttest-mlogloss:1.16682\n",
      "[78]\ttrain-mlogloss:0.444997\ttest-mlogloss:1.16748\n",
      "[79]\ttrain-mlogloss:0.440205\ttest-mlogloss:1.16586\n",
      "[80]\ttrain-mlogloss:0.435179\ttest-mlogloss:1.16606\n",
      "[81]\ttrain-mlogloss:0.430464\ttest-mlogloss:1.16606\n",
      "[82]\ttrain-mlogloss:0.425948\ttest-mlogloss:1.16602\n",
      "[83]\ttrain-mlogloss:0.421622\ttest-mlogloss:1.16526\n",
      "[84]\ttrain-mlogloss:0.417277\ttest-mlogloss:1.16827\n",
      "[85]\ttrain-mlogloss:0.412604\ttest-mlogloss:1.16777\n",
      "[86]\ttrain-mlogloss:0.409156\ttest-mlogloss:1.16958\n",
      "[87]\ttrain-mlogloss:0.404813\ttest-mlogloss:1.16816\n",
      "[88]\ttrain-mlogloss:0.400693\ttest-mlogloss:1.16803\n",
      "[89]\ttrain-mlogloss:0.396527\ttest-mlogloss:1.16904\n",
      "[90]\ttrain-mlogloss:0.392367\ttest-mlogloss:1.16981\n",
      "[91]\ttrain-mlogloss:0.388496\ttest-mlogloss:1.17178\n",
      "[92]\ttrain-mlogloss:0.384521\ttest-mlogloss:1.17118\n",
      "[93]\ttrain-mlogloss:0.380693\ttest-mlogloss:1.17294\n",
      "[94]\ttrain-mlogloss:0.376928\ttest-mlogloss:1.17317\n",
      "[95]\ttrain-mlogloss:0.3732\ttest-mlogloss:1.17535\n",
      "[96]\ttrain-mlogloss:0.369751\ttest-mlogloss:1.17668\n",
      "[97]\ttrain-mlogloss:0.366569\ttest-mlogloss:1.17847\n",
      "[98]\ttrain-mlogloss:0.363234\ttest-mlogloss:1.18012\n",
      "[99]\ttrain-mlogloss:0.359357\ttest-mlogloss:1.18164\n",
      "[100]\ttrain-mlogloss:0.355939\ttest-mlogloss:1.18359\n",
      "[101]\ttrain-mlogloss:0.352648\ttest-mlogloss:1.18633\n",
      "[102]\ttrain-mlogloss:0.349356\ttest-mlogloss:1.18599\n",
      "[103]\ttrain-mlogloss:0.346345\ttest-mlogloss:1.18689\n",
      "[104]\ttrain-mlogloss:0.343477\ttest-mlogloss:1.18897\n",
      "[105]\ttrain-mlogloss:0.340648\ttest-mlogloss:1.19096\n",
      "[106]\ttrain-mlogloss:0.337635\ttest-mlogloss:1.19144\n",
      "[107]\ttrain-mlogloss:0.334423\ttest-mlogloss:1.19324\n",
      "[108]\ttrain-mlogloss:0.331934\ttest-mlogloss:1.1935\n",
      "[109]\ttrain-mlogloss:0.328865\ttest-mlogloss:1.19293\n",
      "[110]\ttrain-mlogloss:0.326344\ttest-mlogloss:1.19378\n",
      "[111]\ttrain-mlogloss:0.323877\ttest-mlogloss:1.19538\n",
      "[112]\ttrain-mlogloss:0.3211\ttest-mlogloss:1.19729\n",
      "[113]\ttrain-mlogloss:0.318798\ttest-mlogloss:1.19818\n",
      "[114]\ttrain-mlogloss:0.316021\ttest-mlogloss:1.19935\n",
      "[115]\ttrain-mlogloss:0.313596\ttest-mlogloss:1.2018\n",
      "[116]\ttrain-mlogloss:0.3109\ttest-mlogloss:1.20344\n",
      "[117]\ttrain-mlogloss:0.308702\ttest-mlogloss:1.20401\n",
      "[118]\ttrain-mlogloss:0.306419\ttest-mlogloss:1.20547\n",
      "[119]\ttrain-mlogloss:0.303858\ttest-mlogloss:1.20541\n",
      "[120]\ttrain-mlogloss:0.301594\ttest-mlogloss:1.20662\n",
      "[121]\ttrain-mlogloss:0.299297\ttest-mlogloss:1.20736\n",
      "[122]\ttrain-mlogloss:0.296761\ttest-mlogloss:1.20847\n",
      "[123]\ttrain-mlogloss:0.294706\ttest-mlogloss:1.2119\n",
      "[124]\ttrain-mlogloss:0.292332\ttest-mlogloss:1.21448\n",
      "[125]\ttrain-mlogloss:0.290034\ttest-mlogloss:1.21519\n",
      "[126]\ttrain-mlogloss:0.287735\ttest-mlogloss:1.21687\n",
      "[127]\ttrain-mlogloss:0.285757\ttest-mlogloss:1.22008\n",
      "[128]\ttrain-mlogloss:0.283844\ttest-mlogloss:1.22024\n",
      "[129]\ttrain-mlogloss:0.281792\ttest-mlogloss:1.22061\n",
      "[130]\ttrain-mlogloss:0.279516\ttest-mlogloss:1.22335\n",
      "[131]\ttrain-mlogloss:0.277622\ttest-mlogloss:1.22497\n",
      "[132]\ttrain-mlogloss:0.275272\ttest-mlogloss:1.22602\n",
      "[133]\ttrain-mlogloss:0.27325\ttest-mlogloss:1.22716\n",
      "[134]\ttrain-mlogloss:0.271316\ttest-mlogloss:1.2278\n",
      "[135]\ttrain-mlogloss:0.269398\ttest-mlogloss:1.23052\n",
      "[136]\ttrain-mlogloss:0.267438\ttest-mlogloss:1.22975\n",
      "[137]\ttrain-mlogloss:0.2657\ttest-mlogloss:1.23166\n",
      "[138]\ttrain-mlogloss:0.263557\ttest-mlogloss:1.23385\n",
      "[139]\ttrain-mlogloss:0.261597\ttest-mlogloss:1.23363\n",
      "[140]\ttrain-mlogloss:0.259687\ttest-mlogloss:1.23454\n",
      "[141]\ttrain-mlogloss:0.258039\ttest-mlogloss:1.2349\n",
      "[142]\ttrain-mlogloss:0.256255\ttest-mlogloss:1.23431\n",
      "[143]\ttrain-mlogloss:0.254434\ttest-mlogloss:1.23655\n",
      "[144]\ttrain-mlogloss:0.252498\ttest-mlogloss:1.23809\n",
      "[145]\ttrain-mlogloss:0.250768\ttest-mlogloss:1.24001\n",
      "[146]\ttrain-mlogloss:0.248959\ttest-mlogloss:1.24049\n",
      "[147]\ttrain-mlogloss:0.247611\ttest-mlogloss:1.24209\n",
      "[148]\ttrain-mlogloss:0.245863\ttest-mlogloss:1.24453\n",
      "[149]\ttrain-mlogloss:0.244292\ttest-mlogloss:1.24609\n",
      "[150]\ttrain-mlogloss:0.242582\ttest-mlogloss:1.2475\n",
      "[151]\ttrain-mlogloss:0.240995\ttest-mlogloss:1.25002\n",
      "[152]\ttrain-mlogloss:0.239356\ttest-mlogloss:1.24944\n",
      "[153]\ttrain-mlogloss:0.237831\ttest-mlogloss:1.24948\n",
      "[154]\ttrain-mlogloss:0.236441\ttest-mlogloss:1.25113\n",
      "[155]\ttrain-mlogloss:0.234914\ttest-mlogloss:1.25111\n",
      "[156]\ttrain-mlogloss:0.233597\ttest-mlogloss:1.25243\n",
      "[157]\ttrain-mlogloss:0.232211\ttest-mlogloss:1.2533\n",
      "[158]\ttrain-mlogloss:0.230921\ttest-mlogloss:1.25454\n",
      "[159]\ttrain-mlogloss:0.229654\ttest-mlogloss:1.25669\n",
      "[160]\ttrain-mlogloss:0.228238\ttest-mlogloss:1.25844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttrain-mlogloss:0.226814\ttest-mlogloss:1.26151\n",
      "[162]\ttrain-mlogloss:0.225299\ttest-mlogloss:1.26134\n",
      "[163]\ttrain-mlogloss:0.223828\ttest-mlogloss:1.26284\n",
      "[164]\ttrain-mlogloss:0.222529\ttest-mlogloss:1.26463\n",
      "[165]\ttrain-mlogloss:0.221054\ttest-mlogloss:1.26605\n",
      "[166]\ttrain-mlogloss:0.219676\ttest-mlogloss:1.26796\n",
      "[167]\ttrain-mlogloss:0.218413\ttest-mlogloss:1.27052\n",
      "[168]\ttrain-mlogloss:0.217222\ttest-mlogloss:1.27146\n",
      "[169]\ttrain-mlogloss:0.215872\ttest-mlogloss:1.27317\n",
      "[170]\ttrain-mlogloss:0.214516\ttest-mlogloss:1.27493\n",
      "[171]\ttrain-mlogloss:0.213127\ttest-mlogloss:1.27627\n",
      "[172]\ttrain-mlogloss:0.211735\ttest-mlogloss:1.27957\n",
      "[173]\ttrain-mlogloss:0.210434\ttest-mlogloss:1.28074\n",
      "[174]\ttrain-mlogloss:0.209319\ttest-mlogloss:1.28247\n",
      "[175]\ttrain-mlogloss:0.208058\ttest-mlogloss:1.28359\n",
      "[176]\ttrain-mlogloss:0.206931\ttest-mlogloss:1.28516\n",
      "[177]\ttrain-mlogloss:0.205931\ttest-mlogloss:1.28757\n",
      "[178]\ttrain-mlogloss:0.204955\ttest-mlogloss:1.28908\n",
      "[179]\ttrain-mlogloss:0.203777\ttest-mlogloss:1.2888\n",
      "[180]\ttrain-mlogloss:0.202627\ttest-mlogloss:1.28939\n",
      "[181]\ttrain-mlogloss:0.201599\ttest-mlogloss:1.29026\n",
      "[182]\ttrain-mlogloss:0.2004\ttest-mlogloss:1.29222\n",
      "[183]\ttrain-mlogloss:0.199317\ttest-mlogloss:1.29362\n",
      "[184]\ttrain-mlogloss:0.198219\ttest-mlogloss:1.29512\n",
      "[185]\ttrain-mlogloss:0.196981\ttest-mlogloss:1.2959\n",
      "[186]\ttrain-mlogloss:0.195842\ttest-mlogloss:1.29713\n",
      "[187]\ttrain-mlogloss:0.194859\ttest-mlogloss:1.29734\n",
      "[188]\ttrain-mlogloss:0.19366\ttest-mlogloss:1.29892\n",
      "[189]\ttrain-mlogloss:0.192776\ttest-mlogloss:1.29972\n",
      "[190]\ttrain-mlogloss:0.191875\ttest-mlogloss:1.30134\n",
      "[191]\ttrain-mlogloss:0.190943\ttest-mlogloss:1.3024\n",
      "[192]\ttrain-mlogloss:0.19007\ttest-mlogloss:1.30445\n",
      "[193]\ttrain-mlogloss:0.189188\ttest-mlogloss:1.30676\n",
      "[194]\ttrain-mlogloss:0.188235\ttest-mlogloss:1.30864\n",
      "[195]\ttrain-mlogloss:0.187139\ttest-mlogloss:1.31004\n",
      "[196]\ttrain-mlogloss:0.186265\ttest-mlogloss:1.3116\n",
      "[197]\ttrain-mlogloss:0.185462\ttest-mlogloss:1.31353\n",
      "[198]\ttrain-mlogloss:0.184588\ttest-mlogloss:1.31336\n",
      "[199]\ttrain-mlogloss:0.183672\ttest-mlogloss:1.31476\n",
      "[200]\ttrain-mlogloss:0.182778\ttest-mlogloss:1.31593\n",
      "[201]\ttrain-mlogloss:0.181717\ttest-mlogloss:1.31573\n",
      "[202]\ttrain-mlogloss:0.180819\ttest-mlogloss:1.31625\n",
      "[203]\ttrain-mlogloss:0.1799\ttest-mlogloss:1.31753\n",
      "[204]\ttrain-mlogloss:0.179097\ttest-mlogloss:1.3189\n",
      "[205]\ttrain-mlogloss:0.178312\ttest-mlogloss:1.32084\n",
      "[206]\ttrain-mlogloss:0.177485\ttest-mlogloss:1.3219\n",
      "[207]\ttrain-mlogloss:0.176764\ttest-mlogloss:1.32306\n",
      "[208]\ttrain-mlogloss:0.175977\ttest-mlogloss:1.32327\n",
      "[209]\ttrain-mlogloss:0.17522\ttest-mlogloss:1.32434\n",
      "[210]\ttrain-mlogloss:0.174543\ttest-mlogloss:1.32538\n",
      "[211]\ttrain-mlogloss:0.173851\ttest-mlogloss:1.3258\n",
      "[212]\ttrain-mlogloss:0.173113\ttest-mlogloss:1.32621\n",
      "[213]\ttrain-mlogloss:0.172253\ttest-mlogloss:1.32767\n",
      "[214]\ttrain-mlogloss:0.171509\ttest-mlogloss:1.32886\n",
      "[215]\ttrain-mlogloss:0.170726\ttest-mlogloss:1.32983\n",
      "[216]\ttrain-mlogloss:0.169955\ttest-mlogloss:1.33144\n",
      "[217]\ttrain-mlogloss:0.169101\ttest-mlogloss:1.33218\n",
      "[218]\ttrain-mlogloss:0.168319\ttest-mlogloss:1.3341\n",
      "[219]\ttrain-mlogloss:0.167644\ttest-mlogloss:1.3342\n",
      "[220]\ttrain-mlogloss:0.166885\ttest-mlogloss:1.33467\n",
      "[221]\ttrain-mlogloss:0.166139\ttest-mlogloss:1.33586\n",
      "[222]\ttrain-mlogloss:0.165286\ttest-mlogloss:1.33763\n",
      "[223]\ttrain-mlogloss:0.164658\ttest-mlogloss:1.33817\n",
      "[224]\ttrain-mlogloss:0.164\ttest-mlogloss:1.34043\n",
      "[225]\ttrain-mlogloss:0.163192\ttest-mlogloss:1.34151\n",
      "[226]\ttrain-mlogloss:0.162471\ttest-mlogloss:1.34307\n",
      "[227]\ttrain-mlogloss:0.161782\ttest-mlogloss:1.34322\n",
      "[228]\ttrain-mlogloss:0.161018\ttest-mlogloss:1.34336\n",
      "[229]\ttrain-mlogloss:0.160346\ttest-mlogloss:1.34411\n",
      "[230]\ttrain-mlogloss:0.159624\ttest-mlogloss:1.34582\n",
      "[231]\ttrain-mlogloss:0.159049\ttest-mlogloss:1.3468\n",
      "[232]\ttrain-mlogloss:0.158337\ttest-mlogloss:1.34826\n",
      "[233]\ttrain-mlogloss:0.157629\ttest-mlogloss:1.35025\n",
      "[234]\ttrain-mlogloss:0.157054\ttest-mlogloss:1.35283\n",
      "[235]\ttrain-mlogloss:0.156409\ttest-mlogloss:1.3535\n",
      "[236]\ttrain-mlogloss:0.155767\ttest-mlogloss:1.3538\n",
      "[237]\ttrain-mlogloss:0.155128\ttest-mlogloss:1.35459\n",
      "[238]\ttrain-mlogloss:0.154512\ttest-mlogloss:1.35509\n",
      "[239]\ttrain-mlogloss:0.153881\ttest-mlogloss:1.35614\n",
      "[240]\ttrain-mlogloss:0.153245\ttest-mlogloss:1.35782\n",
      "[241]\ttrain-mlogloss:0.152688\ttest-mlogloss:1.3585\n",
      "[242]\ttrain-mlogloss:0.152169\ttest-mlogloss:1.36041\n",
      "[243]\ttrain-mlogloss:0.151522\ttest-mlogloss:1.36256\n",
      "[244]\ttrain-mlogloss:0.151022\ttest-mlogloss:1.36278\n",
      "[245]\ttrain-mlogloss:0.150483\ttest-mlogloss:1.36346\n",
      "[246]\ttrain-mlogloss:0.149924\ttest-mlogloss:1.36412\n",
      "[247]\ttrain-mlogloss:0.149274\ttest-mlogloss:1.3647\n",
      "[248]\ttrain-mlogloss:0.148711\ttest-mlogloss:1.36484\n",
      "[249]\ttrain-mlogloss:0.148237\ttest-mlogloss:1.36685\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import operator\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4,\n",
    "    'silent': 1,\n",
    "    'num_class': 5\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(features_train, labels_train)\n",
    "dtest = xgb.DMatrix(features_test, labels_test)\n",
    "\n",
    "num_boost_rounds = 250\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'test')]\n",
    "\n",
    "# train model\n",
    "xgb_model = xgb.train(xgb_params, dtrain, num_boost_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x26d28a28a20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26d289d4860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26d291d6748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importance = xgb_model.get_fscore()\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "importance_df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "\n",
    "# Plot Feature Importance\n",
    "plt.figure()\n",
    "importance_df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(dataframe, importance_scores_df, threshold=0.4):\n",
    "    normalized_df = importance_scores_df.copy()\n",
    "    normalized_df['fscore'] = (importance_scores_df['fscore'] - importance_scores_df['fscore'].min())/(importance_scores_df['fscore'].max()-importance_scores_df['fscore'].min())\n",
    "    normalized_df = normalized_df[normalized_df['fscore'] >= threshold]\n",
    "    new_dataframe = dataframe.filter(items=normalized_df['feature'].tolist())\n",
    "    return new_dataframe\n",
    "\n",
    "features_train_009_df = feature_selection(features_train_df, importance_df, threshold=0.09)\n",
    "features_train_009 = features_train_009_df.as_matrix()\n",
    "features_train_04_df = feature_selection(features_train_df, importance_df, threshold=0.4)\n",
    "features_train_04 = features_train_04_df.as_matrix()\n",
    "\n",
    "features_test_009_df = feature_selection(features_test_df, importance_df, threshold=0.09)\n",
    "features_test_009 = features_test_009_df.as_matrix()\n",
    "features_test_04_df = feature_selection(features_test_df, importance_df, threshold=0.4)\n",
    "features_test_04 = features_test_04_df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (by David)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components for Dataset with threshold=0.4 is 2\n",
      "Number of components for Dataset with threshold=0.09 is 5\n"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "def eigen_scores(dataframe):\n",
    "    dataframe-=np.mean(dataframe, axis=0)\n",
    "    dataframe/=np.std(dataframe, axis=0)\n",
    "    cov_mat=np.cov(dataframe, rowvar=False)\n",
    "    evals, evecs = np.linalg.eigh(cov_mat)\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evecs = evecs[:,idx]\n",
    "    evals = evals[idx]\n",
    "    return evals, evecs\n",
    "\n",
    "def my_pca(dataframe, n_components):\n",
    "    pca=PCA(n_components=n_components)\n",
    "    return pca.fit_transform(dataframe)\n",
    "    \n",
    "eigenvalues, eigenvectors = eigen_scores(features_test_04.copy())\n",
    "n_components_04 = len(np.where(eigenvalues >= 1)[0])\n",
    "print(\"Number of components for Dataset with threshold=0.4 is {}\".format(n_components_04))\n",
    "\n",
    "features_train_04_pca = my_pca(features_train_04, n_components_04)\n",
    "features_test_04_pca = my_pca(features_test_04, n_components_04)\n",
    "\n",
    "eigenvalues, eigenvectors = eigen_scores(features_test_009.copy())\n",
    "n_components_009 = len(np.where(eigenvalues >= 1)[0])\n",
    "print(\"Number of components for Dataset with threshold=0.09 is {}\".format(n_components_009))\n",
    "\n",
    "features_train_009_pca = my_pca(features_train_009, n_components_009)\n",
    "features_test_009_pca = my_pca(features_test_009, n_components_009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling And Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an relatively small dataset. Therefore, we should do our feature selection based on a cross-\n",
    "validated set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE for SVM - Balancing only on the training set, not the validation set  [This is for the traditional training -not the cross validated one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#further divide the 'traditional' non-cross set into training 80/20  for pure training and cross validation  \n",
    "features_train_notoversampled, features_validate, labels_train_notoversampled, labels_validate = train_test_split(features_train, labels_train, test_size = .2, random_state=0)\n",
    "\n",
    "sm = SMOTE(random_state=0, ratio = 1.0, kind= 'svm' )\n",
    "#x_train_res, y_train_res = sm.fit_sample(x_train, y_train)\n",
    "features_train_oversampled, labels_train_oversampled = sm.fit_sample(features_train_notoversampled, labels_train_notoversampled)\n",
    "\n",
    "#re-enter into original variables\n",
    "##features_train = features_train_oversampled\n",
    "##labels_train = labels_train_oversampled\n",
    "\n",
    "#Below 2 lines if we want to want to force the array back into dataframe    \n",
    "##features_train = pd.DataFrame(features_train_oversampled,columns=[\"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\"thalach\",\"exang\",\"oldpeak\",\"slop\",\"ca\",\"thal\"])\n",
    "##labels_train = pd.DataFrame(labels_train_oversampled,columns=[\"pred_attribute\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler (by David)\n",
    "SVC Models are only any good when the data is scaled. Lets scale the data and build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.19127179, -1.39664501, -0.55805859, ..., -0.21964884,\n",
       "         0.91567545, -0.81934649],\n",
       "       [-0.75808205,  0.71600156, -1.32901815, ..., -0.21964884,\n",
       "         0.91567545, -0.81934649],\n",
       "       [ 0.        , -1.39664501,  0.15783243, ..., -0.21964884,\n",
       "         0.91567545, -0.81934649],\n",
       "       ..., \n",
       "       [-2.05765127, -1.39664501,  0.32303804, ..., -0.21964884,\n",
       "         0.91567545, -0.81934649],\n",
       "       [-0.43318974,  0.71600156,  0.98386052, ..., -0.21964884,\n",
       "        -1.09208999,  1.22048488],\n",
       "       [ 0.86637948, -1.39664501,  0.43317512, ..., -0.21964884,\n",
       "         0.91567545, -0.81934649]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing.data import QuantileTransformer\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "Standard_scaler = StandardScaler()\n",
    "Robust_scaler = preprocessing.RobustScaler(quantile_range=(25, 75))\n",
    "Quantile_scalar = preprocessing.QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "features_train = Standard_scaler.fit_transform(features_train)\n",
    "features_test = Standard_scaler.transform(features_test)\n",
    "\n",
    "features_train_009 = Standard_scaler.fit_transform(features_train_009)\n",
    "features_test_009 = Standard_scaler.transform(features_test_009)\n",
    "\n",
    "features_train_04 = Standard_scaler.fit_transform(features_train_04)\n",
    "features_test_04 = Standard_scaler.transform(features_test_04)\n",
    "\n",
    "features_train_009_pca = Standard_scaler.fit_transform(features_train_009_pca)\n",
    "features_test_009_pca = Standard_scaler.transform(features_test_009_pca)\n",
    "\n",
    "features_train_04_pca = Standard_scaler.fit_transform(features_train_04_pca)\n",
    "features_test_04_pca = Standard_scaler.transform(features_test_04_pca)\n",
    "\n",
    "features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing only on the training set, not the validation set\n",
    "Unfortunately SMOTE categorial implementation is not really implemented\n",
    "We will do simple oversampling -> Done using external program SPSS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to files for external program to balance the data\n",
    "\n",
    "#without features selection\n",
    "merged = np.concatenate((features_train, labels_train.reshape((-1, 1))), axis=1)\n",
    "merged_df = pd.DataFrame(merged) \n",
    "merged_df.to_csv(\"data/train_NoEng_NB.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "#features_train_009\n",
    " \n",
    "#features_train_04 \n",
    "\n",
    "#features_train_009_pca \n",
    "\n",
    "#features_train_04_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train_noEng_Balanced.csv\")\n",
    "\n",
    "labels_train_df = df.iloc[:, 22:23]\n",
    "df.drop(df.columns[[-1,]], axis=1, inplace=True)\n",
    "\n",
    "features_train = df.as_matrix()\n",
    "labels_train = labels_train_df.as_matrix()\n",
    "\n",
    "#print(features_train)\n",
    "#print(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn import grid_search\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "def checkmetrics(pred, labels_test, name):\n",
    "    sns.set()\n",
    "    print('The accuracy of ', name, 'is: ', accuracy_score(pred, labels_test))\n",
    "    matrix = confusion_matrix(labels_test, pred)\n",
    "    ax = sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    print(ax)\n",
    "    print(classification_report(pred, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score using lasso:\n",
      "[(0.495, 'thalach'), (0.46999999999999997, 'exang'), (0.45000000000000001, 'age'), (0.44500000000000001, 'oldpeak'), (0.29999999999999999, 'fbs'), (0.185, 'ca'), (0.059999999999999998, 'cp'), (0.0050000000000000001, 'slope'), (0.0050000000000000001, 'restecg'), (0.0, 'trestbps'), (0.0, 'thal'), (0.0, 'sex'), (0.0, 'class'), (0.0, 'chol')]\n",
      "Features sorted by their score using Linear Regression:\n",
      "[(1, 'class'), (3, 'oldpeak'), (5, 'exang'), (7, 'restecg'), (8, 'sex'), (9, 'thal'), (10, 'thalach'), (12, 'ca'), (17, 'slope'), (18, 'chol'), (19, 'cp'), (20, 'age'), (21, 'fbs'), (22, 'trestbps')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class RandomizedLasso is deprecated; The class RandomizedLasso is deprecated in 0.19 and will be removed in 0.21.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Feature selection using RFECV to pick best features,\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.feature_selection import RFECV\n",
    "rlasso = RandomizedLasso(alpha=0.025)\n",
    "names = features_list\n",
    "rlasso.fit(features_train, labels_train)\n",
    " \n",
    "print(\"Features sorted by their score using lasso:\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), rlasso.scores_), \n",
    "                 names), reverse=True))\n",
    "\n",
    "#use linear regression as the model\n",
    "lr = LinearRegression()\n",
    "#rank all features, i.e continue the elimination until the last one\n",
    "rfe = RFE(lr, n_features_to_select=1)\n",
    "rfe.fit(X,Y)\n",
    " \n",
    "print(\"Features sorted by their score using Linear Regression:\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with HyperParameters -Tuning - Default Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 22 candidates, totalling 110 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 110 out of 110 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score: 0.929824561404 with parameters: {'C': 1, 'class_weight': 'balanced', 'gamma': 0.1}\n",
      "The accuracy of  C-Support Vector Classification is:  0.733333333333\n",
      "Axes(0.125,0.125;0.62x0.755)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.83      0.81        29\n",
      "        1.0       0.50      0.46      0.48        13\n",
      "        2.0       0.90      0.82      0.86        11\n",
      "        3.0       1.00      0.60      0.75         5\n",
      "        4.0       0.40      1.00      0.57         2\n",
      "\n",
      "avg / total       0.76      0.73      0.74        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26d289d4710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#svm.SVC  C-Support Vector Classification.\n",
    "parameters ={\n",
    "        'C': [0.1,1], \n",
    "        'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5],\n",
    "        \"class_weight\": ['balanced']\n",
    "        }\n",
    "SVM = svm.SVC()\n",
    "\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=5,n_jobs=-1, return_train_score=True, refit=True,verbose=1)\n",
    "grid_search_cv.fit(features_train, labels_train)\n",
    "resultsdf=pd.DataFrame(grid_search_cv.cv_results_)\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train, labels_train)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.best_estimator_.predict(features_test)\n",
    "\n",
    "checkmetrics(pred, labels_test, 'C-Support Vector Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters ={\n",
    "    'C': [0.1,0.2,0.5,1,2,3,4,5], \n",
    "    'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5],\n",
    "#    \"class_weight\": ['balanced', None]\n",
    "}\n",
    "\n",
    "\n",
    "SVM = svm.SVC(kernel=\"linear\")\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search_cv.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score:\", str(grid_search_cv.score(features_train, labels_train)), \"with parameters:\", grid_search_cv.best_params_)\n",
    "\n",
    "pred = grid_search_cv.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge - Validate - support vector machine linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with one-versus all:\n",
    "parameters ={\n",
    "    'C': [0.1,0.2,0.5,1,2,3,4,5],\n",
    "#    \"class_weight\": ['balanced', None],\n",
    "    'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5], \n",
    "             'decision_function_shape': ['ovo', 'ovr']\n",
    "}\n",
    "SVM = svm.SVC(kernel=\"linear\")\n",
    "grid_search_cv = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search_cv.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score for ovo:\", str(grid_search_cv.score(features_train, labels_train)), 'with parameters:', grid_search_cv.best_params_)\n",
    "pred = grid_search_cv.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge, one vs one - Validate - support vector machine linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different model (standard one vs. rest) loss not automatically being squared hinge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'C': [0.1,0.2,0.5,1,2,3,4,5],\n",
    "#    \"class_weight\": ['balanced', None]\n",
    "}\n",
    "SVM = LinearSVC(loss=\"hinge\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score: \", str(grid_search.score(features_train, labels_train)), 'with parameters:', grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - hinge - Validate - support vector machine linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with Cramer:\n",
    "parameters = {\n",
    "    'C': [0.1,0.2,0.5,1,2,3,4,5],\n",
    "#    \"class_weight\": ['balanced', None],\n",
    "    'multi_class':['ovr', 'crammer_singer']\n",
    "}\n",
    "SVM = LinearSVC(loss=\"hinge\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score: \", str(grid_search.score(features_train, labels_train)), 'with parameters:', grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - hinge, one vs rest - Validate - support vector machine linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification\n",
    "### Polynominal features\n",
    "\n",
    "Note that when there are multiple features, Polynomial Regression is capable of finding relationships\n",
    "between features (which is something a plain Linear Regression model cannot do). This is made possible\n",
    "by the fact that PolynomialFeatures also adds all combinations of features up to the given degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW GRIDSEARCH SHOULD WORK\n",
    "parameters = {\n",
    "    'C': [0.1,0.2,0.5,1,2,3,4,5], \n",
    "    'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5],\n",
    "    \"degree\": [1,2,3],\n",
    "    \"coef0\": [1,10]\n",
    "}\n",
    "SVM = svm.SVC(kernel=\"poly\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score:\", str(grid_search.score(features_train, labels_train)), \"with parameters:\", grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge - Validate - support vector machine linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like the polynomial features method, the similarity features method can be useful with any Machine\n",
    "# Learning algorithm, but it may be computationally expensive to compute all the additional features,\n",
    "# especially on large training sets. However, once again the kernel trick does its SVM magic: it makes it\n",
    "# possible to obtain a similar result as if you had added many similarity features, without actually having to\n",
    "# add them\n",
    "\n",
    "parameters ={\n",
    "    'C': [0.1,0.2,0.5,1,2,3,4,5], \n",
    "    'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5],         \n",
    "#    \"class_weight\": ['balanced', None]\n",
    "}\n",
    "SVM = svm.SVC(kernel=\"rbf\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The train score:\", str(grid_search.score(features_train, labels_train)), \"with parameters:\", grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge - Validate - support vector machine linear')\n",
    "\n",
    "# no need to compare with one vs all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Important Features with threshold >= 0.09  (XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like the polynomial features method, the similarity features method can be useful with any Machine\n",
    "# Learning algorithm, but it may be computationally expensive to compute all the additional features,\n",
    "# especially on large training sets. However, once again the kernel trick does its SVM magic: it makes it\n",
    "# possible to obtain a similar result as if you had added many similarity features, without actually having to\n",
    "# add them\n",
    "\n",
    "parameters ={\n",
    "    'C': [0.1,0.2,0.5,1,2,3,4,5], \n",
    "    'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5],         \n",
    "#    \"class_weight\": ['balanced', None]\n",
    "}\n",
    "SVM = svm.SVC(kernel=\"rbf\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train_009, labels_train)\n",
    "\n",
    "print(\"The train score:\", str(grid_search.score(features_train_009, labels_train)), \"with parameters:\", grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test_009)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge - Validate - support vector machine linear')\n",
    "\n",
    "# no need to compare with one vs all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Important Features with threshold >= 0.4 (XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like the polynomial features method, the similarity features method can be useful with any Machine\n",
    "# Learning algorithm, but it may be computationally expensive to compute all the additional features,\n",
    "# especially on large training sets. However, once again the kernel trick does its SVM magic: it makes it\n",
    "# possible to obtain a similar result as if you had added many similarity features, without actually having to\n",
    "# add them\n",
    "\n",
    "parameters ={\n",
    "    'C': [0.1,0.2,0.5,1,2,3,4,5], \n",
    "    'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5],         \n",
    "#    \"class_weight\": ['balanced', None]\n",
    "}\n",
    "SVM = svm.SVC(kernel=\"rbf\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train_04, labels_train)\n",
    "\n",
    "print(\"The train score:\", str(grid_search.score(features_train_04, labels_train)), \"with parameters:\", grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test_04)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge - Validate - support vector machine linear')\n",
    "\n",
    "# no need to compare with one vs all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PCA with Important Features (threshold >= 0.09) (XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like the polynomial features method, the similarity features method can be useful with any Machine\n",
    "# Learning algorithm, but it may be computationally expensive to compute all the additional features,\n",
    "# especially on large training sets. However, once again the kernel trick does its SVM magic: it makes it\n",
    "# possible to obtain a similar result as if you had added many similarity features, without actually having to\n",
    "# add them\n",
    "\n",
    "parameters ={\n",
    "    'C': [0.1,0.2,0.5,1,2,3,4,5], \n",
    "    'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5],         \n",
    "#    \"class_weight\": ['balanced', None]\n",
    "}\n",
    "SVM = svm.SVC(kernel=\"rbf\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train_009_pca, labels_train)\n",
    "\n",
    "print(\"The train score:\", str(grid_search.score(features_train_009_pca, labels_train)), \"with parameters:\", grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test_009_pca)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge - Validate - support vector machine linear')\n",
    "\n",
    "# no need to compare with one vs all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PCA with Important Features (threshold >= 0.4) by David (XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like the polynomial features method, the similarity features method can be useful with any Machine\n",
    "# Learning algorithm, but it may be computationally expensive to compute all the additional features,\n",
    "# especially on large training sets. However, once again the kernel trick does its SVM magic: it makes it\n",
    "# possible to obtain a similar result as if you had added many similarity features, without actually having to\n",
    "# add them\n",
    "\n",
    "parameters ={\n",
    "    'C': [0.1,0.2,0.5,1,2,3,4,5], \n",
    "    'gamma': [0.00001,0.01,0.05,0.1,0.2,0.5,1,2,3,4,5],         \n",
    "#    \"class_weight\": ['balanced', None]\n",
    "}\n",
    "SVM = svm.SVC(kernel=\"rbf\")\n",
    "grid_search = GridSearchCV(SVM, parameters, cv=10)\n",
    "grid_search.fit(features_train_04_pca, labels_train)\n",
    "\n",
    "print(\"The train score:\", str(grid_search.score(features_train_04_pca, labels_train)), \"with parameters:\", grid_search.best_params_)\n",
    "\n",
    "pred = grid_search.predict(features_test_04_pca)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - sq hinge - Validate - support vector machine linear')\n",
    "\n",
    "# no need to compare with one vs all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
